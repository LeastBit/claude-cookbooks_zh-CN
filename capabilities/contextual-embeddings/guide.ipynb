{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用上下文检索增强RAG\n",
    "\n",
    "> 注意：有关上下文检索的更多背景信息，包括在各种数据集上的附加性能评估，我们建议阅读我们的配套[博客文章](https://www.anthropic.com/news/contextual-retrieval)。\n",
    "\n",
    "检索增强生成（RAG）使Claude能够在提供响应时利用您的内部知识库、代码库或任何其他文档语料库。企业越来越多地构建RAG应用程序来改善客户支持、内部公司文档问答、财务和法律分析、代码生成等领域的流程。\n",
    "\n",
    "在[单独指南](https://github.com/anthropics/anthropic-cookbook/blob/main/capabilities/retrieval_augmented_generation/guide.ipynb)中，我们介绍了如何设置基本检索系统，演示了如何评估其性能，然后概述了几种改进性能的技巧。在本指南中，我们介绍了一种改进检索性能的技术：上下文嵌入。\n",
    "\n",
    "在传统的RAG中，文档通常被分割成更小的块以便高效检索。虽然这种方法对许多应用程序都很有效，但当单独的块缺乏足够的上下文时，可能会导致问题。上下文嵌入通过在嵌入之前为每个块添加相关上下文来解决这个问题。这种方法提高了每个嵌入块的质量，从而实现更准确的检索和更好的整体性能。在我们测试的所有数据源中平均计算，上下文嵌入将前20块检索失败率降低了35%。\n",
    "\n",
    "相同的块特定上下文也可以与BM25搜索结合使用，进一步提高检索性能。我们在\"上下文BM25\"部分中介绍这种技术。\n",
    "\n",
    "在本指南中，我们将演示如何使用包含9个代码库的数据集作为知识库来构建和优化上下文检索系统。我们将介绍：\n",
    "\n",
    "1) 设置基本检索管道以建立性能基线。\n",
    "\n",
    "2) 上下文嵌入：它是什么，为什么有效，以及提示缓存如何使其在实际生产用例中实用。\n",
    "\n",
    "3) 实现上下文嵌入并演示性能改进。\n",
    "\n",
    "4) 上下文BM25：使用*上下文* BM25混合搜索改进性能。\n",
    "\n",
    "5) 通过重新排序改进性能。\n",
    "\n",
    "### 评估指标和数据集：\n",
    "\n",
    "我们使用9个代码库的预分块数据集 - 所有这些都是根据基本字符分割机制进行分块的。我们的评估数据集包含248个查询 - 每个查询都包含一个'黄金块'。我们将使用称为Pass@k的指标来评估性能。Pass@k检查每个查询的前k个检索文档中是否存在'黄金文档'。在这种情况下，上下文嵌入帮助我们将Pass@10性能从~87%提高到~95%。\n",
    "\n",
    "您可以在`data/codebase_chunks.json`中找到代码文件及其块，在`data/evaluation_set.jsonl`中找到评估数据集\n",
    "\n",
    "#### 附加说明：\n",
    "\n",
    "在使用此检索方法时，提示缓存有助于管理成本。此功能目前在Anthropic的第一方API上可用，即将很快在AWS Bedrock和GCP Vertex的第三方合作伙伴环境中提供。我们知道许多客户在构建RAG解决方案时利用AWS Knowledge Bases和GCP Vertex AI API，通过一些定制，此方法可以在任一平台上使用。考虑联系Anthropic或您的AWS/GCP账户团队以获取此方面的指导！\n",
    "\n",
    "为了更容易在Bedrock上使用此方法，AWS团队为我们提供了代码，您可以使用它来实现向每个文档添加上下文的Lambda函数。如果您部署此Lambda函数，则可以在配置[Bedrock Knowledge Base](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html)时选择它作为自定义分块选项。您可以在`contextual-rag-lambda-function`中找到此代码。主要lambda函数代码在`lambda_function.py`中。\n",
    "\n",
    "## 目录\n",
    "\n",
    "1) 设置\n",
    "\n",
    "2) 基本RAG\n",
    "\n",
    "3) 上下文嵌入\n",
    "\n",
    "4) 上下文BM25\n",
    "\n",
    "5) 重新排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置\n",
    "\n",
    "在开始本指南之前，请确保您具备：\n",
    "\n",
    "**技术技能：**\n",
    "- 中级Python编程\n",
    "- 对RAG（检索增强生成）的基本理解\n",
    "- 熟悉向量数据库和嵌入\n",
    "- 基本命令行熟练度\n",
    "\n",
    "**系统要求：**\n",
    "- Python 3.8+\n",
    "- 已安装并运行Docker（可选，用于BM25搜索）\n",
    "- 4GB+可用RAM\n",
    "- ~5-10GB向量数据库磁盘空间\n",
    "\n",
    "**API访问：**\n",
    "- [Anthropic API密钥](https://console.anthropic.com/)（免费套餐足够）\n",
    "- [Voyage AI API密钥](https://www.voyageai.com/)\n",
    "- [Cohere API密钥](https://cohere.com/)\n",
    "\n",
    "**时间和成本：**\n",
    "- 预计完成时间：30-45分钟\n",
    "- API成本：在整个数据集上运行约需$5-10\n",
    "\n",
    "### 库\n",
    "\n",
    "我们需要一些库，包括：\n",
    "\n",
    "1) `anthropic` - 与Claude交互\n",
    "\n",
    "2) `voyageai` - 生成高质量嵌入\n",
    "\n",
    "3) `cohere` - 用于重新排序\n",
    "\n",
    "4) `elasticsearch` 用于高性能BM25搜索\n",
    "\n",
    "3) `pandas`、`numpy`、`matplotlib`和`scikit-learn`用于数据操作和可视化\n",
    "\n",
    "### 环境变量\n",
    "\n",
    "确保设置了以下环境变量：\n",
    "\n",
    "```\n",
    "- VOYAGE_API_KEY\n",
    "- ANTHROPIC_API_KEY\n",
    "- COHERE_API_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade anthropic voyageai cohere elasticsearch pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们提前定义模型名称，以便在新模型发布时更容易更改模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"claude-haiku-4-5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将通过初始化Anthropic客户端开始，我们将使用它来生成上下文描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(\n",
    "    # 这是默认值，可以省略\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化向量数据库类\n",
    "\n",
    "我们将创建一个VectorDB类来处理嵌入存储和相似性搜索。该类在我们的RAG管道中发挥三个关键功能：\n",
    "\n",
    "1. **嵌入生成**：使用Voyage AI的嵌入模型将文本块转换为向量表示\n",
    "2. **存储和缓存**：将嵌入保存到磁盘以避免重新计算（节省时间和API成本）\n",
    "3. **相似性搜索**：使用余弦相似性检索与给定查询最相关的块\n",
    "\n",
    "对于本指南，我们使用一个简单的内存向量数据库与pickle序列化。这使得代码易于理解且不需要外部依赖。该类在生成后自动将嵌入保存到磁盘，因此您只需支付一次嵌入成本。\n",
    "\n",
    "对于生产使用，请考虑托管向量数据库解决方案。\n",
    "\n",
    "下面的VectorDB类遵循与您在生产解决方案中使用的相同接口模式，使其易于稍后交换。主要功能包括批处理（一次128块）、使用tqdm进行进度跟踪，以及查询缓存在评估期间加速重复搜索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name: str, api_key=None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, dataset: List[Dict[str, Any]]):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc[\"chunks\"]) for doc in dataset)\n",
    "\n",
    "        with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "            for doc in dataset:\n",
    "                for chunk in doc[\"chunks\"]:\n",
    "                    texts_to_embed.append(chunk[\"content\"])\n",
    "                    metadata.append(\n",
    "                        {\n",
    "                            \"doc_id\": doc[\"doc_id\"],\n",
    "                            \"original_uuid\": doc[\"original_uuid\"],\n",
    "                            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                            \"original_index\": chunk[\"original_index\"],\n",
    "                            \"content\": chunk[\"content\"],\n",
    "                        }\n",
    "                    )\n",
    "                    pbar.update(1)\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "\n",
    "        print(f\"Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
    "\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        with tqdm(total=len(texts), desc=\"Embedding chunks\") as pbar:\n",
    "            result = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i : i + batch_size]\n",
    "                batch_result = self.client.embed(batch, model=\"voyage-2\").embeddings\n",
    "                result.extend(batch_result)\n",
    "                pbar.update(len(batch))\n",
    "\n",
    "        self.embeddings = result\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "        top_results = []\n",
    "        for idx in top_indices:\n",
    "            result = {\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(similarities[idx]),\n",
    "            }\n",
    "            top_results.append(result)\n",
    "\n",
    "        return top_results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\n",
    "                \"Vector database file not found. Use load_data to create a new database.\"\n",
    "            )\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以使用此类来加载我们的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载您转换后的数据集\n",
    "with open(\"data/codebase_chunks.json\", \"r\") as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# 初始化VectorDB\n",
    "base_db = VectorDB(\"base_db\")\n",
    "\n",
    "# 加载和处理数据\n",
    "base_db.load_data(transformed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本RAG\n",
    "\n",
    "首先，我们将使用一个简单的方法设置基本的RAG管道。这在业界有时被称为'朴素RAG'。基本RAG管道包括以下3个步骤：\n",
    "\n",
    "1) 按标题分块文档 - 仅包含每个子标题的内容\n",
    "\n",
    "2) 嵌入每个文档\n",
    "\n",
    "3) 使用余弦相似性检索文档以回答查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"加载JSONL文件并返回字典列表。\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20\n",
    ") -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "\n",
    "    for query_item in tqdm(queries, desc=\"评估检索\"):\n",
    "        query = query_item[\"query\"]\n",
    "        golden_chunk_uuids = query_item[\"golden_chunk_uuids\"]\n",
    "\n",
    "        # 查找所有黄金块内容\n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next(\n",
    "                (doc for doc in query_item[\"golden_documents\"] if doc[\"uuid\"] == doc_uuid), None\n",
    "            )\n",
    "            if not golden_doc:\n",
    "                print(f\"警告：未找到UUID为{doc_uuid}的黄金文档\")\n",
    "                continue\n",
    "\n",
    "            golden_chunk = next(\n",
    "                (chunk for chunk in golden_doc[\"chunks\"] if chunk[\"index\"] == chunk_index), None\n",
    "            )\n",
    "            if not golden_chunk:\n",
    "                print(\n",
    "                    f\"警告：在文档{doc_uuid}中未找到索引{chunk_index}的黄金块\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            golden_contents.append(golden_chunk[\"content\"].strip())\n",
    "\n",
    "        if not golden_contents:\n",
    "            print(f\"警告：未找到查询的黄金内容: {query}\")\n",
    "            continue\n",
    "\n",
    "        retrieved_docs = retrieval_function(query, db, k=k)\n",
    "\n",
    "        # 计算前k个检索文档中有多少黄金块\n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = (\n",
    "                    doc[\"metadata\"]\n",
    "                    .get(\"original_content\", doc[\"metadata\"].get(\"content\", \"\"))\n",
    "                    .strip()\n",
    "                )\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "\n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "\n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\"pass_at_n\": pass_at_n, \"average_score\": average_score, \"total_queries\": total_queries}\n",
    "\n",
    "\n",
    "def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    使用VectorDB或ContextualVectorDB检索相关文档。\n",
    "\n",
    "    :param query: 查询字符串\n",
    "    :param db: VectorDB或ContextualVectorDB实例\n",
    "    :param k: 检索的顶部结果数\n",
    "    :return: 检索文档列表\n",
    "    \"\"\"\n",
    "    return db.search(query, k=k)\n",
    "\n",
    "\n",
    "def evaluate_db(db, original_jsonl_path: str, k):\n",
    "    # 加载查询和基本事实的原始JSONL数据\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "\n",
    "    # 评估检索\n",
    "    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_and_display(db, jsonl_path: str, k_values: List[int] = [5, 10, 20], db_name: str = \"\"):\n",
    "    \"\"\"\n",
    "    在多个k值上评估检索性能并显示格式化结果。\n",
    "\n",
    "    Args:\n",
    "        db: 向量数据库实例 (VectorDB或ContextualVectorDB)\n",
    "        jsonl_path: 评估数据集路径\n",
    "        k_values: 要评估的k值列表 (默认: [5, 10, 20])\n",
    "        db_name: 被评估数据库的可选名称\n",
    "\n",
    "    Returns:\n",
    "        将k值映射到其结果的字典\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    if db_name:\n",
    "        print(f\"评估结果: {db_name}\")\n",
    "    else:\n",
    "        print(\"评估结果\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"评估Pass@{k}...\")\n",
    "        results[k] = evaluate_db(db, jsonl_path, k)\n",
    "        print()  # 在评估之间添加间距\n",
    "\n",
    "    # 打印汇总表\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"{'指标':<15} {'通过率':<15} {'分数':<15}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    for k in k_values:\n",
    "        pass_rate = f\"{results[k]['pass_at_n']:.2f}%\"\n",
    "        score = f\"{results[k]['average_score']:.4f}\"\n",
    "        print(f\"{'Pass@' + str(k):<15} {pass_rate:<15} {score:<15}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们通过评估基本RAG系统来建立基线性能。我们将在k=5、10和20进行测试，以查看有多少黄金块出现在检索结果的前面。这为我们提供了衡量改进的基准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_and_display(\n",
    "    base_db, \"data/evaluation_set.jsonl\", k_values=[5, 10, 20], db_name=\"基线RAG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些结果显示了我们的基线RAG性能。系统在顶部5个结果中成功检索正确块的81%，在顶部10个结果中提高到87%，在顶部20个结果中达到90%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上下文嵌入\n",
    "\n",
    "对于基本RAG，单独的块在孤立嵌入时通常缺乏足够的上下文。上下文嵌入通过使用Claude生成一个简短的描述来\"定位\"每个块在源文档中的位置来解决这个问题。然后我们将块与此上下文一起嵌入，创建更丰富的向量表示。\n",
    "\n",
    "对于我们代码库数据集中的每个块，我们将块及其完整源文件都传递给Claude。Claude生成一个简明解释，说明块包含什么以及它在整体文件中的位置。此上下文在嵌入之前被预置到块上。\n",
    "\n",
    "### 成本和延迟考虑\n",
    "\n",
    "**何时产生此成本？** 上下文化在摄取时发生一次，而不是在每次查询时发生。与像HyDE（假设文档嵌入）这样的技术不同，后者为每个搜索增加延迟，上下文嵌入在构建向量数据库时是一次性成本。提示缓存使其实用。由于我们按顺序处理来自同一文档的所有块，我们可以利用提示缓存来节省大量成本。\n",
    "\n",
    "1. 第一个块：我们将完整文档写入缓存（支付少量溢价）\n",
    "2. 后续块：从缓存读取文档（这些令牌90%折扣）\n",
    "3. 缓存持续5分钟，有足够时间处理文档中的所有块\n",
    "\n",
    "**成本示例**：对于8k-token文档中800-token块和100令牌生成上下文，总成本为每百万文档令牌$1.02。当您运行下面的代码时，您将在日志中看到缓存节省。\n",
    "\n",
    "**注意：** 一些嵌入模型有固定的输入令牌限制。如果您看到上下文嵌入的性能更差，您的上下文块可能被截断 - 考虑使用具有更大上下文窗口的嵌入模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "让我们通过为单个块生成上下文来看一个上下文嵌入如何工作的示例。我们将使用Claude来创建定位上下文，您还将看到提示缓存指标的实际效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\n",
    "                            \"type\": \"ephemeral\"\n",
    "                        },  # 我们将对完整文档利用提示缓存\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "jsonl_data = load_jsonl(\"data/evaluation_set.jsonl\")\n",
    "# 示例用法\n",
    "doc_content = jsonl_data[0][\"golden_documents\"][0][\"content\"]\n",
    "chunk_content = jsonl_data[0][\"golden_chunks\"][0][\"content\"]\n",
    "\n",
    "response = situate_context(doc_content, chunk_content)\n",
    "print(f\"Situated context: {response.content[0].text}\")\n",
    "print(\"-\" * 10)\n",
    "# 打印缓存性能指标\n",
    "print(f\"输入令牌: {response.usage.input_tokens}\")\n",
    "print(f\"输出令牌: {response.usage.output_tokens}\")\n",
    "print(f\"缓存创建输入令牌: {response.usage.cache_creation_input_tokens}\")\n",
    "print(f\"缓存读取输入令牌: {response.usage.cache_read_input_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建上下文向量数据库\n",
    "\n",
    "现在我们已经了解了如何为单个块生成上下文描述，让我们将其扩展到处理整个数据集。下面的`ContextualVectorDB`类扩展了我们的基本`VectorDB`，在摄取期间自动进行上下文化。\n",
    "\n",
    "**主要功能：**\n",
    "\n",
    "- **并行处理**：使用ThreadPoolExecutor同时对多个块进行上下文化（可配置线程数）\n",
    "- **自动提示缓存**：逐文档处理块以最大化缓存命中\n",
    "- **令牌跟踪**：监控缓存性能并计算实际成本节省\n",
    "- **持久存储**：将嵌入和上下文化元数据保存到磁盘\n",
    "\n",
    "当您运行此程序时，请注意令牌使用统计 - 您将看到70-80%的输入令牌来自缓存，展示了提示缓存的巨大成本节省。在我们737块的数据集上，这将原本约15美元的摄取作业成本降至约3美元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import anthropic\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "class ContextualVectorDB:\n",
    "    def __init__(self, name: str, voyage_api_key=None, anthropic_api_key=None):\n",
    "        if voyage_api_key is None:\n",
    "            voyage_api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        if anthropic_api_key is None:\n",
    "            anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "        self.voyage_client = voyageai.Client(api_key=voyage_api_key)\n",
    "        self.anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/contextual_vector_db.pkl\"\n",
    "\n",
    "        self.token_counts = {\"input\": 0, \"output\": 0, \"cache_read\": 0, \"cache_creation\": 0}\n",
    "        self.token_lock = threading.Lock()\n",
    "\n",
    "    def situate_context(self, doc: str, chunk: str) -> tuple[str, Any]:\n",
    "        DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "        <document>\n",
    "        {doc_content}\n",
    "        </document>\n",
    "        \"\"\"\n",
    "\n",
    "        CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "        Here is the chunk we want to situate within the whole document\n",
    "        <chunk>\n",
    "        {chunk_content}\n",
    "        </chunk>\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "        Answer only with the succinct context and nothing else.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.anthropic_client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                            \"cache_control\": {\n",
    "                                \"type\": \"ephemeral\"\n",
    "                            },  # we will make use of prompt caching for the full documents\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n",
    "        )\n",
    "        return response.content[0].text, response.usage\n",
    "\n",
    "    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 1):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc[\"chunks\"]) for doc in dataset)\n",
    "\n",
    "        def process_chunk(doc, chunk):\n",
    "            # for each chunk, produce the context\n",
    "            contextualized_text, usage = self.situate_context(doc[\"content\"], chunk[\"content\"])\n",
    "            with self.token_lock:\n",
    "                self.token_counts[\"input\"] += usage.input_tokens\n",
    "                self.token_counts[\"output\"] += usage.output_tokens\n",
    "                self.token_counts[\"cache_read\"] += usage.cache_read_input_tokens\n",
    "                self.token_counts[\"cache_creation\"] += usage.cache_creation_input_tokens\n",
    "\n",
    "            return {\n",
    "                # append the context to the original text chunk\n",
    "                \"text_to_embed\": f\"{chunk['content']}\\n\\n{contextualized_text}\",\n",
    "                \"metadata\": {\n",
    "                    \"doc_id\": doc[\"doc_id\"],\n",
    "                    \"original_uuid\": doc[\"original_uuid\"],\n",
    "                    \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                    \"original_index\": chunk[\"original_index\"],\n",
    "                    \"original_content\": chunk[\"content\"],\n",
    "                    \"contextualized_content\": contextualized_text,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        print(f\"Processing {total_chunks} chunks with {parallel_threads} threads\")\n",
    "        with ThreadPoolExecutor(max_workers=parallel_threads) as executor:\n",
    "            futures = []\n",
    "            for doc in dataset:\n",
    "                for chunk in doc[\"chunks\"]:\n",
    "                    futures.append(executor.submit(process_chunk, doc, chunk))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=total_chunks, desc=\"Processing chunks\"):\n",
    "                result = future.result()\n",
    "                texts_to_embed.append(result[\"text_to_embed\"])\n",
    "                metadata.append(result[\"metadata\"])\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "\n",
    "        # logging token usage\n",
    "        print(\n",
    "            f\"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\"\n",
    "        )\n",
    "        print(f\"Total input tokens without caching: {self.token_counts['input']}\")\n",
    "        print(f\"Total output tokens: {self.token_counts['output']}\")\n",
    "        print(f\"Total input tokens written to cache: {self.token_counts['cache_creation']}\")\n",
    "        print(f\"Total input tokens read from cache: {self.token_counts['cache_read']}\")\n",
    "\n",
    "        total_tokens = (\n",
    "            self.token_counts[\"input\"]\n",
    "            + self.token_counts[\"cache_read\"]\n",
    "            + self.token_counts[\"cache_creation\"]\n",
    "        )\n",
    "        savings_percentage = (\n",
    "            (self.token_counts[\"cache_read\"] / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "        )\n",
    "        print(\n",
    "            f\"Total input token savings from prompt caching: {savings_percentage:.2f}% of all input tokens used were read from cache.\"\n",
    "        )\n",
    "        print(\"Tokens read from cache come at a 90 percent discount!\")\n",
    "\n",
    "    # we use voyage AI here for embeddings. Read more here: https://docs.voyageai.com/docs/embeddings\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.voyage_client.embed(texts[i : i + batch_size], model=\"voyage-2\").embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.voyage_client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "\n",
    "        top_results = []\n",
    "        for idx in top_indices:\n",
    "            result = {\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(similarities[idx]),\n",
    "            }\n",
    "            top_results.append(result)\n",
    "        return top_results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\n",
    "                \"Vector database file not found. Use load_data to create a new database.\"\n",
    "            )\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载转换后的数据集\n",
    "with open(\"data/codebase_chunks.json\", \"r\") as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# 初始化ContextualVectorDB\n",
    "contextual_db = ContextualVectorDB(\"my_contextual_db\")\n",
    "\n",
    "# 加载和处理数据\n",
    "# 注意：考虑增加并行线程数来更快运行此程序，或如果担心达到API速率限制而减少并行线程数\n",
    "contextual_db.load_data(transformed_dataset, parallel_threads=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些数字揭示了提示缓存对上下文嵌入的强大功能：\n",
    "\n",
    "- 我们在9个代码库文件中处理了**737个块**\n",
    "- **61.83%的输入令牌**来自缓存（2.27M令牌，90%折扣）\n",
    "- 没有缓存，这将花费输入令牌约**$9.20**\n",
    "- 有缓存，实际成本降至**$2.85**（69%节省）\n",
    "\n",
    "缓存命中率取决于每个文档包含多少块。包含更多块的文档从缓存中受益更多，因为我们只将完整文档写入缓存一次，然后为该文件中的每个块重复读取它。这就是为什么顺序处理文档（而不是随机打乱块）对于最大化缓存效率至关重要。\n",
    "\n",
    "现在让我们评估这种上下文化相比基线在多大程度上改善了我们的检索性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluation Results: Contextual Embeddings\n",
      "============================================================\n",
      "\n",
      "Evaluating Pass@5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:03<00:00, 64.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Pass@10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:03<00:00, 64.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Pass@20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:03<00:00, 64.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Metric          Pass Rate       Score          \n",
      "------------------------------------------------------------\n",
      "Pass@5          88.12%          0.8812         \n",
      "Pass@10         92.34%          0.9234         \n",
      "Pass@20         94.29%          0.9429         \n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_and_display(\n",
    "    contextual_db,\n",
    "    \"data/evaluation_set.jsonl\",\n",
    "    k_values=[5, 10, 20],\n",
    "    db_name=\"Contextual Embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过在嵌入之前为每个块添加上下文，我们在所有k值上将检索失败率降低了**~30-40%**。这意味着在您检索的顶部块中不相关的结果更少，当您将这些块传递给Claude进行最终响应生成时，会获得更好的答案。\n",
    "\n",
    "这种改进在Pass@5最明显，此时精度最重要 - 表明上下文化块不仅更常被检索，而且在相关时排名更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上下文BM25：混合搜索\n",
    "\n",
    "仅上下文嵌入就将我们的Pass@10从87%提高到92%。我们可以通过使用**上下文BM25**结合语义搜索和基于关键字的搜索来进一步推动性能 - 一种进一步降低检索失败率的混合方法。\n",
    "\n",
    "### 为什么使用混合搜索？\n",
    "\n",
    "语义搜索擅长理解和上下文，但可能错过精确的关键字匹配。BM25（概率关键字排名算法）擅长查找特定术语，但缺乏语义理解。通过结合两者，我们获得了两全其美的效果：\n",
    "\n",
    "- **语义搜索**：捕获概念相似性和释义\n",
    "- **BM25**：捕获精确术语、函数名称和特定短语\n",
    "- **倒数排名融合**：智能合并来自两个源的结果\n",
    "\n",
    "### 什么是BM25？\n",
    "\n",
    "BM25是一种概率排名函数，通过考虑文档长度和术语饱和来改进TF-IDF。它广泛应用于生产搜索引擎（包括Elasticsearch），因其有效性而用于排名关键字相关性。有关技术细节，请参阅[这篇博客文章](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables)。\n",
    "\n",
    "我们不仅搜索原始块内容，还搜索我们之前生成的块*和*上下文描述。这意味着BM25可以匹配原始文本或解释上下文中的关键字。\n",
    "\n",
    "### 设置：运行Elasticsearch\n",
    "\n",
    "在运行下面的代码之前，您需要本地运行Elasticsearch。最简单的方法是通过Docker：\n",
    "\n",
    "```bash\n",
    "docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 \\\n",
    "  -e \"discovery.type=single-node\" \\\n",
    "  -e \"xpack.security.enabled=false\" \\\n",
    "  elasticsearch:9.2.0\n",
    "```\n",
    "\n",
    "## 故障排除：\n",
    "- 验证它正在运行：docker ps | grep elasticsearch\n",
    "- 如果端口9200被占用：docker stop elasticsearch && docker rm elasticsearch\n",
    "- 如果出现问题检查日志：docker logs elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合搜索如何工作\n",
    "\n",
    "下面的retrieve_advanced函数实现了三步过程：\n",
    "\n",
    "1. 检索候选：从语义搜索和BM25获取前150个结果\n",
    "2. 分数融合：使用加权倒数排名融合组合排名\n",
    "   - 默认：语义搜索80%权重，BM25 20%权重\n",
    "   - 这些权重是可调的 - 实验以优化您的用例\n",
    "3. 返回top-k：选择融合后得分最高的结果\n",
    "\n",
    "权重系统让您可以根据数据特征在语义理解和关键字精度之间平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "\n",
    "class ElasticsearchBM25:\n",
    "    def __init__(self, index_name: str = \"contextual_bm25_index\"):\n",
    "        self.es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "        self.index_name = index_name\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        index_settings = {\n",
    "            \"settings\": {\n",
    "                \"analysis\": {\"analyzer\": {\"default\": {\"type\": \"english\"}}},\n",
    "                \"similarity\": {\"default\": {\"type\": \"BM25\"}},\n",
    "                \"index.queries.cache.enabled\": False,\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                    \"contextualized_content\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                    \"doc_id\": {\"type\": \"keyword\", \"index\": False},\n",
    "                    \"chunk_id\": {\"type\": \"keyword\", \"index\": False},\n",
    "                    \"original_index\": {\"type\": \"integer\", \"index\": False},\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Change this line - remove 'body=' parameter\n",
    "        if not self.es_client.indices.exists(index=self.index_name):\n",
    "            self.es_client.indices.create(\n",
    "                index=self.index_name,\n",
    "                settings=index_settings[\"settings\"],\n",
    "                mappings=index_settings[\"mappings\"],\n",
    "            )\n",
    "            print(f\"Created index: {self.index_name}\")\n",
    "\n",
    "    def index_documents(self, documents: List[Dict[str, Any]]):\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"content\": doc[\"original_content\"],\n",
    "                    \"contextualized_content\": doc[\"contextualized_content\"],\n",
    "                    \"doc_id\": doc[\"doc_id\"],\n",
    "                    \"chunk_id\": doc[\"chunk_id\"],\n",
    "                    \"original_index\": doc[\"original_index\"],\n",
    "                },\n",
    "            }\n",
    "            for doc in documents\n",
    "        ]\n",
    "        success, _ = bulk(self.es_client, actions)\n",
    "        self.es_client.indices.refresh(index=self.index_name)\n",
    "        return success\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        self.es_client.indices.refresh(index=self.index_name)\n",
    "\n",
    "        # Change this - remove 'body=' and pass query directly\n",
    "        response = self.es_client.search(\n",
    "            index=self.index_name,\n",
    "            query={\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"content\", \"contextualized_content\"],\n",
    "                }\n",
    "            },\n",
    "            size=k,\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"doc_id\": hit[\"_source\"][\"doc_id\"],\n",
    "                \"original_index\": hit[\"_source\"][\"original_index\"],\n",
    "                \"content\": hit[\"_source\"][\"content\"],\n",
    "                \"contextualized_content\": hit[\"_source\"][\"contextualized_content\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "            for hit in response[\"hits\"][\"hits\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "def create_elasticsearch_bm25_index(db: ContextualVectorDB):\n",
    "    es_bm25 = ElasticsearchBM25()\n",
    "    es_bm25.index_documents(db.metadata)\n",
    "    return es_bm25\n",
    "\n",
    "\n",
    "def retrieve_advanced(\n",
    "    query: str,\n",
    "    db: ContextualVectorDB,\n",
    "    es_bm25: ElasticsearchBM25,\n",
    "    k: int,\n",
    "    semantic_weight: float = 0.8,\n",
    "    bm25_weight: float = 0.2,\n",
    "):\n",
    "    num_chunks_to_recall = 150\n",
    "\n",
    "    # Semantic search\n",
    "    semantic_results = db.search(query, k=num_chunks_to_recall)\n",
    "    ranked_chunk_ids = [\n",
    "        (result[\"metadata\"][\"doc_id\"], result[\"metadata\"][\"original_index\"])\n",
    "        for result in semantic_results\n",
    "    ]\n",
    "\n",
    "    # BM25 search using Elasticsearch\n",
    "    bm25_results = es_bm25.search(query, k=num_chunks_to_recall)\n",
    "    ranked_bm25_chunk_ids = [\n",
    "        (result[\"doc_id\"], result[\"original_index\"]) for result in bm25_results\n",
    "    ]\n",
    "\n",
    "    # Combine results\n",
    "    chunk_ids = list(set(ranked_chunk_ids + ranked_bm25_chunk_ids))\n",
    "    chunk_id_to_score = {}\n",
    "\n",
    "    # Initial scoring with weights\n",
    "    for chunk_id in chunk_ids:\n",
    "        score = 0\n",
    "        if chunk_id in ranked_chunk_ids:\n",
    "            index = ranked_chunk_ids.index(chunk_id)\n",
    "            score += semantic_weight * (1 / (index + 1))  # Weighted 1/n scoring for semantic\n",
    "        if chunk_id in ranked_bm25_chunk_ids:\n",
    "            index = ranked_bm25_chunk_ids.index(chunk_id)\n",
    "            score += bm25_weight * (1 / (index + 1))  # Weighted 1/n scoring for BM25\n",
    "        chunk_id_to_score[chunk_id] = score\n",
    "\n",
    "    # Sort chunk IDs by their scores in descending order\n",
    "    sorted_chunk_ids = sorted(\n",
    "        chunk_id_to_score.keys(), key=lambda x: (chunk_id_to_score[x], x[0], x[1]), reverse=True\n",
    "    )\n",
    "\n",
    "    # Assign new scores based on the sorted order\n",
    "    for index, chunk_id in enumerate(sorted_chunk_ids):\n",
    "        chunk_id_to_score[chunk_id] = 1 / (index + 1)\n",
    "\n",
    "    # Prepare the final results\n",
    "    final_results = []\n",
    "    semantic_count = 0\n",
    "    bm25_count = 0\n",
    "    for chunk_id in sorted_chunk_ids[:k]:\n",
    "        chunk_metadata = next(\n",
    "            chunk\n",
    "            for chunk in db.metadata\n",
    "            if chunk[\"doc_id\"] == chunk_id[0] and chunk[\"original_index\"] == chunk_id[1]\n",
    "        )\n",
    "        is_from_semantic = chunk_id in ranked_chunk_ids\n",
    "        is_from_bm25 = chunk_id in ranked_bm25_chunk_ids\n",
    "        final_results.append(\n",
    "            {\n",
    "                \"chunk\": chunk_metadata,\n",
    "                \"score\": chunk_id_to_score[chunk_id],\n",
    "                \"from_semantic\": is_from_semantic,\n",
    "                \"from_bm25\": is_from_bm25,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if is_from_semantic and not is_from_bm25:\n",
    "            semantic_count += 1\n",
    "        elif is_from_bm25 and not is_from_semantic:\n",
    "            bm25_count += 1\n",
    "        else:  # it's in both\n",
    "            semantic_count += 0.5\n",
    "            bm25_count += 0.5\n",
    "\n",
    "    return final_results, semantic_count, bm25_count\n",
    "\n",
    "\n",
    "def evaluate_db_advanced(\n",
    "    db: ContextualVectorDB,\n",
    "    original_jsonl_path: str,\n",
    "    k_values: List[int] = [5, 10, 20],\n",
    "    db_name: str = \"Hybrid Search\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate hybrid search (semantic + BM25) at multiple k values with formatted results.\n",
    "\n",
    "    Args:\n",
    "        db: ContextualVectorDB instance\n",
    "        original_jsonl_path: Path to evaluation dataset\n",
    "        k_values: List of k values to evaluate (default: [5, 10, 20])\n",
    "        db_name: Name for the evaluation display\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping k values to their results and source breakdowns\n",
    "    \"\"\"\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    es_bm25 = create_elasticsearch_bm25_index(db)\n",
    "    results = {}\n",
    "\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"Evaluation Results: {db_name}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    try:\n",
    "        # Warm-up queries\n",
    "        warm_up_queries = original_data[:10]\n",
    "        for query_item in warm_up_queries:\n",
    "            _ = retrieve_advanced(query_item[\"query\"], db, es_bm25, k_values[0])\n",
    "\n",
    "        for k in k_values:\n",
    "            print(f\"Evaluating Pass@{k}...\")\n",
    "\n",
    "            total_score = 0\n",
    "            total_semantic_count = 0\n",
    "            total_bm25_count = 0\n",
    "            total_results = 0\n",
    "\n",
    "            for query_item in tqdm(original_data, desc=f\"Pass@{k}\"):\n",
    "                query = query_item[\"query\"]\n",
    "                golden_chunk_uuids = query_item[\"golden_chunk_uuids\"]\n",
    "\n",
    "                golden_contents = []\n",
    "                for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "                    golden_doc = next(\n",
    "                        (doc for doc in query_item[\"golden_documents\"] if doc[\"uuid\"] == doc_uuid),\n",
    "                        None,\n",
    "                    )\n",
    "                    if golden_doc:\n",
    "                        golden_chunk = next(\n",
    "                            (\n",
    "                                chunk\n",
    "                                for chunk in golden_doc[\"chunks\"]\n",
    "                                if chunk[\"index\"] == chunk_index\n",
    "                            ),\n",
    "                            None,\n",
    "                        )\n",
    "                        if golden_chunk:\n",
    "                            golden_contents.append(golden_chunk[\"content\"].strip())\n",
    "\n",
    "                if not golden_contents:\n",
    "                    continue\n",
    "\n",
    "                retrieved_docs, semantic_count, bm25_count = retrieve_advanced(\n",
    "                    query, db, es_bm25, k\n",
    "                )\n",
    "\n",
    "                chunks_found = 0\n",
    "                for golden_content in golden_contents:\n",
    "                    for doc in retrieved_docs[:k]:\n",
    "                        retrieved_content = doc[\"chunk\"][\"original_content\"].strip()\n",
    "                        if retrieved_content == golden_content:\n",
    "                            chunks_found += 1\n",
    "                            break\n",
    "\n",
    "                query_score = chunks_found / len(golden_contents)\n",
    "                total_score += query_score\n",
    "\n",
    "                total_semantic_count += semantic_count\n",
    "                total_bm25_count += bm25_count\n",
    "                total_results += len(retrieved_docs)\n",
    "\n",
    "            total_queries = len(original_data)\n",
    "            average_score = total_score / total_queries\n",
    "            pass_at_n = average_score * 100\n",
    "\n",
    "            semantic_percentage = (\n",
    "                (total_semantic_count / total_results) * 100 if total_results > 0 else 0\n",
    "            )\n",
    "            bm25_percentage = (total_bm25_count / total_results) * 100 if total_results > 0 else 0\n",
    "\n",
    "            results[k] = {\n",
    "                \"pass_at_n\": pass_at_n,\n",
    "                \"average_score\": average_score,\n",
    "                \"total_queries\": total_queries,\n",
    "                \"semantic_percentage\": semantic_percentage,\n",
    "                \"bm25_percentage\": bm25_percentage,\n",
    "            }\n",
    "\n",
    "            print(f\"Pass@{k}: {pass_at_n:.2f}%\")\n",
    "            print(f\"Semantic: {semantic_percentage:.1f}% | BM25: {bm25_percentage:.1f}%\\n\")\n",
    "\n",
    "        # Print summary table\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"{'Metric':<12} {'Pass Rate':<12} {'Score':<12} {'Semantic':<12} {'BM25':<12}\")\n",
    "        print(f\"{'-' * 70}\")\n",
    "        for k in k_values:\n",
    "            r = results[k]\n",
    "            print(\n",
    "                f\"{'Pass@' + str(k):<12} {r['pass_at_n']:>10.2f}% {r['average_score']:>10.4f} \"\n",
    "                f\"{r['semantic_percentage']:>10.1f}% {r['bm25_percentage']:>10.1f}%\"\n",
    "            )\n",
    "        print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    finally:\n",
    "        # Delete the Elasticsearch index\n",
    "        if es_bm25.es_client.indices.exists(index=es_bm25.index_name):\n",
    "            es_bm25.es_client.indices.delete(index=es_bm25.index_name)\n",
    "            print(f\"Deleted Elasticsearch index: {es_bm25.index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: contextual_bm25_index\n",
      "======================================================================\n",
      "Evaluation Results: Contextual BM25 Hybrid Search\n",
      "======================================================================\n",
      "\n",
      "Evaluating Pass@5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@5: 100%|██████████| 248/248 [00:05<00:00, 41.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 88.86%\n",
      "Semantic: 54.6% | BM25: 45.4%\n",
      "\n",
      "Evaluating Pass@10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@10: 100%|██████████| 248/248 [00:05<00:00, 42.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 92.31%\n",
      "Semantic: 57.6% | BM25: 42.4%\n",
      "\n",
      "Evaluating Pass@20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@20: 100%|██████████| 248/248 [00:05<00:00, 42.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 95.23%\n",
      "Semantic: 60.8% | BM25: 39.2%\n",
      "\n",
      "======================================================================\n",
      "Metric       Pass Rate    Score        Semantic     BM25        \n",
      "----------------------------------------------------------------------\n",
      "Pass@5            88.86%     0.8886       54.6%       45.4%\n",
      "Pass@10           92.31%     0.9231       57.6%       42.4%\n",
      "Pass@20           95.23%     0.9523       60.8%       39.2%\n",
      "======================================================================\n",
      "\n",
      "Deleted Elasticsearch index: contextual_bm25_index\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_db_advanced(\n",
    "    contextual_db,\n",
    "    \"data/evaluation_set.jsonl\",\n",
    "    k_values=[5, 10, 20],\n",
    "    db_name=\"Contextual BM25 Hybrid Search\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新排序\n",
    "\n",
    "我们通过混合搜索取得了强劲的结果（93.21% Pass@10），但还有一种技术可以挤出额外的性能：**重新排序**。\n",
    "\n",
    "### 什么是重新排序？\n",
    "\n",
    "重新排序是一种两阶段检索方法：\n",
    "\n",
    "1. **阶段1 - 广泛检索**：通过检索比您需要的更多候选来撒大网（例如，检索100个块）\n",
    "2. **阶段2 - 精确选择**：使用专门的重新排序模型对这些候选进行评分，只选择top-k最相关的\n",
    "\n",
    "**为什么这有效？** 初始检索方法（嵌入、BM25）针对跨数百万文档的速度进行了优化。重新排序模型较慢但更准确 - 它们可以对较小的候选集进行更深入的分析。这创造了一个速度/精度权衡，在实践中效果很好。\n",
    "\n",
    "### 我们的重新排序方法\n",
    "\n",
    "对于这个例子，我们将使用一个更简单的重新排序管道，仅基于上下文嵌入（不是完整的混合搜索）。这是过程：\n",
    "\n",
    "1. **过度检索**：获取比需要多10倍的结果（例如，当我们需要10个时检索100个块）\n",
    "2. **使用Cohere重新排序**：使用Cohere的`rerank-english-v3.0`模型对所有候选进行评分\n",
    "3. **选择top-k**：只返回得分最高的结果\n",
    "\n",
    "重新排序模型可以访问原始块内容和我们生成的上下文描述，使其具有做出精确相关性判断的丰富信息。\n",
    "\n",
    "### 预期性能\n",
    "\n",
    "添加重新排序可带来适度但有意义的改进：\n",
    "- **没有重新排序**：92.34% Pass@10（仅上下文嵌入）\n",
    "- **有重新排序**：~95% Pass@10（额外2-3%增益）\n",
    "\n",
    "这看起来可能很小，但在生产系统中，将失败率从7.66%降低到~5%可以显著改善用户体验。权衡是查询延迟 - 重新排序根据候选集大小每查询增加~100-200ms延迟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from typing import List, Dict, Any, Callable\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_db_rerank(\n",
    "    db, original_jsonl_path: str, k_values: List[int] = [5, 10, 20], db_name: str = \"Reranking\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate reranking performance at multiple k values with formatted results.\n",
    "\n",
    "    Args:\n",
    "        db: ContextualVectorDB instance\n",
    "        original_jsonl_path: Path to evaluation dataset\n",
    "        k_values: List of k values to evaluate (default: [5, 10, 20])\n",
    "        db_name: Name for the evaluation display\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping k values to their results\n",
    "    \"\"\"\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "    results = {}\n",
    "\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"Evaluation Results: {db_name}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"Evaluating Pass@{k} with reranking...\")\n",
    "\n",
    "        total_score = 0\n",
    "        total_queries = len(original_data)\n",
    "\n",
    "        for query_item in tqdm(original_data, desc=f\"Pass@{k}\"):\n",
    "            query = query_item[\"query\"]\n",
    "            golden_chunk_uuids = query_item[\"golden_chunk_uuids\"]\n",
    "\n",
    "            # Find golden contents\n",
    "            golden_contents = []\n",
    "            for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "                golden_doc = next(\n",
    "                    (doc for doc in query_item[\"golden_documents\"] if doc[\"uuid\"] == doc_uuid), None\n",
    "                )\n",
    "                if golden_doc:\n",
    "                    golden_chunk = next(\n",
    "                        (chunk for chunk in golden_doc[\"chunks\"] if chunk[\"index\"] == chunk_index),\n",
    "                        None,\n",
    "                    )\n",
    "                    if golden_chunk:\n",
    "                        golden_contents.append(golden_chunk[\"content\"].strip())\n",
    "\n",
    "            if not golden_contents:\n",
    "                continue\n",
    "\n",
    "            # Retrieve and rerank\n",
    "            semantic_results = db.search(query, k=k * 10)\n",
    "\n",
    "            # Prepare documents for reranking\n",
    "            documents = [\n",
    "                f\"{res['metadata']['original_content']}\\n\\nContext: {res['metadata']['contextualized_content']}\"\n",
    "                for res in semantic_results\n",
    "            ]\n",
    "\n",
    "            # Rerank\n",
    "            rerank_response = co.rerank(\n",
    "                model=\"rerank-english-v3.0\", query=query, documents=documents, top_n=k\n",
    "            )\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "\n",
    "            # Get final results\n",
    "            retrieved_docs = []\n",
    "            for r in rerank_response.results:\n",
    "                original_result = semantic_results[r.index]\n",
    "                retrieved_docs.append(\n",
    "                    {\"chunk\": original_result[\"metadata\"], \"score\": r.relevance_score}\n",
    "                )\n",
    "\n",
    "            # Check if golden chunks are in results\n",
    "            chunks_found = 0\n",
    "            for golden_content in golden_contents:\n",
    "                for doc in retrieved_docs[:k]:\n",
    "                    retrieved_content = doc[\"chunk\"][\"original_content\"].strip()\n",
    "                    if retrieved_content == golden_content:\n",
    "                        chunks_found += 1\n",
    "                        break\n",
    "\n",
    "            query_score = chunks_found / len(golden_contents)\n",
    "            total_score += query_score\n",
    "\n",
    "        average_score = total_score / total_queries\n",
    "        pass_at_n = average_score * 100\n",
    "\n",
    "        results[k] = {\n",
    "            \"pass_at_n\": pass_at_n,\n",
    "            \"average_score\": average_score,\n",
    "            \"total_queries\": total_queries,\n",
    "        }\n",
    "\n",
    "        print(f\"Pass@{k}: {pass_at_n:.2f}%\")\n",
    "        print(f\"Average Score: {average_score:.4f}\\n\")\n",
    "\n",
    "    # Print summary table\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"{'Metric':<15} {'Pass Rate':<15} {'Score':<15}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    for k in k_values:\n",
    "        pass_rate = f\"{results[k]['pass_at_n']:.2f}%\"\n",
    "        score = f\"{results[k]['average_score']:.4f}\"\n",
    "        print(f\"{'Pass@' + str(k):<15} {pass_rate:<15} {score:<15}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluation Results: Contextual Embeddings + Reranking\n",
      "============================================================\n",
      "\n",
      "Evaluating Pass@5 with reranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@5: 100%|██████████| 248/248 [01:40<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 92.15%\n",
      "Average Score: 0.9215\n",
      "\n",
      "Evaluating Pass@10 with reranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@10: 100%|██████████| 248/248 [02:29<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 95.26%\n",
      "Average Score: 0.9526\n",
      "\n",
      "Evaluating Pass@20 with reranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass@20: 100%|██████████| 248/248 [03:03<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 97.45%\n",
      "Average Score: 0.9745\n",
      "\n",
      "============================================================\n",
      "Metric          Pass Rate       Score          \n",
      "------------------------------------------------------------\n",
      "Pass@5          92.15%          0.9215         \n",
      "Pass@10         95.26%          0.9526         \n",
      "Pass@20         97.45%          0.9745         \n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_db_rerank(\n",
    "    contextual_db,\n",
    "    \"data/evaluation_set.jsonl\",\n",
    "    k_values=[5, 10, 20],\n",
    "    db_name=\"Contextual Embeddings + Reranking\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新排序提供了我们最强的结果，几乎消除了检索失败。让我们看看每种技术如何建立在之前的基础上来实现这种改进。\n",
    "\n",
    "从我们基线RAG系统的87% Pass@10开始，我们通过系统地应用先进的检索技术攀升到95%以上。每种方法都解决了不同的弱点：上下文嵌入解决了\"孤立块\"问题，混合搜索捕获嵌入错过的关键字特定查询，重新排序应用更复杂的相关性评分来优化最终选择。\n",
    "\n",
    "| 方法 | Pass@5 | Pass@10 | Pass@20 |\n",
    "|----------|--------|---------|---------|\n",
    "| **基线RAG** | 80.92% | 87.15% | 90.06% |\n",
    "| **+ 上下文嵌入** | 88.12% | 92.34% | 94.29% |\n",
    "| **+ 混合搜索（BM25）** | 86.43% | 93.21% | 94.99% |\n",
    "| **+ 重新排序** | 92.15% | 95.26% | 97.45% |\n",
    "\n",
    "**关键要点：**\n",
    "\n",
    "1. **上下文嵌入提供了最大的单一改进**（+5-7个百分点），验证了向块添加文档级上下文显著提高检索质量。这种技术 alone就能让您获得90%的最佳性能。\n",
    "\n",
    "2. **重新排序达到最高绝对性能**，达到95.26% Pass@10 - 意味着95%查询的正确块出现在前10个结果中。这代表了相比基线RAG**检索失败率降低47%**（从12.85%失败率降至4.74%）。\n",
    "\n",
    "3. **权衡很重要**：每种技术都增加复杂性和成本：\n",
    "   - 上下文嵌入：一次性摄取成本（使用提示缓存此数据集约需3美元）\n",
    "   - 混合搜索：需要Elasticsearch基础设施和维护\n",
    "   - 重新排序：增加100-200ms查询延迟和每查询API成本（约$0.002每查询）\n",
    "\n",
    "4. **根据您的需求选择方法**：\n",
    "   - **高容量、成本敏感**：仅上下文嵌入（92% Pass@10，无每查询成本）\n",
    "   - **最大精度、延迟容忍**：完整重新排序管道（95% Pass@10，最佳精度）\n",
    "   - **平衡生产系统**：混合搜索，无每查询成本的强大性能（93% Pass@10）\n",
    "\n",
    "对于大多数生产RAG系统，**上下文嵌入提供了最佳性能成本比**，只需一次性摄取成本即可提供92% Pass@10。当您需要额外的2-3个百分点精度并能负担额外的基础设施或查询成本时，混合搜索和重新排序可用。\n",
    "\n",
    "### 下一步和关键要点\n",
    "\n",
    "1) 我们演示了如何使用上下文嵌入来改进检索性能，然后通过上下文BM25和重新排序提供了额外的改进。\n",
    "\n",
    "2) 此示例使用代码库，但这些方法也适用于其他数据类型，如内部公司知识库、财务和法律内容、教育内容等等。\n",
    "\n",
    "3) 如果您是AWS用户，您可以从`contextual-rag-lambda-function`中的Lambda函数开始，如果您的GCP用户，您可以启动自己的Cloud Run实例并遵循类似模式！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anthropic-cookbook (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
