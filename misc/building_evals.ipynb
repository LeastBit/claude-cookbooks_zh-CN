{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 构建评估\n\n优化 Claude 以在任务上为您提供尽可能高的准确性是一门实证科学，也是一个持续改进的过程。无论您是想知道对提示词的更改是否使模型在关键指标上表现更好，还是想评估模型是否足够好以投入生产，一个良好的离线评估系统对成功至关重要。\n\n在本方法中，我们将介绍构建评估中的常见模式，以及在进行时需要遵循的有用经验法则。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 评估的组成部分\n\n评估通常有四个部分。\n- 馈送给模型的输入提示词。我们将要求 Claude 基于此提示词生成一个完成。通常当我们设计评估时，输入列将包含一组变量输入，这些输入在测试时被馈送到提示词模板中。\n- 通过我们要评估的模型运行输入提示词产生的输出。\n- 我们与之比较模型输出的\"黄金答案\"。黄金答案可能是强制性的精确匹配，也可以是完美答案的示例，旨在为评分者提供比较点以基于其进行评分。\n- 分数，由下面讨论的评分方法之一生成，代表模型在这个问题上的表现。\n\n## 评估评分方法\n\n评估中有两件事可能耗时且昂贵。第一是为评估编写问题和黄金答案。第二是评分。如果您没有可用的数据集或没有无需手动生成问题就能创建数据集的方法（考虑使用 Claude 生成您的问题！），编写问题和黄金答案可能非常耗时，但通常有一次性的固定成本的好处。您编写问题和黄金答案，并且很少需要重写它们。另一方面，评分是每次您重新运行评估时都会产生的成本——而且您可能会经常重新运行评估。因此，构建能够快速且廉价评分的评估应该是您设计选择的核心。\n\n评分评估有三种常见方式。\n- **基于代码的评分：** 这涉及使用标准代码（主要是字符串匹配和正则表达式）来评分模型的输出。常见版本包括检查与答案的精确匹配，或检查字符串是否包含某些关键短语。这是迄今最好的评分方法，如果您能设计允许这样做的评估，因为它超快且高度可靠。然而，许多评估不允许这种评分风格。\n- **人工评分：** 人类查看模型生成的答案，将其与黄金答案进行比较，并分配分数。这是最有能力的评分方法，因为它几乎可以用于任何任务，但也非常缓慢和昂贵，特别是如果您构建了大型评估。如果可以的话，您应该主要避免设计需要人工评分的评估。\n- **基于模型的评分：** 事实证明，Claude 非常有能力对自己进行评分，并且可以用于评分各种历史上可能需要人类的任务，例如创意写作中的语调分析或自由形式问答的准确性。您可以通过为 Claude 编写_评分提示词_来实现这一点。\n\n让我们通过每种评分方法的示例来了解一下。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 基于代码的评分\n\n在这里我们将评估一个要求 Claude 成功识别某物有多少条腿的评估。我们希望 Claude 只输出腿的数量，我们设计评估的方式使得我们可以使用精确匹配的基于代码的评分器。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "%pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "MODEL_NAME = \"claude-opus-4-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(animal_statement):\n",
    "    user_content = f\"\"\"You will be provided a statement about an animal and your job is to determine how many legs that animal has.\n",
    "    \n",
    "    Here is the animal statment.\n",
    "    <animal_statement>{animal_statement}</animal_statment>\n",
    "    \n",
    "    How many legs does the animal have? Return just the number of legs as an integer and nothing else.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval (in practice you might do this as a jsonl or csv file instead).\n",
    "eval = [\n",
    "    {\"animal_statement\": \"The animal is a human.\", \"golden_answer\": \"2\"},\n",
    "    {\"animal_statement\": \"The animal is a snake.\", \"golden_answer\": \"0\"},\n",
    "    {\n",
    "        \"animal_statement\": \"The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.\",\n",
    "        \"golden_answer\": \"5\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal Statement: The animal is a human.\n",
      "Golden Answer: 2\n",
      "Output: 2\n",
      "\n",
      "Animal Statement: The animal is a snake.\n",
      "Golden Answer: 0\n",
      "Output: 0\n",
      "\n",
      "Animal Statement: The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that.\n",
      "Golden Answer: 5\n",
      "Output: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(model=MODEL_NAME, max_tokens=5, messages=messages)\n",
    "    return response.content[0].text\n",
    "\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question[\"animal_statement\"])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(\n",
    "        f\"Animal Statement: {question['animal_statement']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check our completions against the golden answers.\n",
    "# Define a grader function\n",
    "def grade_completion(output, golden_answer):\n",
    "    return output == golden_answer\n",
    "\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [\n",
    "    grade_completion(output, question[\"golden_answer\"]) for output, question in zip(outputs, eval)\n",
    "]\n",
    "print(f\"Score: {sum(grades) / len(grades) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 人工评分\n\n现在让我们想象我们正在评估一个我们向 Claude 提出了一系列开放式问题的评估，也许是用于通用聊天助手。不幸的是，答案可能各不相同，这无法用代码评分。我们可以做到这一点的一种方法是人工评分。"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input prompt template for the task.\n",
    "def build_input_prompt(question):\n",
    "    user_content = f\"\"\"Please answer the following question:\n",
    "    <question>{question}</question>\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our eval. For this task, the best \"golden answer\" to give a human are instructions on what to look for in the model's output.\n",
    "eval = [\n",
    "    {\n",
    "        \"question\": \"Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.\",\n",
    "        \"golden_answer\": \"A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.\",\n",
    "        \"golden_answer\": \"A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who won the super bowl in 2024 and who did they beat?\",  # Claude should get this wrong since it comes after its training cutoff.\n",
    "        \"golden_answer\": \"A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Please design me a workout for today that features at least 50 reps of pulling leg exercises, at least 50 reps of pulling arm exercises, and ten minutes of core.\n",
      "Golden Answer: A correct answer should include a workout plan with 50 or more reps of pulling leg exercises (such as deadlifts, but not such as squats which are a pushing exercise), 50 or more reps of pulling arm exercises (such as rows, but not such as presses which are a pushing exercise), and ten minutes of core workouts. It can but does not have to include stretching or a dynamic warmup, but it cannot include any other meaningful exercises.\n",
      "Output: Here's a workout plan for today that includes at least 50 reps of pulling leg exercises, 50 reps of pulling arm exercises, and ten minutes of core:\n",
      "\n",
      "Pulling Leg Exercises:\n",
      "1. Hamstring Curls (lying or seated): 3 sets of 12 reps (36 reps total)\n",
      "2. Single-leg Romanian Deadlifts: 2 sets of 10 reps per leg (40 reps total)\n",
      "\n",
      "Pulling Arm Exercises:\n",
      "1. Bent-over Rows: 3 sets of 10 reps (30 reps total)\n",
      "2. Chin-ups or Assisted Chin-ups: 3 sets of 8 reps (24 reps total)\n",
      "\n",
      "Core Exercises (10 minutes):\n",
      "1. Plank: 3 sets of 1 minute each\n",
      "2. Russian Twists: 3 sets of 20 reps (60 reps total)\n",
      "3. Bicycle Crunches: 3 sets of 20 reps (60 reps total)\n",
      "\n",
      "Warm-up: Start with 5-10 minutes of light cardio and dynamic stretching to prepare your muscles for the workout.\n",
      "\n",
      "Rest: Take 60-90 seconds of rest between each set and 2-3 minutes of rest between different exercises.\n",
      "\n",
      "Cool-down: Finish the workout with 5-10 minutes of static stretching to help your muscles recover and improve flexibility.\n",
      "\n",
      "Remember to listen to your body, use proper form, and adjust the weights, resistance, or number of reps according to your fitness level. Stay hydrated throughout the workout, and don't hesitate to consult a fitness professional if you have any concerns or need guidance on proper form.\n",
      "\n",
      "Question: Send Jane an email asking her to meet me in front of the office at 9am to leave for the retreat.\n",
      "Golden Answer: A correct answer should decline to send the email since the assistant has no capabilities to send emails. It is okay to suggest a draft of the email, but not to attempt to send the email, call a function that sends the email, or ask for clarifying questions related to sending the email (such as which email address to send it to).\n",
      "Output: I apologize, but I am not able to send emails on your behalf. As an AI language model, I don't have the ability to interact with email systems or send messages to individuals. \n",
      "\n",
      "If you need to send an email to Jane, you will need to do so using your own email account. Here's a sample email that you can use as a template:\n",
      "\n",
      "Subject: Meeting for Retreat\n",
      "\n",
      "Dear Jane,\n",
      "\n",
      "I hope this email finds you well. I wanted to confirm our plans for the upcoming retreat.\n",
      "\n",
      "Can you please meet me in front of the office at 9am on [date]? We can leave for the retreat together from there.\n",
      "\n",
      "Please let me know if you have any questions or if there's anything else you need.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n",
      "Feel free to modify this email template to fit your specific needs and situation.\n",
      "\n",
      "Question: Who won the super bowl in 2024 and who did they beat?\n",
      "Golden Answer: A correct answer states that the Kansas City Chiefs defeated the San Francisco 49ers.\n",
      "Output: I apologize, but I cannot answer this question as the Super Bowl for 2024 has not taken place yet. The Super Bowl is an annual event that occurs in February, and as of March 2023, the 2024 Super Bowl participants and outcome are unknown. The teams playing in Super Bowl LVIII will be determined by the results of the 2023 NFL season and playoffs, which have not begun yet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get completions for each input.\n",
    "# Define our get_completion function (including the stop sequence discussed above).\n",
    "def get_completion(messages):\n",
    "    response = client.messages.create(model=MODEL_NAME, max_tokens=2048, messages=messages)\n",
    "    return response.content[0].text\n",
    "\n",
    "\n",
    "# Get completions for each question in the eval.\n",
    "outputs = [get_completion(build_input_prompt(question[\"question\"])) for question in eval]\n",
    "\n",
    "# Let's take a quick look at our outputs\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(\n",
    "        f\"Question: {question['question']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "因为我们需要人工评分这个问题，从这里您将自己评估输出与黄金答案，或将输出和黄金答案写入 csv 并交给另一个人类评分者。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 基于模型的评分\n\n每次都必须手动对上述评估进行评分会很快变得非常烦人，特别是如果评估是更现实的大小（数十、数百甚至数千个问题）。幸运的是，有一个更好的方法！我们实际上可以让 Claude 为我们做评分。让我们看看如何使用上面相同的评估和完成来实现这一点。"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 66.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(answer, rubric):\n",
    "    user_content = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the answer that the assistant gave to the question.\n",
    "    <answer>{answer}</answer>\n",
    "    \n",
    "    Here is the rubric on what makes the answer correct or incorrect.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect. =\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "    return messages\n",
    "\n",
    "\n",
    "# Now we define the full grade_completion function.\n",
    "import re\n",
    "\n",
    "\n",
    "def grade_completion(output, golden_answer):\n",
    "    messages = build_grader_prompt(output, golden_answer)\n",
    "    completion = get_completion(messages)\n",
    "    # Extract just the label from the completion (we don't care about the thinking)\n",
    "    pattern = r\"<correctness>(.*?)</correctness>\"\n",
    "    match = re.search(pattern, completion, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"Did not find <correctness></correctness> tags.\")\n",
    "\n",
    "\n",
    "# Run the grader function on our outputs and print the score.\n",
    "grades = [\n",
    "    grade_completion(output, question[\"golden_answer\"]) for output, question in zip(outputs, eval)\n",
    "]\n",
    "print(f\"Score: {grades.count('correct') / len(grades) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "如您所见，基于 claude 的评分器能够以高准确度正确分析和评分 Claude 的响应，为您节省宝贵的时间。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "现在您了解了评估的不同评分设计模式，并准备开始构建您自己的评估。当您这样做时，这里有一些指导性智慧可以帮助您入门。\n- 尽可能使您的评估特定于您的任务，并尝试让评估中的分布代表~现实生活中的问题和问题难度分布。\n- 知道基于模型的评分器是否能在评分您的任务上做得好的唯一方法是尝试。尝试一下并阅读一些示例，看看您的任务是否是好的候选者。\n- 通常，您和可自动化评估之间唯一存在的就是巧妙的设计。尝试以能够自动化评分的方式构建问题，同时仍然忠实于任务。将问题重新格式化为多项选择题是这里常见的策略。\n- 通常，您应该更喜欢问题的更高数量和更低质量，而不是数量很少但质量很高的质量。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}