{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4d70b6",
   "metadata": {},
   "source": "# æŠ•æœºæ€§æç¤ºè¯ç¼“å­˜\n\næœ¬æ•™ç¨‹æ¼”ç¤º\"æŠ•æœºæ€§æç¤ºè¯ç¼“å­˜\"â€”â€”ä¸€ç§é€šè¿‡åœ¨ç”¨æˆ·ä»åœ¨è¾“å…¥æŸ¥è¯¢æ—¶é¢„çƒ­ç¼“å­˜æ¥å‡å°‘é¦–æ ‡è®°æ—¶é—´ï¼ˆTTFTï¼‰çš„æ¨¡å¼ã€‚\n\n**æ— æŠ•æœºç¼“å­˜ï¼š**\n1. ç”¨æˆ·è¾“å…¥é—®é¢˜ï¼ˆ3ç§’ï¼‰\n2. ç”¨æˆ·æäº¤é—®é¢˜\n3. API å°†ä¸Šä¸‹æ–‡åŠ è½½åˆ°ç¼“å­˜ä¸­å¹¶ç”Ÿæˆå“åº”\n\n**æœ‰æŠ•æœºç¼“å­˜ï¼š**\n1. ç”¨æˆ·å¼€å§‹è¾“å…¥ï¼ˆç¼“å­˜é¢„çƒ­ç«‹å³å¼€å§‹ï¼‰\n2. ç”¨æˆ·ç»§ç»­è¾“å…¥ï¼ˆç¼“å­˜é¢„çƒ­åœ¨åå°å®Œæˆï¼‰\n3. ç”¨æˆ·æäº¤é—®é¢˜\n4. API ä½¿ç”¨æ¸©æš–çš„ç¼“å­˜ç”Ÿæˆå“åº”"
  },
  {
   "cell_type": "markdown",
   "id": "66720f9a",
   "metadata": {},
   "source": "## è®¾ç½®\n\né¦–å…ˆï¼Œè®©æˆ‘ä»¬å®‰è£…æ‰€éœ€çš„åŒ…ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a0adb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anthropic httpx --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4a98035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import time\n",
    "import asyncio\n",
    "import httpx\n",
    "from anthropic import AsyncAnthropic\n",
    "\n",
    "# Configuration constants\n",
    "MODEL = \"claude-sonnet-4-5\"\n",
    "SQLITE_SOURCES = {\n",
    "    \"btree.h\": \"https://sqlite.org/src/raw/18e5e7b2124c23426a283523e5f31a4bff029131b795bb82391f9d2f3136fc50?at=btree.h\",\n",
    "    \"btree.c\": \"https://sqlite.org/src/raw/63ca6b647342e8cef643863cd0962a542f133e1069460725ba4461dcda92b03c?at=btree.c\",\n",
    "}\n",
    "DEFAULT_CLIENT_ARGS = {\n",
    "    \"system\": \"You are an expert systems programmer helping analyze database internals.\",\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d56e2f",
   "metadata": {},
   "source": "## è¾…åŠ©å‡½æ•°\n\nè®©æˆ‘ä»¬è®¾ç½®å‡½æ•°æ¥ä¸‹è½½å¤§å‹ä¸Šä¸‹æ–‡å¹¶å‡†å¤‡æ¶ˆæ¯ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "08b7a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_sqlite_sources() -> dict[str, str]:\n",
    "    print(\"Downloading SQLite source files...\")\n",
    "\n",
    "    source_files = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        tasks = []\n",
    "\n",
    "        async def download_file(filename: str, url: str) -> tuple[str, str]:\n",
    "            response = await client.get(url, follow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "            return filename, response.text\n",
    "\n",
    "        for filename, url in SQLITE_SOURCES.items():\n",
    "            tasks.append(download_file(filename, url))\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        source_files = dict(results)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(source_files)} files in {duration:.2f} seconds\")\n",
    "    return source_files\n",
    "\n",
    "\n",
    "async def create_initial_message():\n",
    "    sources = await get_sqlite_sources()\n",
    "    # Prepare the initial message with the source code as context.\n",
    "    # A Timestamp is included to prevent cache sharing across different runs.\n",
    "    initial_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"\n",
    "Current time: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "Source to Analyze:\n",
    "\n",
    "btree.h:\n",
    "```c\n",
    "{sources[\"btree.h\"]}\n",
    "```\n",
    "\n",
    "btree.c:\n",
    "```c\n",
    "{sources[\"btree.c\"]}\n",
    "```\"\"\",\n",
    "                \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    return initial_message\n",
    "\n",
    "\n",
    "async def sample_one_token(client: AsyncAnthropic, messages: list):\n",
    "    \"\"\"Send a single-token request to warm up the cache\"\"\"\n",
    "    args = copy.deepcopy(DEFAULT_CLIENT_ARGS)\n",
    "    args[\"max_tokens\"] = 1\n",
    "    await client.messages.create(\n",
    "        messages=messages,\n",
    "        model=MODEL,\n",
    "        **args,\n",
    "    )\n",
    "\n",
    "\n",
    "def print_query_statistics(response, query_type: str) -> None:\n",
    "    print(f\"\\n{query_type} query statistics:\")\n",
    "    print(f\"\\tInput tokens: {response.usage.input_tokens}\")\n",
    "    print(f\"\\tOutput tokens: {response.usage.output_tokens}\")\n",
    "    print(f\"\\tCache read input tokens: {getattr(response.usage, 'cache_read_input_tokens', '---')}\")\n",
    "    print(\n",
    "        f\"\\tCache creation input tokens: {getattr(response.usage, 'cache_creation_input_tokens', '---')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b183621",
   "metadata": {},
   "source": "## ç¤ºä¾‹ 1ï¼šæ ‡å‡†æç¤ºè¯ç¼“å­˜ï¼ˆæ— æŠ•æœºç¼“å­˜ï¼‰\n\né¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ ‡å‡†æç¤ºè¯ç¼“å­˜æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ç”¨æˆ·è¾“å…¥ä»–ä»¬çš„é—®é¢˜ï¼Œç„¶åå°†æ•´ä¸ªä¸Šä¸‹æ–‡ + é—®é¢˜å‘é€åˆ° APIï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b16e9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def standard_prompt_caching_demo():\n",
    "    client = AsyncAnthropic()\n",
    "\n",
    "    # Prepare the large context\n",
    "    initial_message = await create_initial_message()\n",
    "\n",
    "    # Simulate user typing time (in real app, this would be actual user input)\n",
    "    print(\"User is typing their question...\")\n",
    "    await asyncio.sleep(3)  # Simulate 3 seconds of typing\n",
    "    user_question = \"What is the purpose of the BtShared structure?\"\n",
    "    print(f\"User submitted: {user_question}\")\n",
    "\n",
    "    # Now send the full request (context + question)\n",
    "    full_message = copy.deepcopy(initial_message)\n",
    "    full_message[\"content\"].append(\n",
    "        {\"type\": \"text\", \"text\": f\"Answer the user's question: {user_question}\"}\n",
    "    )\n",
    "\n",
    "    print(\"\\nSending request to API...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure time to first token\n",
    "    first_token_time = None\n",
    "    async with client.messages.stream(\n",
    "        messages=[full_message],\n",
    "        model=MODEL,\n",
    "        **DEFAULT_CLIENT_ARGS,\n",
    "    ) as stream:\n",
    "        async for text in stream.text_stream:\n",
    "            if first_token_time is None and text.strip():\n",
    "                first_token_time = time.time() - start_time\n",
    "                print(f\"\\nğŸ• Time to first token: {first_token_time:.2f} seconds\")\n",
    "                break\n",
    "\n",
    "        # Get the full response\n",
    "        response = await stream.get_final_message()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total response time: {total_time:.2f} seconds\")\n",
    "    print_query_statistics(response, \"Standard Caching\")\n",
    "\n",
    "    return first_token_time, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6dfa2ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SQLite source files...\n",
      "Successfully downloaded btree.h\n",
      "Successfully downloaded btree.c\n",
      "Downloaded 2 files in 0.30 seconds\n",
      "User is typing their question...\n",
      "User submitted: What is the purpose of the BtShared structure?\n",
      "\n",
      "Sending request to API...\n",
      "\n",
      "ğŸ• Time to first token: 20.87 seconds\n",
      "Total response time: 28.32 seconds\n",
      "\n",
      "Standard Caching query statistics:\n",
      "\tInput tokens: 22\n",
      "\tOutput tokens: 362\n",
      "\tCache read input tokens: 0\n",
      "\tCache creation input tokens: 151629\n"
     ]
    }
   ],
   "source": [
    "# Run the standard demo\n",
    "standard_ttft, standard_total = await standard_prompt_caching_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0c669",
   "metadata": {},
   "source": "## ç¤ºä¾‹ 2ï¼šæŠ•æœºæ€§æç¤ºè¯ç¼“å­˜\n\nç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æŠ•æœºæ€§æç¤ºè¯ç¼“å­˜å¦‚ä½•é€šè¿‡åœ¨ç”¨æˆ·è¾“å…¥æ—¶é¢„çƒ­ç¼“å­˜æ¥æ”¹å–„ TTFTï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c4ca7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def speculative_prompt_caching_demo():\n",
    "    client = AsyncAnthropic()\n",
    "\n",
    "    # The user has a large amount of context they want to interact with,\n",
    "    # in this case it's the sqlite b-tree implementation (~150k tokens).\n",
    "    initial_message = await create_initial_message()\n",
    "\n",
    "    # Start speculative caching while user is typing\n",
    "    print(\"User is typing their question...\")\n",
    "    print(\"ğŸ”¥ Starting cache warming in background...\")\n",
    "\n",
    "    # While the user is typing out their question, we sample a single token\n",
    "    # from the context the user is going to be interacting with with explicit\n",
    "    # prompt caching turned on to warm up the cache.\n",
    "    cache_task = asyncio.create_task(sample_one_token(client, [initial_message]))\n",
    "\n",
    "    # Simulate user typing time\n",
    "    await asyncio.sleep(3)  # Simulate 3 seconds of typing\n",
    "    user_question = \"What is the purpose of the BtShared structure?\"\n",
    "    print(f\"User submitted: {user_question}\")\n",
    "\n",
    "    # Ensure cache warming is complete\n",
    "    await cache_task\n",
    "    print(\"âœ… Cache warming completed!\")\n",
    "\n",
    "    # Prepare messages for cached query. We make sure we\n",
    "    # reuse the same initial message as was cached to ensure we have a cache hit.\n",
    "    cached_message = copy.deepcopy(initial_message)\n",
    "    cached_message[\"content\"].append(\n",
    "        {\"type\": \"text\", \"text\": f\"Answer the user's question: {user_question}\"}\n",
    "    )\n",
    "\n",
    "    print(\"\\nSending request to API (with warm cache)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Measure time to first token\n",
    "    first_token_time = None\n",
    "    async with client.messages.stream(\n",
    "        messages=[cached_message],\n",
    "        model=MODEL,\n",
    "        **DEFAULT_CLIENT_ARGS,\n",
    "    ) as stream:\n",
    "        async for text in stream.text_stream:\n",
    "            if first_token_time is None and text.strip():\n",
    "                first_token_time = time.time() - start_time\n",
    "                print(f\"\\nğŸš€ Time to first token: {first_token_time:.2f} seconds\")\n",
    "                break\n",
    "\n",
    "        # Get the full response\n",
    "        response = await stream.get_final_message()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total response time: {total_time:.2f} seconds\")\n",
    "    print_query_statistics(response, \"Speculative Caching\")\n",
    "\n",
    "    return first_token_time, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c960975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SQLite source files...\n",
      "Successfully downloaded btree.h\n",
      "Successfully downloaded btree.c\n",
      "Downloaded 2 files in 0.36 seconds\n",
      "User is typing their question...\n",
      "ğŸ”¥ Starting cache warming in background...\n",
      "User submitted: What is the purpose of the BtShared structure?\n",
      "âœ… Cache warming completed!\n",
      "\n",
      "Sending request to API (with warm cache)...\n",
      "\n",
      "ğŸš€ Time to first token: 1.94 seconds\n",
      "Total response time: 8.40 seconds\n",
      "\n",
      "Speculative Caching query statistics:\n",
      "\tInput tokens: 22\n",
      "\tOutput tokens: 330\n",
      "\tCache read input tokens: 151629\n",
      "\tCache creation input tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Run the speculative caching demo\n",
    "speculative_ttft, speculative_total = await speculative_prompt_caching_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed1898",
   "metadata": {},
   "source": "## æ€§èƒ½æ¯”è¾ƒ\n\nè®©æˆ‘ä»¬æ¯”è¾ƒç»“æœä»¥æŸ¥çœ‹æŠ•æœºç¼“å­˜çš„å¥½å¤„ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66b3009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Standard Prompt Caching:\n",
      "  Time to First Token: 20.87 seconds\n",
      "  Total Response Time: 28.32 seconds\n",
      "\n",
      "Speculative Prompt Caching:\n",
      "  Time to First Token: 1.94 seconds\n",
      "  Total Response Time: 8.40 seconds\n",
      "\n",
      "ğŸ¯ IMPROVEMENTS:\n",
      "  TTFT Improvement: 90.7% (18.93s faster)\n",
      "  Total Time Improvement: 70.4% (19.92s faster)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nStandard Prompt Caching:\")\n",
    "print(f\"  Time to First Token: {standard_ttft:.2f} seconds\")\n",
    "print(f\"  Total Response Time: {standard_total:.2f} seconds\")\n",
    "\n",
    "print(\"\\nSpeculative Prompt Caching:\")\n",
    "print(f\"  Time to First Token: {speculative_ttft:.2f} seconds\")\n",
    "print(f\"  Total Response Time: {speculative_total:.2f} seconds\")\n",
    "\n",
    "ttft_improvement = (standard_ttft - speculative_ttft) / standard_ttft * 100\n",
    "total_improvement = (standard_total - speculative_total) / standard_total * 100\n",
    "\n",
    "print(\"\\nğŸ¯ IMPROVEMENTS:\")\n",
    "print(\n",
    "    f\"  TTFT Improvement: {ttft_improvement:.1f}% ({standard_ttft - speculative_ttft:.2f}s faster)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Total Time Improvement: {total_improvement:.1f}% ({standard_total - speculative_total:.2f}s faster)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c9bdd",
   "metadata": {},
   "source": "## å…³é”®è¦ç‚¹\n\n1. **æŠ•æœºæ€§ç¼“å­˜æ˜¾è‘—å‡å°‘ TTFT** é€šè¿‡åœ¨ç”¨æˆ·è¾“å…¥æ—¶é¢„çƒ­ç¼“å­˜\n2. **è¯¥æ¨¡å¼å¯¹äºå¤§å‹ä¸Šä¸‹æ–‡ï¼ˆ>1000ä¸ªæ ‡è®°ï¼‰æœ€æœ‰æ•ˆ** è¿™äº›ä¸Šä¸‹æ–‡åœ¨æŸ¥è¯¢ä¹‹é—´è¢«é‡å¤ä½¿ç”¨\n3. **å®ç°å¾ˆç®€å•** - åªéœ€åœ¨ç”¨æˆ·è¾“å…¥æ—¶å‘é€ 1 æ ‡è®°è¯·æ±‚\n4. **ç¼“å­˜é¢„çƒ­ä¸ç”¨æˆ·è¾“å…¥å¹¶è¡Œè¿›è¡Œ** æœ‰æ•ˆåœ°\"éšè—\"äº†ç¼“å­˜åˆ›å»ºæ—¶é—´\n\n## æœ€ä½³å®è·µ\n\n- å°½å¯èƒ½æ—©åœ°å¼€å§‹ç¼“å­˜é¢„çƒ­ï¼ˆä¾‹å¦‚ï¼Œå½“ç”¨æˆ·èšç„¦è¾“å…¥å­—æ®µæ—¶ï¼‰\n- ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä¸Šä¸‹æ–‡è¿›è¡Œé¢„çƒ­å’Œå®é™…è¯·æ±‚ä»¥ç¡®ä¿ç¼“å­˜å‘½ä¸­\n- ç›‘æ§ `cache_read_input_tokens` ä»¥éªŒè¯ç¼“å­˜å‘½ä¸­\n- æ·»åŠ æ—¶é—´æˆ³ä»¥é˜²æ­¢è·¨ä¼šè¯çš„ä¸å¿…è¦ç¼“å­˜å…±äº«"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}