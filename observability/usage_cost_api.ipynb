{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9610be80",
   "metadata": {
    "id": "9610be80"
   },
   "source": "# ä½¿ç”¨ä¸æˆæœ¬ç®¡ç† API æŒ‡å—\n\n**ä»¥ç¼–ç¨‹æ–¹å¼è®¿é—® Claude API ä½¿ç”¨å’Œæˆæœ¬æ•°æ®çš„å®ç”¨æŒ‡å—**"
  },
  {
   "cell_type": "markdown",
   "id": "69b800fc",
   "metadata": {
    "id": "69b800fc"
   },
   "source": "### æ‚¨å¯ä»¥åšä»€ä¹ˆ\n\n**ä½¿ç”¨æƒ…å†µè·Ÿè¸ªï¼š**\n- ç›‘æ§ä»¤ç‰Œæ¶ˆè€—ï¼ˆæœªç¼“å­˜è¾“å…¥ã€è¾“å‡ºã€ç¼“å­˜åˆ›å»º/è¯»å–ï¼‰\n- è·¨æ¨¡å‹ã€å·¥ä½œåŒºå’Œ API å¯†é’¥è·Ÿè¸ªä½¿ç”¨æƒ…å†µ\n- åˆ†æç¼“å­˜æ•ˆç‡å’Œä½¿ç”¨æœåŠ¡å™¨å·¥å…·æƒ…å†µ\n\n**æˆæœ¬åˆ†æï¼š**\n- æŒ‰æœåŠ¡ç±»å‹æ£€ç´¢è¯¦ç»†æˆæœ¬ç»†åˆ†\n- ç›‘æ§è·¨å·¥ä½œåŒºçš„æ”¯å‡ºè¶‹åŠ¿\n- ä¸ºè´¢åŠ¡å’Œæˆæœ¬åˆ†æ‘Šåœºæ™¯ç”ŸæˆæŠ¥å‘Š\n\n**å¸¸è§ç”¨ä¾‹ï¼š**\n- **ä½¿ç”¨æƒ…å†µç›‘æ§**ï¼šè·Ÿè¸ªæ¶ˆè´¹æ¨¡å¼å¹¶ä¼˜åŒ–æˆæœ¬\n- **æˆæœ¬å½’å±**ï¼šæŒ‰å·¥ä½œåŒºå°†è´¹ç”¨åˆ†é…ç»™å›¢é˜Ÿ/é¡¹ç›®\n- **ç¼“å­˜åˆ†æ**ï¼šè¡¡é‡å’Œæ”¹è¿›ç¼“å­˜æ•ˆç‡\n- **è´¢åŠ¡æŠ¥å‘Š**ï¼šç”Ÿæˆæ‰§è¡Œæ‘˜è¦å’Œé¢„ç®—æŠ¥å‘Š\n\n### API æ¦‚è¿°\n\nä¸¤ä¸ªä¸»è¦ç«¯ç‚¹ï¼š\n1. **æ¶ˆæ¯ä½¿ç”¨ API**ï¼šå…·æœ‰çµæ´»åˆ†ç»„çš„ä»¤ç‰Œçº§ä½¿ç”¨æ•°æ®\n2. **æˆæœ¬ API**ï¼šä»¥ç¾å…ƒè®¡ä»·çš„è´¢åŠ¡æ•°æ®ï¼ŒåŒ…å«æœåŠ¡ç»†åˆ†\n\n### å…ˆå†³æ¡ä»¶ä¸å®‰å…¨\n\n- **ç®¡ç†å‘˜ API å¯†é’¥**ï¼šä» [Claude æ§åˆ¶å°](https://console.anthropic.com/settings/admin-keys) è·å–ï¼ˆæ ¼å¼ï¼š`sk-ant-admin...`ï¼‰\n- **å®‰å…¨**ï¼šå°†å¯†é’¥å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­ï¼Œå®šæœŸè½®æ¢ï¼Œåˆ‡å‹¿æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd50a16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edd50a16",
    "outputId": "68f38db3-48ee-429f-cd6e-2d2ae74e009a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connection successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta, time\n",
    "from typing import Dict, Optional, Any\n",
    "\n",
    "\n",
    "class AnthropicAdminAPI:\n",
    "    \"\"\"Secure wrapper for Anthropic Admin API endpoints.\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.api_key = api_key or os.getenv(\"ANTHROPIC_ADMIN_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"Admin API key required. Set ANTHROPIC_ADMIN_API_KEY environment variable.\"\n",
    "            )\n",
    "\n",
    "        if not self.api_key.startswith(\"sk-ant-admin\"):\n",
    "            raise ValueError(\"Invalid Admin API key format.\")\n",
    "\n",
    "        self.base_url = \"https://api.anthropic.com/v1/organizations\"\n",
    "        self.headers = {\n",
    "            \"anthropic-version\": \"2023-06-01\",\n",
    "            \"x-api-key\": self.api_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Make authenticated request with basic error handling.\"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 401:\n",
    "                raise ValueError(\"Invalid API key or insufficient permissions\")\n",
    "            elif response.status_code == 429:\n",
    "                raise requests.exceptions.RequestException(\"Rate limit exceeded - try again later\")\n",
    "            else:\n",
    "                raise requests.exceptions.RequestException(f\"API error: {e}\")\n",
    "\n",
    "\n",
    "# Test connection\n",
    "def test_connection():\n",
    "    try:\n",
    "        client = AnthropicAdminAPI()\n",
    "\n",
    "        # Simple test query - snap to start of day to align with bucket boundaries\n",
    "        params = {\n",
    "            \"starting_at\": (\n",
    "                datetime.combine(datetime.utcnow(), time.min) - timedelta(days=1)\n",
    "            ).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"ending_at\": datetime.combine(datetime.utcnow(), time.min).strftime(\n",
    "                \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "            ),\n",
    "            \"bucket_width\": \"1d\",\n",
    "            \"limit\": 1,\n",
    "        }\n",
    "\n",
    "        client._make_request(\"usage_report/messages\", params)\n",
    "        print(\"âœ… Connection successful!\")\n",
    "        return client\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "client = test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a0721",
   "metadata": {
    "id": "781a0721"
   },
   "source": "## åŸºæœ¬ä½¿ç”¨ä¸æˆæœ¬è·Ÿè¸ª\n\n### ç†è§£ä½¿ç”¨æ•°æ®\n\næ¶ˆæ¯ä½¿ç”¨ API æä¾› **æ—¶é—´æ¡¶** ä¸­çš„ä»¤ç‰Œæ¶ˆè€— - åŒ…å«èšåˆä½¿ç”¨çš„å›ºå®šé—´éš”ã€‚\n\n**å…³é”®æŒ‡æ ‡ï¼š**\n- **uncached_input_tokens**ï¼šæ–°è¾“å…¥ä»¤ç‰Œï¼ˆæç¤ºã€ç³»ç»Ÿæ¶ˆæ¯ï¼‰\n- **output_tokens**ï¼šClaude çš„å“åº”\n- **cache_creation**ï¼šç”¨äºé‡ç”¨çš„ç¼“å­˜ä»¤ç‰Œ\n- **cache_read_input_tokens**ï¼šé‡å¤ä½¿ç”¨çš„å…ˆå‰ç¼“å­˜ä»¤ç‰Œ\n\n### åŸºæœ¬ä½¿ç”¨æŸ¥è¯¢"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b26143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9b26143",
    "outputId": "c60b91c0-084d-4629-eddd-cf0fb941e042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Usage Summary:\n",
      "Uncached input tokens: 267,751\n",
      "Output tokens: 2,848,746\n",
      "Cache creation: 0\n",
      "Cache reads: 0\n",
      "Cache efficiency: 0.0%\n",
      "Web searches: 0\n"
     ]
    }
   ],
   "source": [
    "def get_daily_usage(client, days_back=7):\n",
    "    \"\"\"Get usage data for the last N days.\"\"\"\n",
    "    end_time = datetime.combine(datetime.utcnow(), time.min)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "\n",
    "    params = {\n",
    "        \"starting_at\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"ending_at\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"bucket_width\": \"1d\",\n",
    "        \"limit\": days_back,\n",
    "    }\n",
    "\n",
    "    return client._make_request(\"usage_report/messages\", params)\n",
    "\n",
    "\n",
    "def analyze_usage_data(response):\n",
    "    \"\"\"Process and display usage data.\"\"\"\n",
    "    if not response or not response.get(\"data\"):\n",
    "        print(\"No usage data found.\")\n",
    "        return\n",
    "\n",
    "    total_uncached_input = total_output = total_cache_creation = 0\n",
    "    total_cache_reads = total_web_searches = 0\n",
    "    daily_data = []\n",
    "\n",
    "    for bucket in response[\"data\"]:\n",
    "        date = bucket[\"starting_at\"][:10]\n",
    "\n",
    "        # Sum all results in bucket\n",
    "        bucket_uncached = bucket_output = bucket_cache_creation = 0\n",
    "        bucket_cache_reads = bucket_web_searches = 0\n",
    "\n",
    "        for result in bucket[\"results\"]:\n",
    "            bucket_uncached += result.get(\"uncached_input_tokens\", 0)\n",
    "            bucket_output += result.get(\"output_tokens\", 0)\n",
    "\n",
    "            cache_creation = result.get(\"cache_creation\", {})\n",
    "            bucket_cache_creation += cache_creation.get(\n",
    "                \"ephemeral_1h_input_tokens\", 0\n",
    "            ) + cache_creation.get(\"ephemeral_5m_input_tokens\", 0)\n",
    "            bucket_cache_reads += result.get(\"cache_read_input_tokens\", 0)\n",
    "\n",
    "            server_tools = result.get(\"server_tool_use\", {})\n",
    "            bucket_web_searches += server_tools.get(\"web_search_requests\", 0)\n",
    "\n",
    "        daily_data.append(\n",
    "            {\n",
    "                \"date\": date,\n",
    "                \"uncached_input_tokens\": bucket_uncached,\n",
    "                \"output_tokens\": bucket_output,\n",
    "                \"cache_creation\": bucket_cache_creation,\n",
    "                \"cache_reads\": bucket_cache_reads,\n",
    "                \"web_searches\": bucket_web_searches,\n",
    "                \"total_tokens\": bucket_uncached + bucket_output,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add to totals\n",
    "        total_uncached_input += bucket_uncached\n",
    "        total_output += bucket_output\n",
    "        total_cache_creation += bucket_cache_creation\n",
    "        total_cache_reads += bucket_cache_reads\n",
    "        total_web_searches += bucket_web_searches\n",
    "\n",
    "    # Calculate cache efficiency\n",
    "    total_input_tokens = total_uncached_input + total_cache_creation + total_cache_reads\n",
    "    cache_efficiency = (\n",
    "        (total_cache_reads / total_input_tokens * 100) if total_input_tokens > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Display summary\n",
    "    print(\"ğŸ“Š Usage Summary:\")\n",
    "    print(f\"Uncached input tokens: {total_uncached_input:,}\")\n",
    "    print(f\"Output tokens: {total_output:,}\")\n",
    "    print(f\"Cache creation: {total_cache_creation:,}\")\n",
    "    print(f\"Cache reads: {total_cache_reads:,}\")\n",
    "    print(f\"Cache efficiency: {cache_efficiency:.1f}%\")\n",
    "    print(f\"Web searches: {total_web_searches:,}\")\n",
    "\n",
    "    return daily_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if client:\n",
    "    usage_response = get_daily_usage(client, days_back=7)\n",
    "    daily_usage = analyze_usage_data(usage_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3164089",
   "metadata": {
    "id": "e3164089"
   },
   "source": "## åŸºæœ¬æˆæœ¬è·Ÿè¸ª\n\næ³¨æ„ï¼šPriority Tier æˆæœ¬ä½¿ç”¨ä¸åŒçš„è®¡è´¹æ¨¡å¼ï¼Œæ°¸è¿œä¸ä¼šå‡ºç°åœ¨æˆæœ¬ç«¯ç‚¹ä¸­ã€‚æ‚¨å¯ä»¥åœ¨ä½¿ç”¨ç«¯ç‚¹ä¸­è·Ÿè¸ª Priority Tier çš„ä½¿ç”¨æƒ…å†µï¼Œä½†ä¸èƒ½è·Ÿè¸ªæˆæœ¬ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "SP4zefdtF0ft",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP4zefdtF0ft",
    "outputId": "a1daa8d0-f36f-474b-9967-6fe00a0efe52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’° Cost Summary:\n",
      "Total cost: $83.7574\n",
      "Average daily cost: $11.9653\n"
     ]
    }
   ],
   "source": [
    "def get_daily_costs(client, days_back=7):\n",
    "    \"\"\"Get cost data for the last N days.\"\"\"\n",
    "    end_time = datetime.combine(datetime.utcnow(), time.min)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "\n",
    "    params = {\n",
    "        \"starting_at\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"ending_at\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"bucket_width\": \"1d\",  # Only 1d supported for cost API\n",
    "        \"limit\": min(days_back, 31),  # Max 31 days per request\n",
    "    }\n",
    "\n",
    "    return client._make_request(\"cost_report\", params)\n",
    "\n",
    "\n",
    "def analyze_cost_data(response):\n",
    "    \"\"\"Process and display cost data.\"\"\"\n",
    "    if not response or not response.get(\"data\"):\n",
    "        print(\"No cost data found.\")\n",
    "        return\n",
    "\n",
    "    total_cost_minor_units = 0\n",
    "    daily_costs = []\n",
    "\n",
    "    for bucket in response[\"data\"]:\n",
    "        date = bucket[\"starting_at\"][:10]\n",
    "\n",
    "        # Sum all costs in this bucket\n",
    "        bucket_cost = 0\n",
    "        for result in bucket[\"results\"]:\n",
    "            # Convert string amounts to float if needed\n",
    "            amount = result.get(\"amount\", 0)\n",
    "            if isinstance(amount, str):\n",
    "                try:\n",
    "                    amount = float(amount)\n",
    "                except (ValueError, TypeError):\n",
    "                    amount = 0\n",
    "            bucket_cost += amount\n",
    "\n",
    "        daily_costs.append(\n",
    "            {\n",
    "                \"date\": date,\n",
    "                \"cost_minor_units\": bucket_cost,\n",
    "                \"cost_usd\": bucket_cost / 100,  # Convert to dollars\n",
    "            }\n",
    "        )\n",
    "\n",
    "        total_cost_minor_units += bucket_cost\n",
    "\n",
    "    total_cost_usd = total_cost_minor_units / 100\n",
    "\n",
    "    print(\"ğŸ’° Cost Summary:\")\n",
    "    print(f\"Total cost: ${total_cost_usd:.4f}\")\n",
    "    print(f\"Average daily cost: ${total_cost_usd / len(daily_costs):.4f}\")\n",
    "\n",
    "    return daily_costs\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if client:\n",
    "    cost_response = get_daily_costs(client, days_back=7)\n",
    "    daily_costs = analyze_cost_data(cost_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce76fc4",
   "metadata": {
    "id": "6ce76fc4"
   },
   "source": "## åˆ†ç»„ã€è¿‡æ»¤ä¸åˆ†é¡µ\n\n### æ—¶é—´ç²’åº¦é€‰é¡¹\n\n**ä½¿ç”¨ API** æ”¯æŒä¸‰ç§ç²’åº¦ï¼š\n- `1m`ï¼ˆ1 åˆ†é’Ÿï¼‰ï¼šé«˜åˆ†è¾¨ç‡åˆ†æï¼Œæ¯æ¬¡è¯·æ±‚æœ€å¤š 1440 ä¸ªæ¡¶\n- `1h`ï¼ˆ1 å°æ—¶ï¼‰ï¼šä¸­ç­‰åˆ†è¾¨ç‡ï¼Œæ¯æ¬¡è¯·æ±‚æœ€å¤š 168 ä¸ªæ¡¶\n- `1d`ï¼ˆ1 å¤©ï¼‰ï¼šæ¯æ—¥åˆ†æï¼Œæ¯æ¬¡è¯·æ±‚æœ€å¤š 31 ä¸ªæ¡¶\n\n**æˆæœ¬ API** æ”¯æŒï¼š\n- `1d`ï¼ˆ1 å¤©ï¼‰ï¼šå”¯ä¸€å¯ç”¨é€‰é¡¹ï¼Œæ¯æ¬¡è¯·æ±‚æœ€å¤š 31 ä¸ªæ¡¶\n\n### åˆ†ç»„å’Œè¿‡æ»¤"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f1a7ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87f1a7ac",
    "outputId": "3266318c-8af4-4647-9360-b133ffa82452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Usage by Model:\n",
      "  claude-3-5-haiku-20241022: 995,781 tokens\n",
      "  claude-sonnet-4-5: 861,880 tokens\n",
      "  claude-opus-4-1: 394,646 tokens\n",
      "  claude-sonnet-4-5: 356,766 tokens\n",
      "  claude-opus-4-20250514: 308,223 tokens\n",
      "  claude-opus-4-1: 199,201 tokens\n",
      "Found 7 days of filtered usage data\n"
     ]
    }
   ],
   "source": [
    "def get_usage_by_model(client, days_back=7):\n",
    "    \"\"\"Get usage data grouped by model, handling pagination automatically.\"\"\"\n",
    "    end_time = datetime.combine(datetime.utcnow(), time.min)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "\n",
    "    params = {\n",
    "        \"starting_at\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"ending_at\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"group_by[]\": [\"model\"],\n",
    "        \"bucket_width\": \"1d\",\n",
    "    }\n",
    "\n",
    "    # Aggregate across all pages of data\n",
    "    model_usage = {}\n",
    "    page_count = 0\n",
    "    max_pages = 10  # Reasonable limit to avoid infinite loops\n",
    "\n",
    "    try:\n",
    "        next_page = None\n",
    "\n",
    "        while page_count < max_pages:\n",
    "            current_params = params.copy()\n",
    "            if next_page:\n",
    "                current_params[\"page\"] = next_page\n",
    "\n",
    "            response = client._make_request(\"usage_report/messages\", current_params)\n",
    "            page_count += 1\n",
    "\n",
    "            # Process this page's data\n",
    "            for bucket in response.get(\"data\", []):\n",
    "                for result in bucket.get(\"results\", []):\n",
    "                    model = result.get(\"model\", \"Unknown\")\n",
    "                    uncached = result.get(\"uncached_input_tokens\", 0)\n",
    "                    output = result.get(\"output_tokens\", 0)\n",
    "                    cache_creation = result.get(\"cache_creation\", {})\n",
    "                    cache_creation_tokens = cache_creation.get(\n",
    "                        \"ephemeral_1h_input_tokens\", 0\n",
    "                    ) + cache_creation.get(\"ephemeral_5m_input_tokens\", 0)\n",
    "                    cache_reads = result.get(\"cache_read_input_tokens\", 0)\n",
    "                    tokens = uncached + output + cache_creation_tokens + cache_reads\n",
    "\n",
    "                    if model not in model_usage:\n",
    "                        model_usage[model] = 0\n",
    "                    model_usage[model] += tokens\n",
    "\n",
    "            # Check if there's more data\n",
    "            if not response.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "            next_page = response.get(\"next_page\")\n",
    "            if not next_page:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error retrieving usage data: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # Display results\n",
    "    print(\"ğŸ“Š Usage by Model:\")\n",
    "    if not model_usage:\n",
    "        print(f\"  No usage data found in the last {days_back} days\")\n",
    "        print(\"  ğŸ’¡ Try increasing the time range or check if you have recent API usage\")\n",
    "    else:\n",
    "        for model, tokens in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {model}: {tokens:,} tokens\")\n",
    "\n",
    "    return model_usage\n",
    "\n",
    "\n",
    "def filter_usage_example(client):\n",
    "    \"\"\"Example of filtering usage data.\"\"\"\n",
    "    params = {\n",
    "        \"starting_at\": (datetime.combine(datetime.utcnow(), time.min) - timedelta(days=7)).strftime(\n",
    "            \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "        ),\n",
    "        \"ending_at\": datetime.combine(datetime.utcnow(), time.min).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"models[]\": [\"claude-sonnet-4-5\"],  # Filter to specific model\n",
    "        \"service_tiers[]\": [\"standard\"],  # Filter to standard tier\n",
    "        \"bucket_width\": \"1d\",\n",
    "    }\n",
    "\n",
    "    response = client._make_request(\"usage_report/messages\", params)\n",
    "    print(f\"Found {len(response.get('data', []))} days of filtered usage data\")\n",
    "    return response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if client:\n",
    "    model_usage = get_usage_by_model(client, days_back=14)\n",
    "    filtered_usage = filter_usage_example(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bc11d",
   "metadata": {
    "id": "487bc11d"
   },
   "source": "### å¤§æ•°æ®é›†åˆ†é¡µ"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cc5437",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7cc5437",
    "outputId": "e7875fb4-09ae-44d2-f5fc-12a4cbdcce5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Fetching paginated data...\n",
      "  Page 1: 24 time buckets\n",
      "  Page 2: 24 time buckets\n",
      "  Page 3: 24 time buckets\n",
      "âœ… Complete: Retrieved all data in 3 pages\n",
      "ğŸ“Š Total retrieved: 72 time buckets\n",
      "ğŸ“ˆ Total tokens across all data: 1,336,287\n"
     ]
    }
   ],
   "source": [
    "def fetch_all_usage_data(client, params, max_pages=10):\n",
    "    \"\"\"Fetch all paginated usage data.\"\"\"\n",
    "    all_data = []\n",
    "    page_count = 0\n",
    "    next_page = None\n",
    "\n",
    "    print(\"ğŸ“¥ Fetching paginated data...\")\n",
    "\n",
    "    while page_count < max_pages:\n",
    "        current_params = params.copy()\n",
    "        if next_page:\n",
    "            current_params[\"page\"] = next_page\n",
    "\n",
    "        try:\n",
    "            response = client._make_request(\"usage_report/messages\", current_params)\n",
    "\n",
    "            if not response or not response.get(\"data\"):\n",
    "                break\n",
    "\n",
    "            page_data = response[\"data\"]\n",
    "            all_data.extend(page_data)\n",
    "            page_count += 1\n",
    "\n",
    "            print(f\"  Page {page_count}: {len(page_data)} time buckets\")\n",
    "\n",
    "            if not response.get(\"has_more\", False):\n",
    "                print(f\"âœ… Complete: Retrieved all data in {page_count} pages\")\n",
    "                break\n",
    "\n",
    "            next_page = response.get(\"next_page\")\n",
    "            if not next_page:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error on page {page_count + 1}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"ğŸ“Š Total retrieved: {len(all_data)} time buckets\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def large_dataset_example(client, days_back=3):\n",
    "    \"\"\"Example of handling a large dataset with pagination.\"\"\"\n",
    "    # Use recent time range to ensure we have data\n",
    "    start_time = datetime.combine(datetime.utcnow(), time.min) - timedelta(days=days_back)\n",
    "    end_time = datetime.combine(datetime.utcnow(), time.min)\n",
    "\n",
    "    params = {\n",
    "        \"starting_at\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"ending_at\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"bucket_width\": \"1h\",  # Hourly data for more buckets\n",
    "        \"group_by[]\": [\"model\"],\n",
    "        \"limit\": 24,  # One day per page\n",
    "    }\n",
    "\n",
    "    all_buckets = fetch_all_usage_data(client, params, max_pages=5)\n",
    "\n",
    "    # Process the large dataset\n",
    "    if all_buckets:\n",
    "        total_tokens = sum(\n",
    "            sum(\n",
    "                result.get(\"uncached_input_tokens\", 0) + result.get(\"output_tokens\", 0)\n",
    "                for result in bucket[\"results\"]\n",
    "            )\n",
    "            for bucket in all_buckets\n",
    "        )\n",
    "        print(f\"ğŸ“ˆ Total tokens across all data: {total_tokens:,}\")\n",
    "\n",
    "    return all_buckets\n",
    "\n",
    "\n",
    "# Example usage - use shorter time range to find recent data\n",
    "if client:\n",
    "    large_dataset = large_dataset_example(client, days_back=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55bbb55",
   "metadata": {
    "id": "d55bbb55"
   },
   "source": "## ç®€å•æ•°æ®å¯¼å‡º\n\n### ç”¨äºå¤–éƒ¨åˆ†æçš„ CSV å¯¼å‡º"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f365296a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f365296a",
    "outputId": "87b4a733-6f90-4b8d-a109-7a23281b2a4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported 36 rows to my_usage_data.csv\n",
      "âœ… Exported 72 cost records to my_cost_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def export_usage_to_csv(client, output_file=\"usage_data.csv\", days_back=30):\n",
    "    \"\"\"Export usage data to CSV for external analysis.\"\"\"\n",
    "\n",
    "    end_time = datetime.combine(datetime.utcnow(), time.min)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "\n",
    "    params = {\n",
    "        \"starting_at\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"ending_at\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"group_by[]\": [\"model\", \"service_tier\", \"workspace_id\"],\n",
    "        \"bucket_width\": \"1d\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Collect all data across pages\n",
    "        rows = []\n",
    "        page_count = 0\n",
    "        max_pages = 20  # Allow more pages for export\n",
    "        next_page = None\n",
    "\n",
    "        while page_count < max_pages:\n",
    "            current_params = params.copy()\n",
    "            if next_page:\n",
    "                current_params[\"page\"] = next_page\n",
    "\n",
    "            response = client._make_request(\"usage_report/messages\", current_params)\n",
    "            page_count += 1\n",
    "\n",
    "            # Process this page's data\n",
    "            for bucket in response.get(\"data\", []):\n",
    "                date = bucket[\"starting_at\"][:10]\n",
    "                for result in bucket[\"results\"]:\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"date\": date,\n",
    "                            \"model\": result.get(\"model\", \"\"),\n",
    "                            \"service_tier\": result.get(\"service_tier\", \"\"),\n",
    "                            \"workspace_id\": result.get(\"workspace_id\", \"\"),\n",
    "                            \"uncached_input_tokens\": result.get(\"uncached_input_tokens\", 0),\n",
    "                            \"output_tokens\": result.get(\"output_tokens\", 0),\n",
    "                            \"cache_creation_tokens\": (\n",
    "                                result.get(\"cache_creation\", {}).get(\"ephemeral_1h_input_tokens\", 0)\n",
    "                                + result.get(\"cache_creation\", {}).get(\n",
    "                                    \"ephemeral_5m_input_tokens\", 0\n",
    "                                )\n",
    "                            ),\n",
    "                            \"cache_read_tokens\": result.get(\"cache_read_input_tokens\", 0),\n",
    "                            \"web_search_requests\": result.get(\"server_tool_use\", {}).get(\n",
    "                                \"web_search_requests\", 0\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # Check if there's more data\n",
    "            if not response.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "            next_page = response.get(\"next_page\")\n",
    "            if not next_page:\n",
    "                break\n",
    "\n",
    "        # Write CSV\n",
    "        if rows:\n",
    "            with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=rows[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "\n",
    "            print(f\"âœ… Exported {len(rows)} rows to {output_file}\")\n",
    "        else:\n",
    "            print(f\"No usage data to export for the last {days_back} days\")\n",
    "            print(\"ğŸ’¡ Try increasing days_back or check if you have recent API usage\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Export failed: {e}\")\n",
    "\n",
    "\n",
    "def export_costs_to_csv(client, output_file=\"cost_data.csv\", days_back=30):\n",
    "    \"\"\"Export cost data to CSV.\"\"\"\n",
    "\n",
    "    end_time = datetime.combine(datetime.utcnow(), time.min)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "\n",
    "    params = {\n",
    "        \"starting_at\": start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"ending_at\": end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"group_by[]\": [\"workspace_id\", \"description\"],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Collect all data across pages\n",
    "        rows = []\n",
    "        page_count = 0\n",
    "        max_pages = 20\n",
    "        next_page = None\n",
    "\n",
    "        while page_count < max_pages:\n",
    "            current_params = params.copy()\n",
    "            if next_page:\n",
    "                current_params[\"page\"] = next_page\n",
    "\n",
    "            response = client._make_request(\"cost_report\", current_params)\n",
    "            page_count += 1\n",
    "\n",
    "            # Process this page's data\n",
    "            for bucket in response.get(\"data\", []):\n",
    "                date = bucket[\"starting_at\"][:10]\n",
    "                for result in bucket[\"results\"]:\n",
    "                    # Handle both string and numeric amounts\n",
    "                    amount = result.get(\"amount\", 0)\n",
    "                    if isinstance(amount, str):\n",
    "                        try:\n",
    "                            amount = float(amount)\n",
    "                        except (ValueError, TypeError):\n",
    "                            amount = 0\n",
    "\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"date\": date,\n",
    "                            \"workspace_id\": result.get(\n",
    "                                \"workspace_id\", \"\"\n",
    "                            ),  # null for default workspace\n",
    "                            \"description\": result.get(\"description\", \"\"),\n",
    "                            \"currency\": result.get(\"currency\", \"USD\"),\n",
    "                            \"amount_usd\": amount / 100,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # Check if there's more data\n",
    "            if not response.get(\"has_more\", False):\n",
    "                break\n",
    "\n",
    "            next_page = response.get(\"next_page\")\n",
    "            if not next_page:\n",
    "                break\n",
    "\n",
    "        if rows:\n",
    "            with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=rows[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "\n",
    "            print(f\"âœ… Exported {len(rows)} cost records to {output_file}\")\n",
    "        else:\n",
    "            print(f\"No cost data to export for the last {days_back} days\")\n",
    "            print(\"ğŸ’¡ Try increasing days_back or check if you have recent API usage\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cost export failed: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if client:\n",
    "    export_usage_to_csv(client, \"my_usage_data.csv\", days_back=14)\n",
    "    export_costs_to_csv(client, \"my_cost_data.csv\", days_back=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327a1c5",
   "metadata": {
    "id": "5327a1c5"
   },
   "source": "## æ€»ç»“\n\næœ¬æŒ‡å—æ¶µç›–äº†ä½¿ç”¨ä½¿ç”¨ä¸æˆæœ¬ç®¡ç† API çš„åŸºæœ¬æ¨¡å¼ï¼š\n\n- **åŸºæœ¬æŸ¥è¯¢** è·å–ä½¿ç”¨å’Œæˆæœ¬æ•°æ®\n- **åˆ†ç»„å’Œè¿‡æ»¤** è¿›è¡Œè¯¦ç»†åˆ†æ\n- **åˆ†é¡µ** å¤„ç†å¤§å‹æ•°æ®é›†\n- **æˆæœ¬æè¿°è§£æ** è¿›è¡Œåˆ†ç±»\n- **å¸¸è§é™·é˜±** é¿å…é—®é¢˜\n- **ç®€å• CSV å¯¼å‡º** ä¾›å¤–éƒ¨å·¥å…·ä½¿ç”¨\n\n### ä¸‹ä¸€æ­¥\n\n- æŸ¥çœ‹[å®˜æ–¹ API æ–‡æ¡£](https://docs.claude.com)è·å–æœ€æ–°çš„å­—æ®µå®šä¹‰\n- é¦–å…ˆä½¿ç”¨å°æ—¥æœŸèŒƒå›´æµ‹è¯•æ‚¨çš„é›†æˆ\n- è€ƒè™‘æ‚¨çš„ç”¨ä¾‹çš„æ•°æ®ä¿ç•™éœ€æ±‚\n- ç›‘æ§å¯èƒ½å¢å¼ºæ‚¨åˆ†æçš„æ–° API åŠŸèƒ½\n\n### é‡è¦æç¤º\n\n- éšç€ API çš„æˆç†Ÿï¼Œå­—æ®µåç§°å’Œå¯ç”¨é€‰é¡¹å¯èƒ½ä¼šæ¼”å˜\n- åœ¨ç”Ÿäº§ä»£ç ä¸­å§‹ç»ˆä¼˜é›…åœ°å¤„ç†æœªçŸ¥å€¼\n- API è®¾è®¡ç”¨äºå†å²åˆ†æï¼Œè€Œéå®æ—¶ç›‘æ§\n- Priority Tier æˆæœ¬ä½¿ç”¨ä¸åŒçš„è®¡è´¹æ¨¡å¼ï¼Œä¸ä¼šå‡ºç°åœ¨æˆæœ¬ç«¯ç‚¹ä¸­\n\nå¿«ä¹åˆ†æï¼ğŸ“Š"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}