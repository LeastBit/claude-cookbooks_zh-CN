{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 如何使用 Claude 3 和 MongoDB 构建 RAG 系统\n\n\n本教程实现了一个聊天机器人，被提示扮演风险投资技术分析师的角色。该聊天机器人是一个简单的RAG系统，以一组科技新闻文章作为其知识源。\n本笔记本涵盖以下内容：\n\n1. 遵循全面的教程来设置开发环境，从安装必要的库到配置MongoDB数据库。\n2. 学习高效的数据处理方法，包括创建向量搜索索引和准备用于数据摄取和查询处理的数据。\n3. 了解如何在RAG系统中使用Claude 3模型，基于从数据库检索到的上下文信息生成精确的响应。\n\n\n您需要以下内容：\n- Claude API 密钥\n- VoyageAI API 密钥\n- Hugging Face 访问令牌"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 步骤1：库安装、数据加载和准备\n\n\n以下是实现代码中使用的工具和库的简要说明：\n- anthropic:  Antronic的官方Python库，可访问最先进的语言模型。该库提供对Claude 3系列模型的访问，这些模型可以理解文本和图像。\n- datasets: 该库是Hugging Face生态系统的一部分。通过安装'datasets'，我们可以访问许多预处理的和即用的数据集，这些对于训练和微调机器学习模型或基准测试其性能至关重要。\n- pandas: 这个数据科学库提供了强大的数据结构和数据操作、处理和分析方法。\n- voyageai: 这是访问VoyageAI套件嵌入模型的官方Python客户端库。\n- pymongo: PyMongo是MongoDB的Python工具包。它实现了与MongoDB数据库的交互。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkePkAfZLQ_R"
   },
   "outputs": [],
   "source": [
    "!pip install pymongo datasets pandas anthropic voyageai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "下面的代码片段执行以下步骤：\n1. 导入必要的库：\n- `os` 用于与操作系统交互，\n- `requests` 用于发出HTTP请求，\n- `io` 模块中的 `BytesIO` 用于处理内存中的字节对象（如文件），\n- `pandas`（as pd）用于数据操作和分析，以及\n- `userdata` 从google.colab可以访问存储在Google Colab机密中的环境变量。\n2. 函数定义：定义了 `download_and_combine_parquet_files` 函数，有两个参数：\n- `parquet_file_urls`：字符串URL列表，每个URL指向包含tech-news-embedding数据集子集合的Parquet文件。\n- `hf_token` 是表示Hugging Face授权令牌的字符串。访问令牌可以从 [Hugging Face平台](https://huggingface.co/docs/hub/en/security-tokens#:~:text=To%20create%20an%20access%20token,you%27re%20ready%20to%20go!) 创建或复制\n3. 下载和读取Parquet文件：函数遍历parquet_file_urls中的每个URL。对于每个URL，它：\n- 使用requests.get方法发出GET请求，传递URL和授权标头。\n- 检查响应状态码是否为200（OK），表示请求成功。\n- 如果成功，它将响应的内容读入BytesIO对象（以将其作为内存中的文件处理），然后使用pandas.read_parquet从该对象中读取Parquet文件到Pandas DataFrame。\n- 将DataFrame追加到all_dataframes列表。\n4. 合并DataFrame：在将所有Parquet文件下载并读入DataFrame后，检查确保 `all_dataframes` 不为空。如果有要使用的DataFrame，则使用pd.concat将所有DataFrame连接为单个DataFrame，并设置ignore_index=True以重新索引新的合并DataFrame。这个合并的DataFrame是 `download_and_combine_parquet_files` 函数的整体过程输出。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoY9qBGJNo2F"
   },
   "outputs": [],
   "source": "import requests\nfrom io import BytesIO\nimport pandas as pd\nfrom google.colab import userdata\n\n\ndef download_and_combine_parquet_files(parquet_file_urls, hf_token):\n    \"\"\"\n    使用给定的Hugging Face令牌从提供的URL下载Parquet文件，\n    并返回合并的DataFrame。\n\n    参数:\n    - parquet_file_urls: 字符串列表，Parquet文件的URL。\n    - hf_token: 字符串，Hugging Face授权令牌。\n\n    返回:\n    - combined_df: pandas DataFrame，包含来自所有Parquet文件的合并数据。\n    \"\"\"\n    headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n    all_dataframes = []\n\n    for parquet_file_url in parquet_file_urls:\n        response = requests.get(parquet_file_url, headers=headers)\n        if response.status_code == 200:\n            parquet_bytes = BytesIO(response.content)\n            df = pd.read_parquet(parquet_bytes)\n            all_dataframes.append(df)\n        else:\n            print(\n                f\"从 {parquet_file_url} 下载Parquet文件失败: {response.status_code}\"\n            )\n\n    if all_dataframes:\n        combined_df = pd.concat(all_dataframes, ignore_index=True)\n        return combined_df\n    else:\n        print(\"没有DataFrame可连接。\")\n        return None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "下面是本教程所需的Parquet文件列表。完整文件列表位于[这里](https://huggingface.co/datasets/MongoDB/tech-news-embeddings/tree/refs%2Fconvert%2Fparquet/default/train)。每个Parquet文件代表大约45,000个数据点。\n\n在下面的代码片段中，tech-news-embeddings数据集的一个子集被分组为单个DataFrame，然后将其分配给变量 `combined_df`。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sa8PrlAOawR"
   },
   "outputs": [],
   "source": "# 取消注释下面的链接以加载更多数据\n# 完整数据列表请访问: https://huggingface.co/datasets/MongoDB/tech-news-embeddings/tree/refs%2Fconvert%2Fparquet/default/train\nparquet_files = [\n    \"https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0000.parquet\",\n    # \"https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0001.parquet\",\n    # \"https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0002.parquet\",\n    # \"https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0003.parquet\",\n    # \"https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0004.parquet\",\n    # \"https://huggingface.co/api/datasets/AIatMongoDB/tech-news-embeddings/parquet/default/train/0005.parquet\",\n]\n\nhf_token = userdata.get(\"HF_TOKEN\")\ncombined_df = download_and_combine_parquet_files(parquet_files, hf_token)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "作为数据准备的最后阶段，下面的代码片段显示了从分组数据集中删除 `_id` 列的步骤，因为对于本教程的后续步骤来说这是不必要的。此外，每个数据点的embedding列内的数据从numpy数组转换为Python列表，以防止在数据摄取期间发生与不兼容数据类型相关的错误。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEx4i1ehRliI"
   },
   "outputs": [],
   "source": "# 从初始数据集中删除_id列\ncombined_df = combined_df.drop(columns=[\"_id\"])\n\n# 删除初始的嵌入列，因为我们将使用VoyageAI嵌入模型创建新的嵌入\ncombined_df = combined_df.drop(columns=[\"embedding\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "6UsKPi32OWX3",
    "outputId": "231e7041-7b4d-49f7-c7c5-1d81e7e4adea"
   },
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 由于VoyageAI API的速率限制，将用于此演示的文档数量限制为500\n# 更多关于VoyageAI速率限制的信息请访问: https://docs.voyageai.com/docs/rate-limits\nmax_documents = 500\n\nif len(combined_df) > max_documents:\n    combined_df = combined_df[:max_documents]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import voyageai\n\nvo = voyageai.Client(api_key=userdata.get(\"VOYAGE_API_KEY\"))\n\n\ndef get_embedding(text: str) -> list[float]:\n    if not text.strip():\n        print(\"尝试为空文本获取嵌入向量。\")\n        return []\n\n    embedding = vo.embed(text, model=\"voyage-large-2\", input_type=\"document\")\n\n    return embedding.embeddings[0]\n\n\ncombined_df[\"embedding\"] = combined_df[\"description\"].apply(get_embedding)\n\ncombined_df.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXv9MrqLSZw8"
   },
   "source": "## 步骤2：数据库和集合创建\n\n**要创建新的MongoDB数据库，请设置数据库集群：**\n1. 注册一个[免费的MongoDB Atlas账户](https://www.mongodb.com/cloud/atlas/register?utm_campaign=devrel&utm_source=community&utm_medium=cta&utm_content=Partner%20Cookbook&utm_term=richmond.alake)，或对于现有用户，[登录MongoDB Atlas](https://account.mongodb.com/account/login?utm_campaign=devrel&utm_source=community&utm_medium=cta&utm_content=Partner%20Cookbook&utm_term=richmond.alake)\n2. 选择左侧窗格中的\"Database\"选项，这将导航到数据库部署页面，显示任何现有集群的部署规范。通过点击\"+Create\"按钮创建新的数据库集群。\n3. 有关数据库集群设置和获取URI的帮助，请参考我们的MongoDB集群设置指南和获取连接字符串指南。\n注意：在创建概念证明时，不要忘记为Python主机或任何IP的0.0.0.0/0列入白名单。\n4. 成功创建和部署集群后，集群在'数据库部署'页面上变得可访问。\n5. 点击集群的\"Connect\"按钮，查看通过各种语言驱动程序设置与集群连接的选项。\n6. 本教程只需要集群的URI（唯一资源标识符）。获取URI并将其复制到Google Colab机密环境中，变量名为MONGO_URI，或将其放置在.env文件或等效文件中。\n\n\n一旦您创建了集群，导航到集群页面并通过单击+创建数据库在MongoDB Atlas集群内创建数据库和集合。\n数据库将命名为 `tech_news`，集合将命名为 `hacker_noon_tech_news`。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 步骤3：向量搜索索引创建\n\n到目前为止，您已经创建了集群、数据库和集合。\n\n本节中的步骤对于确保可以使用输入到聊天机器人中的查询并在hacker_noon_tech_news集合中的记录中进行搜索来执行向量搜索至关重要。本步骤的目标是创建向量搜索索引。要实现这一点，请参考官方的[向量搜索索引创建指南](https://www.mongodb.com/docs/atlas/atlas-vector-search/create-index/)。\n\n在使用MongoDB Atlas上的JSON编辑器创建向量搜索索引时，请确保您的向量搜索索引命名为vector_index，向量搜索索引定义如下：\n\n```\n{\n \"fields\": [{\n     \"numDimensions\": 1536,\n     \"path\": \"embedding\",\n     \"similarity\": \"cosine\",\n     \"type\": \"vector\"\n   }]\n}\n\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 步骤4：数据摄取\n\n要将数据摄取到前面步骤中创建的MongoDB数据库中。必须执行以下操作：\n- 连接到数据库和集合\n- 清除集合中的任何现有记录\n- 在摄取之前将数据集的Pandas DataFrame转换为字典\n- 使用批量操作将字典摄取到MongoDB中\n\n本教程需要集群的URI（唯一资源标识符）。获取URI并将其复制到Google Colab机密环境中，变量名为MONGO_URI，或将其放置在.env文件或等效文件中。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yv0dMtWnQhDy",
    "outputId": "5bc5276b-46b3-4bc3-f5e4-19802ff910c3"
   },
   "outputs": [],
   "source": "import pymongo\nfrom google.colab import userdata\n\n\ndef get_mongo_client(mongo_uri):\n    \"\"\"与MongoDB建立连接。\"\"\"\n    try:\n        client = pymongo.MongoClient(mongo_uri)\n        print(\"连接MongoDB成功\")\n        return client\n    except pymongo.errors.ConnectionFailure as e:\n        print(f\"连接失败: {e}\")\n        return None\n\n\nmongo_uri = userdata.get(\"MONGO_URI\")\nif not mongo_uri:\n    print(\"MONGO_URI未在环境变量中设置\")\n\nmongo_client = get_mongo_client(mongo_uri)\n\nDB_NAME = \"tech_news\"\nCOLLECTION_NAME = \"hacker_noon_tech_news\"\n\ndb = mongo_client[DB_NAME]\ncollection = db[COLLECTION_NAME]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NowJ2mlQrfT",
    "outputId": "31bfa7ac-dc62-421e-92c5-f644e03d7c1f"
   },
   "outputs": [],
   "source": "# 确保我们使用的是全新集合\n# 删除集合中的任何现有记录\ncollection.delete_many({})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdFsJXXRjEZe"
   },
   "outputs": [],
   "source": "# 数据摄取\ncombined_df_json = combined_df.to_dict(orient=\"records\")\ncollection.insert_many(combined_df_json)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 步骤5：向量搜索\n\n本节展示了创建向量搜索自定义函数的过程，该函数接受用户查询，对应聊天机器人的输入。该函数还接受第二个参数 `collection`，它指向包含要在其上执行向量搜索操作的记录的数据库集合。\n\n`vector_search` 函数产生从MongoDB聚合管道中概述的一系列操作派生的向量搜索结果。该管道包括 `$vectorSearch` 和 `$project` 阶段，并基于用户查询的向量嵌入执行查询。然后它格式化结果，省略对后续过程不必要的任何记录属性。\n\n下面的代码片段执行以下操作以允许对电影进行语义搜索：\n1. 定义 `vector_search` 函数，该函数接受用户的查询字符串和MongoDB集合作为输入，并返回基于向量相似性搜索匹配查询的文档列表。\n2. 通过调用前面定义的函数 `get_embedding` 为用户查询生成嵌入，该函数将查询字符串转换为向量表示。\n3. 为MongoDB的聚合函数构造管道，包含两个主要阶段：`$vectorSearch` 和 `$project`。\n4. `$vectorSearch` 阶段执行实际的向量搜索。index字段指定要用于向量搜索的向量索引，这应与前面步骤中在向量搜索索引定义中输入的名称相对应。queryVector字段采用用户查询的嵌入表示。path字段对应于包含嵌入的文档字段。`numCandidates` 指定要考虑考虑的候选文档数量，以及要返回的结果数量限制。\n5. $project阶段格式化结果以排除_id和`embedding`字段。\n6. 聚合执行定义的管道以获得向量搜索结果。最后的操作将数据库返回的游标转换为列表。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsSBrAu4nSJC"
   },
   "outputs": [],
   "source": "def vector_search(user_query, collection):\n    \"\"\"\n    基于用户查询在MongoDB集合中执行向量搜索。\n\n    Args:\n    user_query (str): 用户的查询字符串。\n    collection (MongoCollection): 要搜索的MongoDB集合。\n\n    Returns:\n    list: 匹配文档的列表。\n    \"\"\"\n\n    # 为用户查询生成嵌入\n    query_embedding = get_embedding(user_query)\n\n    if query_embedding is None:\n        return \"无效查询或嵌入生成失败。\"\n\n    # 定义向量搜索管道\n    pipeline = [\n        {\n            \"$vectorSearch\": {\n                \"index\": \"vector_index\",\n                \"queryVector\": query_embedding,\n                \"path\": \"embedding\",\n                \"numCandidates\": 150,  # 要考虑的候选匹配数\n                \"limit\": 5,  # 返回前5个匹配\n            }\n        },\n        {\n            \"$project\": {\n                \"_id\": 0,  # 排除_id字段\n                \"embedding\": 0,  # 排除嵌入字段\n                \"score\": {\n                    \"$meta\": \"vectorSearchScore\"  # 包含搜索分数\n                },\n            }\n        },\n    ]\n\n    # 执行搜索\n    results = collection.aggregate(pipeline)\n    return list(results)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 步骤6：使用Claude 3模型处理用户查询\n\n教程的最后部分概述了执行的操作序列如下：\n\n- 接受字符串形式的用户查询。\n- 利用VoyageAI嵌入模型为用户查询生成嵌入。\n- 加载Anthropic Claude 3，特别是'claude-opus-4-1'模型，作为RAG系统的基础模型。\n- 使用用户查询的嵌入执行向量搜索，以从知识库中获取相关信息，为基础模型提供额外的上下文。\n- 将用户查询和收集到的其他信息都提交给基础模型以生成响应。\n\n\n一个重要的注意事项是，用户查询嵌入的维度与MongoDB Atlas上向量搜索索引定义中设置的维度相匹配。"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "本节的下一步是导入anthropic库并加载客户端以访问Anthropic的处理消息和访问Claude模型的方法。确保您从[Anthropic官方网站](https://console.anthropic.com/settings/keys)设置页面获取Claude API密钥。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wADfSOFOnw0s"
   },
   "outputs": [],
   "source": "import anthropic\n\nclient = anthropic.Client(api_key=userdata.get(\"ANTHROPIC_API_KEY\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "下面是对下面代码片段中操作的更详细描述：\n\n1. 向量搜索执行：函数首先使用用户的查询和指定的集合作为参数调用 `vector_search`。这在集合内执行搜索，利用向量嵌入来查找与查询相关的信息。\n2. 编译搜索结果：`search_result` 初始化为空字符串，用于聚合搜索中的信息。通过遍历 `vector_search` 函数返回的结果来编译搜索结果，将每个项目的详细信息（标题、公司名称、URL、发布日期、文章URL和描述）格式化为人类可读的字符串，将此信息追加到search_result，在每个条目的末尾添加换行符\\n。\n3. 使用Anthropic客户端生成响应：函数然后构造对Claude API的请求（通过客户端对象，可能是前面创建的anthropic.Client类的实例）。它指定：\n- 要使用的模型（\"claude-opus-4-1\"）表示Claude 3模型的特定版本。\n- 生成响应的最大令牌限制（max_tokens=1024）。\n- 系统描述指导模型表现为\"风险投资技术分析师\"，可以访问科技公司的文章和信息，使用此上下文提供建议。\n- 模型要处理的实际消息将用户查询与聚合搜索结果作为上下文相结合。\n4. 返回生成的响应和搜索结果：从响应的第一个内容项中提取并返回响应文本，以及编译的搜索结果。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrG8Ho50nhVR"
   },
   "outputs": [],
   "source": "def handle_user_query(query, collection):\n    get_knowledge = vector_search(query, collection)\n\n    search_result = \"\"\n    for result in get_knowledge:\n        search_result += (\n            f\"标题: {result.get('title', 'N/A')}, \"\n            f\"公司名称: {result.get('companyName', 'N/A')}, \"\n            f\"公司网址: {result.get('companyUrl', 'N/A')}, \"\n            f\"发布日期: {result.get('published_at', 'N/A')}, \"\n            f\"文章网址: {result.get('url', 'N/A')}, \"\n            f\"描述: {result.get('description', 'N/A')}, \\n\"\n        )\n\n    response = client.messages.create(\n        model=\"claude-opus-4-1\",\n        max_tokens=1024,\n        system=\"你是风险投资技术分析师，可以访问一些科技公司的文章和信息。你使用给你的信息来提供建议。\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"回答这个用户查询: \"\n                + query\n                + \" 包含以下上下文: \"\n                + search_result,\n            }\n        ],\n    )\n\n    return (response.content[0].text), search_result"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "本教程的最后一步是初始化查询，将其传递给 `handle_user_query` 函数并打印返回的响应。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3OcaBbmoY2H",
    "outputId": "06c231ef-cbaa-48cd-e226-9d1c8450b737"
   },
   "outputs": [],
   "source": "# 执行查询并检索来源\nquery = \"给我最好的科技股投资并告诉我为什么\"\nresponse, source_information = handle_user_query(query, collection)\n\nprint(f\"响应: {response}\")\nprint(f\"\\n来源信息: \\n{source_information}\")"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}