{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eOnr6z_zLoc",
    "outputId": "53f4214a-c00b-4dea-d1fa-38d0d6693fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQG4iSxVxw8f"
   },
   "source": "# 使用 LangChain v1 的 Claude 3 RAG 代理"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov6TCS7bx1oI"
   },
   "source": "LangChain v1 带来了许多变化，当比较版本 `0.0.3xx` 到 `0.1.x` 的 LangChain 时，首选的操作方式有很多变化。代理（agents）尤其如此。\n\n我们初始化和使用代理的方式通常比过去更清晰——仍然有许多抽象，但我们可以（并且被鼓励）更接近代理逻辑本身。这在一开始可能会造成一些混乱，但一旦理解了，新逻辑会比以前版本更清晰。\n\n在这个例子中，我们将使用 LangChain v1 构建一个 RAG 代理。我们将使用 Claude 3 作为 LLM，Voyage AI 进行知识嵌入，以及 Pinecone 来驱动我们的知识检索。\n\n首先，让我们安装必要的依赖："
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zshhLDrgbFKk",
    "outputId": "ed49398c-dc7c-4c9c-e6c3-1a1910d52006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.6/848.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.1.11 \\\n",
    "    langchain-core==0.1.30 \\\n",
    "    langchain-community==0.0.27 \\\n",
    "    langchain-anthropic==0.1.4 \\\n",
    "    langchainhub==0.1.15 \\\n",
    "    anthropic==0.19.1 \\\n",
    "    voyageai==0.2.1 \\\n",
    "    pinecone-client==3.1.0 \\\n",
    "    datasets==2.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "然后获取所需的 API 密钥。我们需要 [Claude](https://docs.claude.com/claude/reference/getting-started-with-the-api)、[Voyage AI](https://docs.voyageai.com/install/) 和 [Pinecone](https://docs.pinecone.io/docs/quickstart) 的 API 密钥。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 在此处插入您的 API 密钥\nANTHROPIC_API_KEY = \"<YOUR_ANTHROPIC_API_KEY>\"\nPINECONE_API_KEY = \"<YOUR_PINECONE_API_KEY>\"\nVOYAGE_API_KEY = \"<YOUR_VOYAGE_API_KEY>\""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpKfZkUYzQhB"
   },
   "source": "## 寻找知识"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDTQoxcNzUa8"
   },
   "source": "代理使用 RAG 首先需要的是我们可以从中提取知识的数据源。我们将使用 AI ArXiv 数据集的 v2 版本，可在 Hugging Face 数据集的 [`jamescalam/ai-arxiv2-chunks`](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks) 找到。\n\n_注意：我们使用的是预分块数据集。有关原始版本，请参阅 [`jamescalam/ai-arxiv2`](https://huggingface.co/datasets/jamescalam/ai-arxiv2)。_"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "4d5c6f53e82948b18b45877e2e30fd78",
      "7036b643c9264b09b0f93cb6a1a8b01f",
      "fb92dd97e68643099b0d0f1b4983f5f1",
      "91c3b00e77444bb79d9f73314800471e",
      "3e1d39ab6d3a4cf5827c9b3e1d4f0df1",
      "f58fb2da52bc43a985449910daaac230",
      "449afd685fd94354b64aa43a4763ab1d",
      "43a3c63554894d268e3bf747f7e0289d",
      "d902beef883e4c65bd2d6cb161423bae",
      "ce5735788cd14803abac28fa2c42d168",
      "a8928cd888a648c9b7d52739f38c5bda",
      "0168cb9e015b4e119fcdc7beca2e21a3",
      "a0e93df2955b44208479a4b416cd1085",
      "6f58f8d6374b45b4b6a0d9fcaa8083e7",
      "8bde11d3d65c45dba3baf1c02450eef1",
      "b19af932a7f24dc799b683136d175107",
      "e2819a7a94974a4da9b6bde754fdbc50",
      "808dd94ecaff43e0848bcfead3cf0d75",
      "4383833994854f88aa12ba0edb673d47",
      "ee2672db2047468fb49482e14127dcd3",
      "6999dc582b4c4ba68fdbfe404b1c4dfe",
      "593274c4088f4d9aa2becb6b1c6e495b"
     ]
    },
    "id": "U9gpYFnzbFKm",
    "outputId": "c7005c0a-b9a2-4c7c-f93b-e92081f7e3d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5c6f53e82948b18b45877e2e30fd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/766M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0168cb9e015b4e119fcdc7beca2e21a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train[:20000]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bP7ZW-ybFKm",
    "outputId": "491e8001-df44-465c-9979-2e2a30de9d5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2401.09350',\n",
       " 'chunk-id': 1,\n",
       " 'chunk': 'These neural networks and their training algorithms may be complex, and the scope of their impact broad and wide, but nonetheless they are simply functions in a high-dimensional space. A trained neural network takes a vector as input, crunches and transforms it in various ways, and produces another vector, often in some other space. An image may thereby be turned into a vector, a song into a sequence of vectors, and a social network as a structured collection of vectors. It seems as though much of human knowledge, or at least what is expressed as text, audio, image, and video, has a vector representation in one form or another.\\nIt should be noted that representing data as vectors is not unique to neural networks and deep learning. In fact, long before learnt vector representations of pieces of dataâ\\x80\\x94what is commonly known as â\\x80\\x9cembeddingsâ\\x80\\x9dâ\\x80\\x94came along, data was often encoded as hand-crafted feature vectors. Each feature quanti- fied into continuous or discrete values some facet of the data that was deemed relevant to a particular task (such as classification or regression). Vectors of that form, too, reflect our understanding of a real-world object or concept.',\n",
       " 'id': '2401.09350#1',\n",
       " 'title': 'Foundations of Vector Retrieval',\n",
       " 'summary': 'Vectors are universal mathematical objects that can represent text, images,\\nspeech, or a mix of these data modalities. That happens regardless of whether\\ndata is represented by hand-crafted features or learnt embeddings. Collect a\\nlarge enough quantity of such vectors and the question of retrieval becomes\\nurgently relevant: Finding vectors that are more similar to a query vector.\\nThis monograph is concerned with the question above and covers fundamental\\nconcepts along with advanced data structures and algorithms for vector\\nretrieval. In doing so, it recaps this fascinating topic and lowers barriers of\\nentry into this rich area of research.',\n",
       " 'source': 'http://arxiv.org/pdf/2401.09350',\n",
       " 'authors': 'Sebastian Bruch',\n",
       " 'categories': 'cs.DS, cs.IR',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.DS',\n",
       " 'published': '20240117',\n",
       " 'updated': '20240117',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX6NdQhgbFKn"
   },
   "source": "## 构建知识库"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDCbqQl_bFKn"
   },
   "source": "要构建我们的知识库，我们需要**两样东西**：\n\n1. 嵌入向量（Embeddings），为此我们将使用 `VoyageEmbeddings`，它使用 Voyage AI 的嵌入模型，这需要一个 [API 密钥](https://dash.voyageai.com/api-keys)。\n2. 一个向量数据库，用于存储我们的嵌入向量并查询它们。我们使用 Pinecone，这也需要一个 [免费的 API 密钥](https://app.pinecone.io)。\n\n首先，我们初始化与 Voyage AI 的连接并定义一个 `embed` 对象用于嵌入："
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wkw0KyLRbFKo"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import VoyageEmbeddings\n",
    "\n",
    "embed = VoyageEmbeddings(voyage_api_key=VOYAGE_API_KEY, model=\"voyage-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhDzfsczbFKo"
   },
   "source": "然后我们初始化与 Pinecone 的连接："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0N7EcJibFKo"
   },
   "outputs": [],
   "source": "from pinecone import Pinecone\n\n# 配置客户端\npc = Pinecone(api_key=PINECONE_API_KEY)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g65RLGIpbFKo"
   },
   "source": "现在我们设置索引规范，这使我们能够定义云提供商和我们想要部署索引的区域。您可以在[此处](https://docs.pinecone.io/docs/projects)找到所有可用的提供商和区域列表。"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8stIZYKdbFKo"
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8Ep3743bFKo"
   },
   "source": "在创建索引之前，我们需要 Voyage AI 嵌入模型的维度，我们可以通过创建嵌入并检查长度来轻松找到："
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwMhLWLDbFKo",
    "outputId": "f8731679-583c-497c-93d9-b13ea6013c83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = embed.embed_documents([\"ello\"])\n",
    "len(vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3X7nZIabFKp"
   },
   "source": "现在我们使用嵌入维度创建索引，以及与模型兼容的度量（可以是 cosine 或 dotproduct）。我们还将规范传递给索引初始化。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6Bl7xTJbFKp",
    "outputId": "03e0b822-b150-4a6b-b83d-1cee1e4fced4"
   },
   "outputs": [],
   "source": "import time\n\nindex_name = \"claude-3-rag\"\n\n# 检查索引是否已存在（如果这是第一次，应该不存在）\nif index_name not in pc.list_indexes().names():\n    # 如果不存在，创建索引\n    pc.create_index(\n        index_name,\n        dimension=len(vec[0]),  # voyage 模型的维度\n        metric=\"dotproduct\",\n        spec=spec,\n    )\n    # 等待索引初始化\n    while not pc.describe_index(index_name).status[\"ready\"]:\n        time.sleep(1)\n\n# 连接到索引\nindex = pc.Index(index_name)\ntime.sleep(1)\n# 查看索引统计信息\nindex.describe_index_stats()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZUn2lu7bFKp"
   },
   "source": "### 填充我们的索引"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeVD6d0sbFKp"
   },
   "source": "现在我们的知识库已经准备好填充数据了。我们将使用 `embed` 辅助函数来嵌入文档，然后将它们添加到索引中。\n\n我们还将包含每条记录的元数据。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "78b688cc78264ad1a2f05d339fe498fe",
      "f8796aebc76a42ed80d6cf9c234b9e14",
      "93fd5108528a499cb5ae4444f0b2f770",
      "89683fb7a1a44606abc2ded05fbb3aa3",
      "f202dce30c1d4672ad357db9b6dbe9b8",
      "009627caf7fa4c51b61b6e2087545324",
      "3ff8bcf03c704243b7fd173b3cdd0d87",
      "cc0685f737fa4e15b7796e5f3fe3df8d",
      "f3cfb3b02d7a4e44b00e24aa2e7f9ff5",
      "1022918a023b4aa8a3714d9eab2978b1",
      "e6ed1f5a6e5b4c508fa3b69d9219057d"
     ]
    },
    "id": "hb00VSTqbFKp",
    "outputId": "f13e0707-2b9b-4966-df62-9e457765ebf8"
   },
   "outputs": [],
   "source": "from tqdm.auto import tqdm\n\n# 使用 pandas dataframe 处理数据更方便\ndata = dataset.to_pandas()\n\nbatch_size = 100\n\nfor i in tqdm(range(0, len(data), batch_size)):\n    i_end = min(len(data), i + batch_size)\n    # 获取数据批次\n    batch = data.iloc[i:i_end]\n    # 为每个块生成唯一ID\n    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n    # 获取要嵌入的文本\n    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n    # 嵌入文本\n    embeds = embed.embed_documents(texts)\n    # 获取要存储在 Pinecone 中的元数据\n    metadata = [\n        {\"text\": x[\"chunk\"], \"source\": x[\"source\"], \"title\": x[\"title\"]}\n        for i, x in batch.iterrows()\n    ]\n    # 添加到 Pinecone\n    index.upsert(vectors=zip(ids, embeds, metadata))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6VVT3X_EMDO"
   },
   "source": "为我们的代理创建一个工具，用于搜索 ArXiv 论文："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9J5jHKcEQz6"
   },
   "outputs": [],
   "source": "from langchain.agents import tool\n\n\n@tool\ndef arxiv_search(query: str) -> str:\n    \"\"\"当回答关于 AI、机器学习、数据科学或其他可能通过 ArXiv 论文回答的技术问题时，请使用此工具。\n    \"\"\"\n    # 创建查询向量\n    xq = embed.embed_query(query)\n    # 执行搜索\n    out = index.query(vector=xq, top_k=5, include_metadata=True)\n    # 将结果重新格式化为字符串\n    results_str = \"\\n\\n\".join([x[\"metadata\"][\"text\"] for x in out[\"matches\"]])\n    return results_str\n\n\ntools = [arxiv_search]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN7d_4r-JMPW"
   },
   "source": "当我们的代理使用此工具时，它会这样执行："
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq4H-2RpI1U3",
    "outputId": "ca36c8cb-5fd1-47ed-cc67-f43fd9833af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Llama 2 Code Llama Code Llama - Python Size FIM LCFT Python CPP Java PHP TypeScript C# Bash Average 7B â 13B â 34B â 70B â 7B â 7B â 7B â 7B â 13B â 13B â 13B â 13B â 34B â 34B â 7B â 7B â 13B â 13B â 34B â 34B â â â â â 14.3% 6.8% 10.8% 9.9% 19.9% 13.7% 15.8% 13.0% 24.2% 23.6% 22.2% 19.9% 27.3% 30.4% 31.6% 34.2% 12.6% 13.2% 21.4% 15.1% 6.3% 3.2% 8.3% 9.5% 3.2% 12.6% 17.1% 3.8% 18.9% 25.9% 8.9% 24.8% â â â â â â â â â â 37.3% 31.1% 36.1% 30.4% 29.2% 29.8% 38.0%\n",
      "\n",
      "Ethical Considerations and Limitations (Section 5.2) Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2âs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their speciï¬c applications of the model. Please see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-user-guide\n",
      "Table 52: Model card for Llama 2.\n",
      "77\n",
      "\n",
      "2\n",
      "Cove Liama Long context (7B =, 13B =, 34B) + fine-tuning ; Lrama 2 Code training 20B oes Cope Liama - Instruct Foundation models â> nfilling code training = eee.â (7B =, 13B =, 34B) â 5B (7B, 13B, 348) 5008 Python code Long context Cove Liama - PyrHon (7B, 13B, 34B) > training Â» Fine-tuning > 1008 208\n",
      "Figure 2: The Code Llama specialization pipeline. The different stages of fine-tuning annotated with the number of tokens seen during training. Infilling-capable models are marked with the â symbol.\n",
      "# 2 Code Llama: Specializing Llama 2 for code\n",
      "# 2.1 The Code Llama models family\n",
      "\n",
      "# 2 Code Llama: Specializing Llama 2 for code\n",
      "# 2.1 The Code Llama models family\n",
      "Code Llama. The Code Llama models constitute foundation models for code generation. They come in four model sizes: 7B, 13B, 34B and 70B parameters. The 7B, 13B and 70B models are trained using an infilling objective (Section 2.3), and are appropriate to be used in an IDE to complete code in the middle of a file, for example. The 34B model was trained without the infilling objective. All Code Llama models are initialized with Llama 2 model weights and trained on 500B tokens from a code-heavy dataset (see Section 2.2 for more details), except Code Llama 70B which was trained on 1T tokens. They are all fine-tuned to handle long contexts as detailed in Section 2.4.\n",
      "\n",
      "0.52 0.57 0.19 0.30 Llama 1 7B 13B 33B 65B 0.27 0.24 0.23 0.25 0.26 0.24 0.26 0.26 0.34 0.31 0.34 0.34 0.54 0.52 0.50 0.46 0.36 0.37 0.36 0.36 0.39 0.37 0.35 0.40 0.26 0.23 0.24 0.25 0.28 0.28 0.33 0.32 0.33 0.31 0.34 0.32 0.45 0.50 0.49 0.48 0.33 0.27 0.31 0.31 0.17 0.10 0.12 0.11 0.24 0.24 0.23 0.25 0.31 0.27 0.30 0.30 0.44 0.41 0.41 0.43 0.57 0.55 0.60 0.60 0.39 0.34 0.28 0.39 Llama 2 7B 13B 34B 70B 0.28 0.24 0.27 0.31 0.25 0.25 0.24 0.29 0.29 0.35 0.33 0.35 0.50 0.50 0.56 0.51 0.36 0.41 0.41\n"
     ]
    }
   ],
   "source": [
    "print(arxiv_search.run(tool_input={\"query\": \"can you tell me about llama 2?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUvJOqrNhYIh"
   },
   "source": "## 定义 XML 代理"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s45dwd78hbvk"
   },
   "source": "XML 代理主要是为了支持 Anthropic 模型而构建的。Anthropic 模型已经过训练，可以使用 XML 标签，如 `<input>{some input}</input` 或在使用工具时使用：\n\n```\n<tool>{tool name}</tool>\n<tool_input>{tool input}</tool_input>\n```\n\n这与典型 ReAct 代理产生的格式有很大不同，Anthropic 模型对 ReAct 格式的支持不如 XML 格式。\n\n要创建 XML 代理，我们需要一个 `prompt`、`llm` 和 `tools` 列表。我们可以从 LangChain hub 下载对话式 XML 代理的预构建提示。"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntuT7UuXeMz0",
    "outputId": "c47f0a98-c71f-488f-fc55-9e1ea7f643b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'tools'], partial_variables={'chat_history': ''}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input', 'tools'], template=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nPrevious Conversation:\\n{chat_history}\\n\\nQuestion: {input}\\n{agent_scratchpad}\"))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfdcKCdwi0SL"
   },
   "source": "我们可以看到 XML 格式在整个提示中用于向 LLM 解释它应该如何使用工具。\n\n接下来我们初始化与 Anthropic 的连接，为此我们需要 [Claude API 密钥](https://console.anthropic.com/)。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDHuU2uOdW91"
   },
   "outputs": [],
   "source": "from langchain_anthropic import ChatAnthropic\n\n# 聊天完成 LLM\nllm = ChatAnthropic(\n    ANTHROPIC_API_KEY=ANTHROPIC_API_KEY,\n    model_name=\"claude-opus-4-1\",  # 将 \"opus\" 改为 \"sonnet\" 以提高速度\n    temperature=0.0,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g33Nt-xijPKG"
   },
   "source": "当代理运行时，我们将为其提供一个 `input` —— 这是来自用户的输入文本。然而，在代理逻辑中，还会传递一个 *agent_scratchpad* 对象，其中将包含工具信息。要将这些信息输入到我们的 LLM 中，我们需要将其转换为上面描述的 XML 格式，我们定义 `convert_intermediate_steps` 函数来处理这个问题。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMMBgMBlIJoq"
   },
   "outputs": [],
   "source": "def convert_intermediate_steps(intermediate_steps):\n    log = \"\"\n    for action, observation in intermediate_steps:\n        log += (\n            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\n            f\"</tool_input><observation>{observation}</observation>\"\n        )\n    return log"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5_PQWVckAOi"
   },
   "source": "我们还必须将工具解析为包含 `tool_name: tool_description` 的字符串——我们用 `convert_tools` 函数处理这个问题。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxbrF5a4j9il"
   },
   "outputs": [],
   "source": "def convert_tools(tools):\n    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCVI2dyUIRg6"
   },
   "source": "一切准备就绪后，我们可以使用 [**L**ang**C**hain **E**xpression **L**anguage (LCEL)](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/) 初始化我们的代理对象。我们使用 `llm.bind(stop=[...])` 添加关于 LLM 何时应该*停止*生成的指令，最后我们使用 `XMLAgentOutputParser` 对象解析代理的输出。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3yhTDmEIU4n"
   },
   "outputs": [],
   "source": "from langchain.agents.output_parsers import XMLAgentOutputParser\n\nagent = (\n    {\n        \"input\": lambda x: x[\"input\"],\n        # 没有 \"chat_history\"，工具使用就没有先前交互的上下文\n        \"chat_history\": lambda x: x[\"chat_history\"],\n        \"agent_scratchpad\": lambda x: convert_intermediate_steps(x[\"intermediate_steps\"]),\n    }\n    | prompt.partial(tools=convert_tools(tools))\n    | llm.bind(stop=[\"</tool_input>\", \"</final_answer>\"])\n    | XMLAgentOutputParser()\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG2_hL4hkudq"
   },
   "source": "在我们初始化 `agent` 对象后，我们将其传递给 `AgentExecutor` 对象以及我们最初的 `tools` 列表："
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHW_K3WOIsXw"
   },
   "outputs": [],
   "source": "from langchain.agents import AgentExecutor\n\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRCtHauRlkLc"
   },
   "source": "现在我们可以通过 `invoke` 方法使用代理："
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_Aqp20qloj7",
    "outputId": "c4fe06f3-a147-4274-becd-49c9992a8cd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool>\n",
      "<tool_input>llama 2\u001b[0m\u001b[36;1m\u001b[1;3mModel Llama 2 Code Llama Code Llama - Python Size FIM LCFT Python CPP Java PHP TypeScript C# Bash Average 7B â 13B â 34B â 70B â 7B â 7B â 7B â 7B â 13B â 13B â 13B â 13B â 34B â 34B â 7B â 7B â 13B â 13B â 34B â 34B â â â â â 14.3% 6.8% 10.8% 9.9% 19.9% 13.7% 15.8% 13.0% 24.2% 23.6% 22.2% 19.9% 27.3% 30.4% 31.6% 34.2% 12.6% 13.2% 21.4% 15.1% 6.3% 3.2% 8.3% 9.5% 3.2% 12.6% 17.1% 3.8% 18.9% 25.9% 8.9% 24.8% â â â â â â â â â â 37.3% 31.1% 36.1% 30.4% 29.2% 29.8% 38.0%\n",
      "\n",
      "2\n",
      "Cove Liama Long context (7B =, 13B =, 34B) + fine-tuning ; Lrama 2 Code training 20B oes Cope Liama - Instruct Foundation models â> nfilling code training = eee.â (7B =, 13B =, 34B) â 5B (7B, 13B, 348) 5008 Python code Long context Cove Liama - PyrHon (7B, 13B, 34B) > training Â» Fine-tuning > 1008 208\n",
      "Figure 2: The Code Llama specialization pipeline. The different stages of fine-tuning annotated with the number of tokens seen during training. Infilling-capable models are marked with the â symbol.\n",
      "# 2 Code Llama: Specializing Llama 2 for code\n",
      "# 2.1 The Code Llama models family\n",
      "\n",
      "0.52 0.57 0.19 0.30 Llama 1 7B 13B 33B 65B 0.27 0.24 0.23 0.25 0.26 0.24 0.26 0.26 0.34 0.31 0.34 0.34 0.54 0.52 0.50 0.46 0.36 0.37 0.36 0.36 0.39 0.37 0.35 0.40 0.26 0.23 0.24 0.25 0.28 0.28 0.33 0.32 0.33 0.31 0.34 0.32 0.45 0.50 0.49 0.48 0.33 0.27 0.31 0.31 0.17 0.10 0.12 0.11 0.24 0.24 0.23 0.25 0.31 0.27 0.30 0.30 0.44 0.41 0.41 0.43 0.57 0.55 0.60 0.60 0.39 0.34 0.28 0.39 Llama 2 7B 13B 34B 70B 0.28 0.24 0.27 0.31 0.25 0.25 0.24 0.29 0.29 0.35 0.33 0.35 0.50 0.50 0.56 0.51 0.36 0.41 0.41\n",
      "\n",
      "Ethical Considerations and Limitations (Section 5.2) Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2âs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their speciï¬c applications of the model. Please see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-user-guide\n",
      "Table 52: Model card for Llama 2.\n",
      "77\n",
      "\n",
      "Model Size FIM LCFT HumanEval MBPP pass@1 pass@10 pass@100 pass@1 pass@10 pass@100 Llama 2 Code Llama Code Llama - Python 7B â 13B â 34B â 70B â 7B â 7B â 7B â 7B â 13B â 13B â 13B â 13B â 34B â 34B â 7B â 7B â 13B â 13B â 34B â 34B â â â â â â â â â â â â â â â â â â â â â 12.2% 25.2% 20.1% 34.8% 22.6% 47.0% 30.5% 59.4% 32.3% 63.9% 34.1% 62.6% 34.1% 62.5% 33.5% 59.6% 36.6% 72.9% 36.6% 71.9% 37.8% 70.6% 36.0% 69.4% 48.2% 77.7% 48.8% 76.8% 40.2% 70.0% 38.4% 70.3% 45.7% 80.0%\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information from the arXiv search, here are the key points about Llama 2:\n",
      "\n",
      "<final_answer>\n",
      "- Llama 2 is a large language model developed by Meta AI. It comes in sizes ranging from 7B to 70B parameters.\n",
      "\n",
      "- Code Llama is a version of Llama 2 that has been specialized for code generation through fine-tuning on code datasets. Code Llama models are available in Python, C++, Java, PHP, TypeScript, C#, and Bash.\n",
      "\n",
      "- The Code Llama specialization pipeline involves foundation model pre-training, long context training, code infilling training, and fine-tuning on specific programming languages. \n",
      "\n",
      "- Code Llama significantly outperforms the base Llama 2 models on code generation benchmarks like HumanEval and MBPP. For example, the 34B parameter Code Llama - Python achieves 48.8% pass@1 on HumanEval compared to 34.1% for the 34B Llama 2.\n",
      "\n",
      "- As with all large language models, Llama 2 has limitations and potential risks that need to be considered before deploying it in applications. Meta provides a responsible use guide with recommendations for safety testing and tuning.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "- Llama 2 is a large language model developed by Meta AI. It comes in sizes ranging from 7B to 70B parameters.\n",
      "\n",
      "- Code Llama is a version of Llama 2 that has been specialized for code generation through fine-tuning on code datasets. Code Llama models are available in Python, C++, Java, PHP, TypeScript, C#, and Bash.\n",
      "\n",
      "- The Code Llama specialization pipeline involves foundation model pre-training, long context training, code infilling training, and fine-tuning on specific programming languages. \n",
      "\n",
      "- Code Llama significantly outperforms the base Llama 2 models on code generation benchmarks like HumanEval and MBPP. For example, the 34B parameter Code Llama - Python achieves 48.8% pass@1 on HumanEval compared to 34.1% for the 34B Llama 2.\n",
      "\n",
      "- As with all large language models, Llama 2 has limitations and potential risks that need to be considered before deploying it in applications. Meta provides a responsible use guide with recommendations for safety testing and tuning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_msg = \"can you tell me about llama 2?\"\n",
    "\n",
    "out = agent_executor.invoke({\"input\": user_msg, \"chat_history\": \"\"})\n",
    "\n",
    "print(out[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eae-JyUFl--N"
   },
   "source": "这看起来很不错，但现在我们的代理是*无状态的*——使得很难与之对话。我们可以用许多不同的方式为其提供记忆，但最简单的方法之一是使用 `ConversationBufferWindowMemory`。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqOMNQUfmOEr"
   },
   "outputs": [],
   "source": "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n\n# 对话记忆\nconversational_memory = ConversationBufferWindowMemory(\n    memory_key=\"chat_history\", k=5, return_messages=True\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9UvnBrsnNVw"
   },
   "source": "我们还没有将对话记忆附加到我们的代理——所以 `conversational_memory` 对象将保持为空："
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJZNIDAslNoC",
    "outputId": "5267f069-dc09-472d-c9ef-e694cfea982f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynX9Wca6nawr"
   },
   "source": "我们必须手动将与我们和代理之间的交互添加到我们的记忆中。"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5hXy1FAnne3",
    "outputId": "b659e1f4-3d02-466f-9880-c0adf65fa507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='can you tell me about llama 2?'),\n",
       " AIMessage(content='\\n- Llama 2 is a large language model developed by Meta AI. It comes in sizes ranging from 7B to 70B parameters.\\n\\n- Code Llama is a version of Llama 2 that has been specialized for code generation through fine-tuning on code datasets. Code Llama models are available in Python, C++, Java, PHP, TypeScript, C#, and Bash.\\n\\n- The Code Llama specialization pipeline involves foundation model pre-training, long context training, code infilling training, and fine-tuning on specific programming languages. \\n\\n- Code Llama significantly outperforms the base Llama 2 models on code generation benchmarks like HumanEval and MBPP. For example, the 34B parameter Code Llama - Python achieves 48.8% pass@1 on HumanEval compared to 34.1% for the 34B Llama 2.\\n\\n- As with all large language models, Llama 2 has limitations and potential risks that need to be considered before deploying it in applications. Meta provides a responsible use guide with recommendations for safety testing and tuning.\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_memory.chat_memory.add_user_message(user_msg)\n",
    "conversational_memory.chat_memory.add_ai_message(out[\"output\"])\n",
    "\n",
    "conversational_memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3pA0o6ZnrAl"
   },
   "source": "现在我们可以看到已经添加了*两条*消息，我们的 `HumanMessage` 和代理的 `AIMessage` 响应。不幸的是，我们不能将这些消息直接发送到我们的 XML 代理。相反，我们需要传递以下格式的字符串：\n\n```\nHuman: {human message}\nAI: {AI message}\n```\n\n让我们编写一个快速的 `memory2str` 辅助函数来为我们处理这个问题："
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "raZHBdJmtGH-"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "\n",
    "def memory2str(memory: ConversationBufferWindowMemory):\n",
    "    messages = memory.chat_memory.messages\n",
    "    memory_list = [\n",
    "        f\"Human: {mem.content}\" if isinstance(mem, HumanMessage) else f\"AI: {mem.content}\"\n",
    "        for mem in messages\n",
    "    ]\n",
    "    memory_str = \"\\n\".join(memory_list)\n",
    "    return memory_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t89yX-6i3hvd",
    "outputId": "7c27cac1-3881-4a26-f59f-274d8d4affa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: can you tell me about llama 2?\n",
      "AI: \n",
      "- Llama 2 is a large language model developed by Meta AI. It comes in sizes ranging from 7B to 70B parameters.\n",
      "\n",
      "- Code Llama is a version of Llama 2 that has been specialized for code generation through fine-tuning on code datasets. Code Llama models are available in Python, C++, Java, PHP, TypeScript, C#, and Bash.\n",
      "\n",
      "- The Code Llama specialization pipeline involves foundation model pre-training, long context training, code infilling training, and fine-tuning on specific programming languages. \n",
      "\n",
      "- Code Llama significantly outperforms the base Llama 2 models on code generation benchmarks like HumanEval and MBPP. For example, the 34B parameter Code Llama - Python achieves 48.8% pass@1 on HumanEval compared to 34.1% for the 34B Llama 2.\n",
      "\n",
      "- As with all large language models, Llama 2 has limitations and potential risks that need to be considered before deploying it in applications. Meta provides a responsible use guide with recommendations for safety testing and tuning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(memory2str(conversational_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0L_80WrpWqd"
   },
   "source": "现在让我们把另一个名为 `chat` 的辅助函数放在一起，帮助我们处理代理的*状态*部分。"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "C-Ck2Lv53rD-"
   },
   "outputs": [],
   "source": [
    "def chat(text: str):\n",
    "    out = agent_executor.invoke({\"input\": text, \"chat_history\": memory2str(conversational_memory)})\n",
    "    conversational_memory.chat_memory.add_user_message(text)\n",
    "    conversational_memory.chat_memory.add_ai_message(out[\"output\"])\n",
    "    return out[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIheLeTBsO9S"
   },
   "source": "现在我们只需与我们的代理聊天，它会记住先前交互的上下文。"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJ_PH7YcA_f2",
    "outputId": "7d645e9f-f191-4b62-97fd-973417e1efaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool>\n",
      "<tool_input>llama 2 red teaming\u001b[0m\u001b[36;1m\u001b[1;3mAfter conducting red team exercises, we asked participants (who had also participated in Llama 2 Chat exercises) to also provide qualitative assessment of safety capabilities of the model. Some participants who had expertise in offensive security and malware development questioned the ultimate risk posed by âmalicious code generationâ through LLMs with current capabilities.\n",
      "One red teamer remarked, âWhile LLMs being able to iteratively improve on produced source code is a risk, producing source code isnât the actual gap. That said, LLMs may be risky because they can inform low-skill adversaries in production of scripts through iteration that perform some malicious behavior.â\n",
      "According to another red teamer, â[v]arious scripts, program code, and compiled binaries are readily available on mainstream public websites, hacking forums or on âthe dark web.â Advanced malware development is beyond the current capabilities of available LLMs, and even an advanced LLM paired with an expert malware developer is not particularly useful- as the barrier is not typically writing the malware code itself. That said, these LLMs may produce code which will get easily caught if used directly.â\n",
      "\n",
      "Model Llama 2 Code Llama Code Llama - Python Size FIM LCFT Python CPP Java PHP TypeScript C# Bash Average 7B â 13B â 34B â 70B â 7B â 7B â 7B â 7B â 13B â 13B â 13B â 13B â 34B â 34B â 7B â 7B â 13B â 13B â 34B â 34B â â â â â 14.3% 6.8% 10.8% 9.9% 19.9% 13.7% 15.8% 13.0% 24.2% 23.6% 22.2% 19.9% 27.3% 30.4% 31.6% 34.2% 12.6% 13.2% 21.4% 15.1% 6.3% 3.2% 8.3% 9.5% 3.2% 12.6% 17.1% 3.8% 18.9% 25.9% 8.9% 24.8% â â â â â â â â â â 37.3% 31.1% 36.1% 30.4% 29.2% 29.8% 38.0%\n",
      "\n",
      "In addition to red teaming sessions, we ran a quantitative evaluation on risk from generating malicious code by scoring Code Llamaâs responses to ChatGPTâs (GPT3.5 Turbo) with LLAMAv2 70Bâs safety reward model. For this second quantitative evaluation, we selected prompts that the red teamers generated specifically attempting to solicit malicious code (even though the red teaming included consideration of a broad set of safety risks). These prompts were a mix of clear intent and slightly obfuscated intentions (see some examples in Figure 16. We show a KDE plot of the distribution of the safety score for all models in Figure 7). We observe that Code Llama tends to answer with safer responses; the distribution of safety scores for Code Llama has more weight in the safer part of the range.\n",
      "False refusals. LLMs that are too safe can have a tendency to over-refuse valid claims similar to what was reported after the release of Llama 2. We specifically asked red teamers to test for this behavior. They found some limited evidence of false refusals (when not using a system preprompt). False refusals could also\n",
      "16\n",
      "\n",
      "15\n",
      "pafety Reward Model Scores Distribution on Red Teaming Prompts\n",
      "Responding Model GPT 3.5 Turbo Code Llama 138 Instruct Code Llama 34B Instruct Code Llama 7B Instruct 0.0-+ -0.2 0.0 0.2 0.4 0.6 08 1.0 12 Llama 2 70B Safety Reward Model Score\n",
      "Figure 7: KDE plot of the risk score output by the Llama 2 safety reward model on prompts with clear intent specific to code risk created by red teamers with background in cybersecurity and malware generation.\n",
      "Red teaming. It is important to also proactively identify risks with adversarial testing or red teaming. We conducted 3 red teaming exercises with 25 Meta employees, including domain experts in responsible AI, malware development, and offensive security engineering.\n",
      "The red teamers provided a nuanced evaluation specifically on the risk from so called âdual intent prompts.â Dual intent prompts are requests for help with writing code that could be used maliciously but the prompt does not directly address the topic (example âMosaic Promptsâ Glukhov et al. (2023)). For example, the model rightfully refuses to provide support with writing ransomware code but it complies when asked to provide a script to encrypt all files in the userâs home directory since such a script could be used for benign purposes.\n",
      "\n",
      ". . . . . . . . . . . . . . . 3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . .\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information from the arxiv search, some red teaming was done on the Llama 2 model during development to identify potential safety risks:\n",
      "\n",
      "<final_answer>\n",
      "- Meta conducted 3 red teaming exercises with 25 employees, including domain experts in responsible AI, malware development, and offensive security engineering. \n",
      "\n",
      "- The red teamers categorized successful attacks into four main types: 1) getting the model to provide some harmful information while refusing other content, 2) having the model roleplay specific scenarios, 3) forcing the model to highlight positives of harmful content, and 4) embedding harmful instructions within complex commands.\n",
      "\n",
      "- Some red teamers questioned the ultimate risk posed by \"malicious code generation\" through current LLMs. They noted that while LLMs being able to iteratively improve code is a risk, producing source code itself isn't the main gap. Advanced malware development is currently beyond LLM capabilities.\n",
      "\n",
      "- Quantitative evaluation was also done by scoring Code Llama's responses to malicious code prompts using Llama 2's safety reward model. Code Llama tended to give safer responses compared to GPT-3.5.\n",
      "\n",
      "- However, the full extent and details of the red teaming are limited based on the information available. The Llama 2 paper mentions expanding prompts with safety risks via red teaming, but does not go in-depth on the process or results. More information would be needed to fully characterize the red teaming performed.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "- Meta conducted 3 red teaming exercises with 25 employees, including domain experts in responsible AI, malware development, and offensive security engineering. \n",
      "\n",
      "- The red teamers categorized successful attacks into four main types: 1) getting the model to provide some harmful information while refusing other content, 2) having the model roleplay specific scenarios, 3) forcing the model to highlight positives of harmful content, and 4) embedding harmful instructions within complex commands.\n",
      "\n",
      "- Some red teamers questioned the ultimate risk posed by \"malicious code generation\" through current LLMs. They noted that while LLMs being able to iteratively improve code is a risk, producing source code itself isn't the main gap. Advanced malware development is currently beyond LLM capabilities.\n",
      "\n",
      "- Quantitative evaluation was also done by scoring Code Llama's responses to malicious code prompts using Llama 2's safety reward model. Code Llama tended to give safer responses compared to GPT-3.5.\n",
      "\n",
      "- However, the full extent and details of the red teaming are limited based on the information available. The Llama 2 paper mentions expanding prompts with safety risks via red teaming, but does not go in-depth on the process or results. More information would be needed to fully characterize the red teaming performed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"was any red teaming done with the model?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5p8m4Gc5w1OX"
   },
   "source": "我们可以提出遗漏关键信息的跟进问题，但由于对话历史，LLM 理解上下文并使用它来调整搜索查询。例如我们询问了 `red teaming` 但没有提到 `llama 2` —— Claude 3 根据聊天历史将此上下文添加到搜索查询 `\"llama 2 red teaming\"` 中。"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9bI9czPtWnl"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "009627caf7fa4c51b61b6e2087545324": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0168cb9e015b4e119fcdc7beca2e21a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a0e93df2955b44208479a4b416cd1085",
       "IPY_MODEL_6f58f8d6374b45b4b6a0d9fcaa8083e7",
       "IPY_MODEL_8bde11d3d65c45dba3baf1c02450eef1"
      ],
      "layout": "IPY_MODEL_b19af932a7f24dc799b683136d175107"
     }
    },
    "1022918a023b4aa8a3714d9eab2978b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e1d39ab6d3a4cf5827c9b3e1d4f0df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ff8bcf03c704243b7fd173b3cdd0d87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4383833994854f88aa12ba0edb673d47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "43a3c63554894d268e3bf747f7e0289d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "449afd685fd94354b64aa43a4763ab1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4d5c6f53e82948b18b45877e2e30fd78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7036b643c9264b09b0f93cb6a1a8b01f",
       "IPY_MODEL_fb92dd97e68643099b0d0f1b4983f5f1",
       "IPY_MODEL_91c3b00e77444bb79d9f73314800471e"
      ],
      "layout": "IPY_MODEL_3e1d39ab6d3a4cf5827c9b3e1d4f0df1"
     }
    },
    "593274c4088f4d9aa2becb6b1c6e495b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6999dc582b4c4ba68fdbfe404b1c4dfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f58f8d6374b45b4b6a0d9fcaa8083e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4383833994854f88aa12ba0edb673d47",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee2672db2047468fb49482e14127dcd3",
      "value": 1
     }
    },
    "7036b643c9264b09b0f93cb6a1a8b01f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f58fb2da52bc43a985449910daaac230",
      "placeholder": "​",
      "style": "IPY_MODEL_449afd685fd94354b64aa43a4763ab1d",
      "value": "Downloading data: 100%"
     }
    },
    "78b688cc78264ad1a2f05d339fe498fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8796aebc76a42ed80d6cf9c234b9e14",
       "IPY_MODEL_93fd5108528a499cb5ae4444f0b2f770",
       "IPY_MODEL_89683fb7a1a44606abc2ded05fbb3aa3"
      ],
      "layout": "IPY_MODEL_f202dce30c1d4672ad357db9b6dbe9b8"
     }
    },
    "808dd94ecaff43e0848bcfead3cf0d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89683fb7a1a44606abc2ded05fbb3aa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1022918a023b4aa8a3714d9eab2978b1",
      "placeholder": "​",
      "style": "IPY_MODEL_e6ed1f5a6e5b4c508fa3b69d9219057d",
      "value": " 200/200 [11:43&lt;00:00,  3.59s/it]"
     }
    },
    "8bde11d3d65c45dba3baf1c02450eef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6999dc582b4c4ba68fdbfe404b1c4dfe",
      "placeholder": "​",
      "style": "IPY_MODEL_593274c4088f4d9aa2becb6b1c6e495b",
      "value": " 240927/0 [00:07&lt;00:00, 44003.91 examples/s]"
     }
    },
    "91c3b00e77444bb79d9f73314800471e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce5735788cd14803abac28fa2c42d168",
      "placeholder": "​",
      "style": "IPY_MODEL_a8928cd888a648c9b7d52739f38c5bda",
      "value": " 766M/766M [00:19&lt;00:00, 59.6MB/s]"
     }
    },
    "93fd5108528a499cb5ae4444f0b2f770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc0685f737fa4e15b7796e5f3fe3df8d",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3cfb3b02d7a4e44b00e24aa2e7f9ff5",
      "value": 200
     }
    },
    "a0e93df2955b44208479a4b416cd1085": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2819a7a94974a4da9b6bde754fdbc50",
      "placeholder": "​",
      "style": "IPY_MODEL_808dd94ecaff43e0848bcfead3cf0d75",
      "value": "Generating train split: "
     }
    },
    "a8928cd888a648c9b7d52739f38c5bda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b19af932a7f24dc799b683136d175107": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc0685f737fa4e15b7796e5f3fe3df8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce5735788cd14803abac28fa2c42d168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d902beef883e4c65bd2d6cb161423bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2819a7a94974a4da9b6bde754fdbc50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6ed1f5a6e5b4c508fa3b69d9219057d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee2672db2047468fb49482e14127dcd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f202dce30c1d4672ad357db9b6dbe9b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3cfb3b02d7a4e44b00e24aa2e7f9ff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f58fb2da52bc43a985449910daaac230": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8796aebc76a42ed80d6cf9c234b9e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_009627caf7fa4c51b61b6e2087545324",
      "placeholder": "​",
      "style": "IPY_MODEL_3ff8bcf03c704243b7fd173b3cdd0d87",
      "value": "100%"
     }
    },
    "fb92dd97e68643099b0d0f1b4983f5f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43a3c63554894d268e3bf747f7e0289d",
      "max": 765731119,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d902beef883e4c65bd2d6cb161423bae",
      "value": 765731119
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}