{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# å·¥å…·è¯„ä¼°\n\nå¤šä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è¿è¡Œè¯„ä¼°æ–‡ä»¶ä¸­çš„å•ä¸ªè¯„ä¼°ä»»åŠ¡ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# åµŒå…¥çš„è¯„ä¼°å™¨æç¤ºè¯\nEVALUATION_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªå¯ä»¥ä½¿ç”¨å·¥å…·çš„AIåŠ©æ‰‹ã€‚\n\nå½“ç»™å®šä»»åŠ¡æ—¶ï¼Œä½ å¿…é¡»ï¼š\n1. ä½¿ç”¨å¯ç”¨å·¥å…·å®Œæˆä»»åŠ¡\n2. åœ¨<summary>æ ‡ç­¾ä¸­æä¾›æ¯ä¸ªæ­¥éª¤çš„æ–¹æ³•æ‘˜è¦\n3. åœ¨<feedback>æ ‡ç­¾ä¸­æä¾›å¯¹å·¥å…·çš„åé¦ˆ\n4. åœ¨<response>æ ‡ç­¾ä¸­æä¾›æœ€ç»ˆå“åº”\n\næ‘˜è¦è¦æ±‚ï¼š\n- åœ¨<summary>æ ‡ç­¾ä¸­ï¼Œä½ å¿…é¡»è§£é‡Šï¼š\n  - ä½ å®Œæˆä»»åŠ¡çš„æ­¥éª¤\n  - ä½ ä½¿ç”¨äº†å“ªäº›å·¥å…·ï¼Œä½¿ç”¨é¡ºåºä»¥åŠåŸå› \n  - ä½ æä¾›ç»™æ¯ä¸ªå·¥å…·çš„è¾“å…¥\n  - ä½ ä»å·¥å…·æ”¶åˆ°çš„è¾“å‡º\n  - ä½ å¦‚ä½•å¾—å‡ºå“åº”çš„æ‘˜è¦\n\nåé¦ˆè¦æ±‚ï¼š\n- åœ¨<feedback>æ ‡ç­¾ä¸­ï¼Œå¯¹å·¥å…·æä¾›å»ºè®¾æ€§åé¦ˆï¼š\n  - è¯„è®ºå·¥å…·åç§°ï¼šå®ƒä»¬æ˜¯å¦æ¸…æ™°ä¸”å…·æœ‰æè¿°æ€§ï¼Ÿ\n  - è¯„è®ºè¾“å…¥å‚æ•°ï¼šæ–‡æ¡£æ˜¯å¦å®Œå–„ï¼Ÿå¿…éœ€å‚æ•°å’Œå¯é€‰å‚æ•°æ˜¯å¦æ¸…æ¥šï¼Ÿ\n  - è¯„è®ºæè¿°ï¼šå®ƒä»¬æ˜¯å¦å‡†ç¡®æè¿°äº†å·¥å…·çš„åŠŸèƒ½ï¼Ÿ\n  - è¯„è®ºä½¿ç”¨å·¥å…·æ—¶é‡åˆ°çš„ä»»ä½•é”™è¯¯ï¼šå·¥å…·æ‰§è¡Œå¤±è´¥äº†å—ï¼Ÿå·¥å…·è¿”å›äº†å¤ªå¤šæ ‡è®°å—ï¼Ÿ\n  - è¯†åˆ«å…·ä½“çš„æ”¹è¿›é¢†åŸŸå¹¶è§£é‡Šä¸ºä»€ä¹ˆå®ƒä»¬ä¼šæœ‰å¸®åŠ©\n  - åœ¨å»ºè®®ä¸­è¦å…·ä½“ä¸”å¯æ‰§è¡Œ\n  \nå“åº”è¦æ±‚ï¼š\n- ä½ çš„å“åº”åº”è¯¥ç®€æ´å¹¶ç›´æ¥å›ç­”æ‰€é—®çš„é—®é¢˜\n- å§‹ç»ˆå°†æœ€ç»ˆå“åº”åŒ…è£…åœ¨<response>æ ‡ç­¾ä¸­\n- å¦‚æœæ— æ³•è§£å†³ä»»åŠ¡ï¼Œè¿”å›<response>NOT_FOUND</response>\n- å¯¹äºæ•°å€¼å“åº”ï¼Œä»…æä¾›æ•°å­—\n- å¯¹äºIDï¼Œä»…æä¾›ID\n- å¯¹äºåç§°æˆ–æ–‡æœ¬ï¼Œæä¾›è¯·æ±‚çš„ç¡®åˆ‡æ–‡æœ¬\n- ä½ çš„å“åº”åº”è¯¥æ”¾åœ¨æœ€å\"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## æç¤ºè¯"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def agent_loop(prompt: str, tools: List[Dict[str, Any]] = None) -> Tuple[str, Dict[str, Any]]:\n    \"\"\"ç”¨äºå·¥å…·è¯„ä¼°çš„ç®€åŒ–æ™ºèƒ½ä½“ç±»\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    response = client.messages.create(\n        model=model,\n        max_tokens=4096,\n        system=EVALUATION_PROMPT,\n        messages=messages,\n        tools=tools,\n    )\n\n    messages.append({\"role\": \"assistant\", \"content\": response.content})\n\n    # ä½¿ç”¨è®¡æ—¶è·Ÿè¸ªå·¥å…·è°ƒç”¨\n    tool_metrics = {}  # {tool_name: {\"count\": N, \"durations\": [X1, X2, ...]}}\n\n    def _prepare_tool_result(tool_use_id, tool_result):\n        return {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"tool_result\",\n                    \"tool_use_id\": tool_use_id,\n                    \"content\": tool_result,\n                }\n            ],\n        }\n\n    while response.stop_reason == \"tool_use\":\n        tool_use = next(block for block in response.content if block.type == \"tool_use\")\n        tool_name = tool_use.name\n\n        tool_start_ts = time.time()\n        try:\n            tool_response = eval(\n                f\"{tool_name}(**tool_use.input)\"\n            )  # ä½¿ç”¨è¾“å…¥è°ƒç”¨å·¥å…·å‡½æ•°\n        except Exception as e:\n            tool_response = f\"æ‰§è¡Œå·¥å…· {tool_name} æ—¶å‡ºé”™: {str(e)}\\n\"\n            tool_response += traceback.format_exc()\n        tool_duration = time.time() - tool_start_ts\n\n        # æ›´æ–°å·¥å…·æŒ‡æ ‡\n        if tool_name not in tool_metrics:\n            tool_metrics[tool_name] = {\"count\": 0, \"durations\": []}\n        tool_metrics[tool_name][\"count\"] += 1\n        tool_metrics[tool_name][\"durations\"].append(tool_duration)\n\n        # å‡†å¤‡å·¥å…·ç»“æœå¹¶é™„åŠ åˆ°æ¶ˆæ¯\n        messages.append(_prepare_tool_result(tool_use.id, tool_response))\n        response = client.messages.create(\n            model=model,\n            max_tokens=4096,\n            system=EVALUATION_PROMPT,\n            messages=messages,\n            tools=tools,\n        )\n        messages.append({\"role\": \"assistant\", \"content\": response.content})\n\n    response = next(\n        (block.text for block in response.content if hasattr(block, \"text\")),\n        None,\n    )\n    return response, tool_metrics"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## æ™ºèƒ½ä½“å¾ªç¯"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_evaluation_file(file_path: Path) -> List[Dict[str, Any]]:\n    \"\"\"è§£æXMLè¯„ä¼°æ–‡ä»¶å¹¶è¿”å›è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ã€‚\"\"\"\n    try:\n        tree = ET.parse(file_path)\n        root = tree.getroot()\n        evaluations = []\n\n        # æ£€æŸ¥ä»»åŠ¡å…ƒç´ \n        tasks = root.findall(\".//task\")\n        for task in tasks:\n            prompt_elem = task.find(\"prompt\")\n            response_elem = task.find(\"response\")\n\n            if prompt_elem is not None and response_elem is not None:\n                eval_dict = {\n                    \"prompt\": (prompt_elem.text or \"\").strip(),\n                    \"response\": (response_elem.text or \"\").strip(),\n                }\n                evaluations.append(eval_dict)\n\n        return evaluations\n    except Exception as e:\n        print(f\"è§£æè¯„ä¼°æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}\")\n        return []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## è¾…åŠ©å‡½æ•°"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_single_task(\n    task: Dict[str, Any], tools: List[Dict[str, Any]], task_index: int\n) -> Dict[str, Any]:\n    \"\"\"ä½¿ç”¨ç»™å®šå·¥å…·è¯„ä¼°å•ä¸ªä»»åŠ¡ã€‚\"\"\"\n    start_time = time.time()\n\n    # è¿è¡Œä»»åŠ¡\n    print(f\"ä»»åŠ¡ {task_index + 1}: è¿è¡Œä»»åŠ¡ï¼Œæç¤ºè¯: {task['prompt']}\")\n    response, tool_metrics = agent_loop(task[\"prompt\"], tools)\n\n    # æå–æ‰€æœ‰æ ‡è®°çš„å†…å®¹\n    def _extract_xml_content(text, tag):\n        pattern = rf\"<{tag}>(.*?)</{tag}>\"\n        matches = re.findall(pattern, text, re.DOTALL)\n        return matches[-1].strip() if matches else None\n\n    response, summary, feedback = (\n        _extract_xml_content(response, tag) for tag in [\"response\", \"summary\", \"feedback\"]\n    )\n    duration_seconds = time.time() - start_time\n\n    return {\n        \"prompt\": task[\"prompt\"],\n        \"expected\": task[\"response\"],\n        \"actual\": response,\n        \"score\": int(response == task[\"response\"]),\n        \"total_duration\": duration_seconds,\n        \"tool_calls\": tool_metrics,\n        \"num_tool_calls\": sum(len(metrics[\"durations\"]) for metrics in tool_metrics.values()),\n        \"summary\": summary,\n        \"feedback\": feedback,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# æŠ¥å‘Šæ¨¡æ¿\nREPORT_HEADER = \"\"\"\n# è¯„ä¼°æŠ¥å‘Š\n\n## æ‘˜è¦\n\n- **å‡†ç¡®ç‡**: {correct}/{total} ({accuracy:.1f}%)\n- **å¹³å‡ä»»åŠ¡æŒç»­æ—¶é—´**: {average_duration_s:.2f}ç§’\n- **å¹³å‡æ¯æ¬¡ä»»åŠ¡å·¥å…·è°ƒç”¨æ¬¡æ•°**: {average_tool_calls:.2f}\n- **å·¥å…·è°ƒç”¨æ€»æ•°**: {total_tool_calls}\n\n---\n\"\"\"\n\nTASK_TEMPLATE = \"\"\"\n### ä»»åŠ¡\n\n**æç¤ºè¯**: {prompt}\n**æ ‡å‡†ç­”æ¡ˆå“åº”**: `{expected_response}`\n**å®é™…å“åº”**: `{actual_response}`\n**æ˜¯å¦æ­£ç¡®**: {correct_indicator}\n**æŒç»­æ—¶é—´**: {total_duration:.2f}ç§’\n**å·¥å…·è°ƒç”¨**: {tool_calls}\n\n**æ‘˜è¦**\n{summary}\n\n**åé¦ˆ**\n{feedback}\n\n---\n\"\"\"\n\n\ndef run_evaluation(eval_path: str, tools: List[Dict[str, Any]]) -> str:\n    \"\"\"\n    ä½¿ç”¨ç®€å•å¾ªç¯å’Œæä¾›çš„å·¥å…·è¿è¡Œè¯„ä¼°ã€‚\n\n    Args:\n        eval_path: XMLè¯„ä¼°æ–‡ä»¶è·¯å¾„\n        tools: ç”¨äºè¯„ä¼°çš„å·¥å…·å®šä¹‰åˆ—è¡¨\n\n    \"\"\"\n    print(\"ğŸš€ å¼€å§‹è¯„ä¼°\")\n\n    eval_file = Path(eval_path)\n\n    # è§£æè¯„ä¼°ä»»åŠ¡\n    tasks = parse_evaluation_file(eval_file)\n\n    print(f\"ğŸ“‹ å·²åŠ è½½ {len(tasks)} ä¸ªè¯„ä¼°ä»»åŠ¡\")\n\n    # ç®€å•å¾ªç¯è¿è¡Œæ‰€æœ‰ä»»åŠ¡\n    results = []\n    for i, task in enumerate(tasks):\n        print(f\"å¤„ç†ä»»åŠ¡ {i + 1}/{len(tasks)}\")\n        results.append(evaluate_single_task(task, tools, i))\n\n    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡\n    correct = sum(r[\"score\"] for r in results)\n    accuracy = (correct / len(results)) * 100\n    average_duration_s = sum(r[\"total_duration\"] for r in results) / len(results)\n    average_tool_calls = sum(r[\"num_tool_calls\"] for r in results) / len(results)\n    total_tool_calls = sum(r[\"num_tool_calls\"] for r in results)\n\n    report = REPORT_HEADER.format(\n        correct=correct,\n        total=len(results),\n        accuracy=accuracy,\n        average_duration_s=average_duration_s,\n        average_tool_calls=average_tool_calls,\n        total_tool_calls=total_tool_calls,\n    )\n\n    report += \"\".join(\n        [\n            TASK_TEMPLATE.format(\n                prompt=task[\"prompt\"],\n                expected_response=task[\"response\"],\n                actual_response=result[\"actual\"],\n                correct_indicator=\"âœ…\" if result[\"score\"] else \"âŒ\",\n                total_duration=result[\"total_duration\"],\n                tool_calls=json.dumps(result[\"tool_calls\"], indent=2),\n                summary=result[\"summary\"] or \"N/A\",\n                feedback=result[\"feedback\"] or \"N/A\",\n            )\n            for task, result in zip(tasks, results)\n        ]\n    )\n    # å°†æ‰€æœ‰éƒ¨åˆ†è¿æ¥æˆæœ€ç»ˆæŠ¥å‘Š\n    return report"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ä¸»è¦è¯„ä¼°å‡½æ•°"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculator(expression: str) -> str:\n    \"\"\"æ‰§è¡Œç®—æœ¯è¿ç®—çš„åŸºæœ¬è®¡ç®—å™¨ã€‚\"\"\"\n    try:\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return str(result)\n    except Exception as e:\n        return f\"é”™è¯¯: {str(e)}\"\n\n\n# å®šä¹‰è®¡ç®—å™¨çš„å·¥å…·æ¨¡å¼\ncalculator_tool = {\n    \"name\": \"calculator\",\n    \"description\": \"\",  # ä¸€ä¸ªæ²¡æœ‰å¸®åŠ©çš„å·¥å…·æè¿°ã€‚\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"expression\": {\n                \"type\": \"string\",\n                \"description\": \"\",  # ä¸€ä¸ªæ²¡æœ‰å¸®åŠ©çš„æ¨¡å¼æè¿°ã€‚\n            }\n        },\n        \"required\": [\"expression\"],\n    },\n}\n\n# è®¾ç½®å·¥å…·åˆ—è¡¨\ntools = [calculator_tool]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## è®¡ç®—å™¨å·¥å…·"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è¿è¡Œè¯„ä¼°\nprint(\"âœ… ä½¿ç”¨è®¡ç®—å™¨å·¥å…·\")\n\nreport = run_evaluation(eval_path=\"evaluation.xml\", tools=tools)\n\nprint(report)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## è¿è¡Œè¯„ä¼°"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}