{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ä½¿ç”¨Claude Sonnet 4.5è¿›è¡Œè®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†\n\nå­¦ä¹ å¦‚ä½•ä½¿ç”¨Claudeçš„è®°å¿†å·¥å…·å’Œä¸Šä¸‹æ–‡ç¼–è¾‘åŠŸèƒ½æ„å»ºè·¨å¯¹è¯å­¦ä¹ å’Œæ”¹è¿›çš„AIä»£ç†ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ç›®å½•\n\n1. [ä»‹ç»ï¼šä¸ºä»€ä¹ˆè®°å¿†å¾ˆé‡è¦](#introduction)\n2. [ä½¿ç”¨æ¡ˆä¾‹](#use-cases)\n3. [å¿«é€Ÿå¼€å§‹ç¤ºä¾‹](#quick-start)\n4. [å·¥ä½œåŸç†](#how-it-works)\n5. [ä»£ç å®¡æŸ¥åŠ©æ‰‹æ¼”ç¤º](#demo)\n6. [å®é™…åº”ç”¨](#real-world)\n7. [æœ€ä½³å®è·µ](#best-practices)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## è®¾ç½®\n\n### å¯¹äºVSCodeç”¨æˆ·\n\n```bash\n# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\npython -m venv .venv\n\n# 2. æ¿€æ´»å®ƒ\nsource .venv/bin/activate  # macOS/Linux\n# æˆ–è€…: .venv\\Scripts\\activate  # Windows\n\n# 3. å®‰è£…ä¾èµ–\npip install -r requirements.txt\n\n# 4. åœ¨VSCodeä¸­é€‰æ‹©.venvä½œä¸ºå†…æ ¸ï¼ˆå³ä¸Šè§’ï¼‰\n```\n\n### APIå¯†é’¥\n\n```bash\ncp .env.example .env\n# ç¼–è¾‘.envå¹¶æ·»åŠ æ‚¨çš„ANTHROPIC_API_KEY\n```\n\nä»ä»¥ä¸‹ç½‘å€è·å–æ‚¨çš„APIå¯†é’¥ï¼šhttps://console.anthropic.com/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. ä»‹ç»ï¼šä¸ºä»€ä¹ˆè®°å¿†å¾ˆé‡è¦ {#introduction}\n\næœ¬æ•™ç¨‹æ¼”ç¤ºäº†[AIä»£ç†çš„æœ‰æ•ˆä¸Šä¸‹æ–‡å·¥ç¨‹](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)ä¸€æ–‡ä¸­æè¿°çš„ä¸Šä¸‹æ–‡å·¥ç¨‹æ¨¡å¼çš„å®é™…å®ç°ã€‚è¯¥æ–‡æ¶µç›–äº†ä¸Šä¸‹æ–‡æ˜¯æœ‰é™èµ„æºçš„åŸå› ã€æ³¨æ„åŠ›é¢„ç®—å¦‚ä½•å·¥ä½œï¼Œä»¥åŠæ„å»ºæœ‰æ•ˆä»£ç†çš„ç­–ç•¥â€”â€”æ‚¨å°†åœ¨è¿™é‡Œçœ‹åˆ°è¿™äº›æŠ€æœ¯çš„å®é™…åº”ç”¨ã€‚\n\n### é—®é¢˜\n\nå¤§è¯­è¨€æ¨¡å‹æœ‰æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆClaude 4ä¸º200kä¸ªæ ‡è®°ï¼‰ã€‚è™½ç„¶è¿™çœ‹èµ·æ¥å¾ˆå¤§ï¼Œä½†ä¼šå‡ºç°å‡ ä¸ªæŒ‘æˆ˜ï¼š\n\n- **ä¸Šä¸‹æ–‡é™åˆ¶**ï¼šé•¿å¯¹è¯æˆ–å¤æ‚ä»»åŠ¡å¯èƒ½è¶…å‡ºå¯ç”¨ä¸Šä¸‹æ–‡\n- **è®¡ç®—æˆæœ¬**ï¼šå¤„ç†å¤§ä¸Šä¸‹æ–‡å¾ˆæ˜‚è´µâ€”â€”æ³¨æ„åŠ›æœºåˆ¶å‘ˆäºŒæ¬¡æ–¹æ‰©å±•\n- **é‡å¤æ¨¡å¼**ï¼šè·¨å¯¹è¯çš„ç±»ä¼¼ä»»åŠ¡æ¯æ¬¡éƒ½éœ€è¦é‡æ–°è§£é‡Šä¸Šä¸‹æ–‡\n- **ä¿¡æ¯ä¸¢å¤±**ï¼šå½“ä¸Šä¸‹æ–‡å¡«æ»¡æ—¶ï¼Œè¾ƒæ—©çš„é‡è¦ä¿¡æ¯ä¼šä¸¢å¤±\n\n### è§£å†³æ–¹æ¡ˆ\n\nClaude Sonnet 4.5å¼•å…¥äº†ä¸¤ä¸ªå¼ºå¤§çš„åŠŸèƒ½ï¼š\n\n1. **è®°å¿†å·¥å…·** (`memory_20250818`)ï¼šå®ç°è·¨å¯¹è¯å­¦ä¹ \n   - Claudeå¯ä»¥å†™ä¸‹å®ƒå­¦åˆ°çš„ä¸œè¥¿ä»¥ä¾›å°†æ¥å‚è€ƒ\n   - `/memories`ç›®å½•ä¸‹çš„åŸºäºæ–‡ä»¶çš„ç³»ç»Ÿ\n   - å®¢æˆ·ç«¯å®ç°è®©æ‚¨å®Œå…¨æ§åˆ¶\n\n2. **ä¸Šä¸‹æ–‡ç¼–è¾‘** (`clear_tool_uses_20250919`)ï¼šè‡ªåŠ¨ç®¡ç†ä¸Šä¸‹æ–‡\n   - å½“ä¸Šä¸‹æ–‡å¢é•¿è¾ƒå¤§æ—¶æ¸…é™¤æ—§å·¥å…·ç»“æœ\n   - ä¿ç•™æœ€è¿‘ä¸Šä¸‹æ–‡åŒæ—¶ä¿ç•™è®°å¿†\n   - å¯é…ç½®çš„è§¦å‘å™¨å’Œä¿ç•™ç­–ç•¥\n\n### å¥½å¤„\n\næ„å»º**éšç€æ—¶é—´æ¨ç§»åœ¨æ‚¨çš„ç‰¹å®šä»»åŠ¡ä¸Šå˜å¾—æ›´å‡ºè‰²**çš„AIä»£ç†ï¼š\n\n- **ä¼šè¯1**ï¼šClaudeè§£å†³é—®é¢˜ï¼Œè®°å½•æ¨¡å¼\n- **ä¼šè¯2**ï¼šClaudeç«‹å³åº”ç”¨å­¦åˆ°çš„æ¨¡å¼ï¼ˆæ›´å¿«ï¼ï¼‰\n- **é•¿ä¼šè¯**ï¼šä¸Šä¸‹æ–‡ç¼–è¾‘ä¿æŒå¯¹è¯å¯æ§\n\næŠŠå®ƒæƒ³è±¡æˆç»™Claudeä¸€ä¸ªç¬”è®°æœ¬åšç¬”è®°å’Œå‚è€ƒâ€”â€”å°±åƒäººç±»åšçš„é‚£æ ·ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. ä½¿ç”¨æ¡ˆä¾‹ {#use-cases}\n\nè®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å¯ä»¥å®ç°å¼ºå¤§çš„æ–°å·¥ä½œæµç¨‹ï¼š\n\n### ğŸ” ä»£ç å®¡æŸ¥åŠ©æ‰‹\n- ä»è¿‡å»çš„å®¡æŸ¥ä¸­å­¦ä¹ è°ƒè¯•æ¨¡å¼\n- åœ¨æœªæ¥çš„ä¼šè¯ä¸­ç«‹å³è¯†åˆ«ç±»ä¼¼çš„é”™è¯¯\n- å»ºç«‹å›¢é˜Ÿç‰¹å®šçš„ä»£ç è´¨é‡çŸ¥è¯†\n- **ç”Ÿäº§å°±ç»ª**ï¼šä¸[claude-code-action](https://github.com/anthropics/claude-code-action)é›†æˆè¿›è¡ŒGitHub PRå®¡æŸ¥\n\n### ğŸ“š ç ”ç©¶åŠ©æ‰‹\n- åœ¨å¤šä¸ªä¼šè¯ä¸­ç§¯ç´¯ä¸»é¢˜çŸ¥è¯†\n- è¿æ¥ä¸åŒç ”ç©¶çº¿ç´¢çš„è§è§£\n- ç»´æŠ¤å‚è€ƒæ–‡çŒ®å’Œæ¥æºè·Ÿè¸ª\n\n### ğŸ’¬ å®¢æˆ·æ”¯æŒæœºå™¨äºº\n- å­¦ä¹ ç”¨æˆ·åå¥½å’Œæ²Ÿé€šé£æ ¼\n- è®°ä½å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ\n- ä»äº¤äº’ä¸­å»ºç«‹äº§å“çŸ¥è¯†åº“\n\n### ğŸ“Š æ•°æ®åˆ†æåŠ©æ‰‹\n- è®°ä½æ•°æ®é›†æ¨¡å¼å’Œå¼‚å¸¸\n- å­˜å‚¨æ•ˆæœè‰¯å¥½çš„åˆ†ææŠ€æœ¯\n- éšç€æ—¶é—´æ¨ç§»å»ºç«‹é¢†åŸŸç‰¹å®šè§è§£\n\n**æ”¯æŒçš„æ¨¡å‹**ï¼šClaude Opus 4.1 (`claude-opus-4-1`) å’Œ Claude Sonnet 4.5 (`claude-sonnet-4-5`)\n\n**æœ¬æ•™ç¨‹ä¾§é‡äºä»£ç å®¡æŸ¥åŠ©æ‰‹**ï¼Œå› ä¸ºå®ƒæ¸…æ¥šåœ°å±•ç¤ºäº†è®°å¿†ï¼ˆå­¦ä¹ æ¨¡å¼ï¼‰å’Œä¸Šä¸‹æ–‡ç¼–è¾‘ï¼ˆå¤„ç†é•¿å®¡æŸ¥ï¼‰ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ {#quick-start}\n\nè®©æˆ‘ä»¬é€šè¿‡ç®€å•ç¤ºä¾‹çœ‹çœ‹è®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†çš„å®é™…åº”ç”¨ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### è®¾ç½®\n\né¦–å…ˆï¼Œå®‰è£…ä¾èµ–é¡¹å¹¶é…ç½®æ‚¨çš„ç¯å¢ƒï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Option 1: From requirements.txt\n",
    "# %pip install -q -r requirements.txt\n",
    "\n",
    "# Option 2: Direct install\n",
    "%pip install -q anthropic python-dotenv ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**âš ï¸ é‡è¦**ï¼šåœ¨æ­¤ç›®å½•ä¸­åˆ›å»º`.env`æ–‡ä»¶ï¼š\n\n```bash\n# å°†.env.exampleå¤åˆ¶åˆ°.envå¹¶æ·»åŠ æ‚¨çš„APIå¯†é’¥\ncp .env.example .env\n```\n\nç„¶åç¼–è¾‘`.env`ä»¥ä»https://console.anthropic.com/æ·»åŠ æ‚¨çš„Anthropic APIå¯†é’¥"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API key loaded\n",
      "âœ“ Using model: claude-sonnet-4-5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import cast\n",
    "\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "MODEL = os.getenv(\"ANTHROPIC_MODEL\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY not found. Copy .env.example to .env and add your API key.\")\n",
    "\n",
    "if not MODEL:\n",
    "    raise ValueError(\"ANTHROPIC_MODEL not found. Copy .env.example to .env and set the model.\")\n",
    "\n",
    "MODEL = cast(str, MODEL)\n",
    "\n",
    "client = Anthropic(api_key=API_KEY)\n",
    "\n",
    "print(\"âœ“ API key loaded\")\n",
    "print(f\"âœ“ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ç¤ºä¾‹1ï¼šåŸºæœ¬è®°å¿†ä½¿ç”¨\n\nè®©æˆ‘ä»¬çœ‹çœ‹Claudeå¦‚ä½•ä½¿ç”¨è®°å¿†æ¥å­˜å‚¨ä¿¡æ¯ä»¥ä¾›å°†æ¥å‚è€ƒã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**è¾…åŠ©å‡½æ•°**\n\nè¿™äº›ç¤ºä¾‹ä½¿ç”¨æ¥è‡ª`demo_helpers.py`çš„è¾…åŠ©å‡½æ•°ï¼š\n\n- **`run_conversation_loop()`**ï¼šå¤„ç†APIå¯¹è¯å¾ªç¯\n  - è°ƒç”¨å¯ç”¨è®°å¿†å·¥å…·çš„Claudeçš„API\n  - æ‰§è¡Œå·¥å…·ä½¿ç”¨ï¼ˆè®°å¿†æ“ä½œï¼‰\n  - ç»§ç»­ç›´åˆ°Claudeåœæ­¢ä½¿ç”¨å·¥å…·\n  - è¿”å›æœ€ç»ˆå“åº”\n\n- **`run_conversation_turn()`**ï¼šå•æ¬¡å›åˆï¼ˆåœ¨ç¤ºä¾‹3ä¸­ä½¿ç”¨ï¼‰\n  - ä¸ä¸Šé¢ç›¸åŒï¼Œä½†åœ¨ä¸€æ¬¡APIè°ƒç”¨åè¿”å›\n  - å½“æ‚¨éœ€è¦ç»†ç²’åº¦æ§åˆ¶æ—¶å¾ˆæœ‰ç”¨\n\n- **`print_context_management_info()`**ï¼šæ˜¾ç¤ºä¸Šä¸‹æ–‡æ¸…é™¤ç»Ÿè®¡ä¿¡æ¯\n  - æ˜¾ç¤ºä¿å­˜çš„æ ‡è®°ã€æ¸…é™¤çš„å·¥å…·ä½¿ç”¨\n  - æœ‰åŠ©äºå¯è§†åŒ–ä¸Šä¸‹æ–‡ç¼–è¾‘ä½•æ—¶è§¦å‘"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**âš ï¸ å…³äºè®°å¿†æ¸…é™¤çš„è¯´æ˜**\n\nä»¥ä¸‹å•å…ƒæ ¼æ¸…é™¤æ‰€æœ‰è®°å¿†æ–‡ä»¶ä»¥ä¸ºæ­¤æ¼”ç¤ºæä¾›å¹²å‡€çš„ç¯å¢ƒã€‚è¿™å¯¹äºå¤šæ¬¡è¿è¡Œç¬”è®°æœ¬ä»¥æŸ¥çœ‹ä¸€è‡´ç»“æœå¾ˆæœ‰ç”¨ã€‚\n\n**åœ¨ç”Ÿäº§åº”ç”¨ç¨‹åºä¸­**ï¼Œæ‚¨åº”è¯¥ä»”ç»†è€ƒè™‘æ˜¯å¦æ¸…é™¤æ‰€æœ‰è®°å¿†ï¼Œå› ä¸ºå®ƒä¼šæ°¸ä¹…åˆ é™¤å­¦åˆ°çš„æ¨¡å¼ã€‚è€ƒè™‘ä½¿ç”¨é€‰æ‹©æ€§åˆ é™¤æˆ–å°†è®°å¿†ç»„ç»‡åˆ°é¡¹ç›®ç‰¹å®šç›®å½•ä¸­ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Clearing previous memories...\n",
      "âœ“ Memory cleared\n",
      "\n",
      "============================================================\n",
      "ğŸ“ SESSION 1: Learning from a bug\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Turn 1:\n",
      "ğŸ’¬ Claude: I'll review this multi-threaded web scraper for you. Let me first check my memory and then analyze the code.\n",
      "\n",
      "  ğŸ”§ Memory tool: view /memories\n",
      "  âœ“ Result: Directory: /memories\n",
      "(empty)\n",
      "\n",
      "ğŸ”„ Turn 2:\n",
      "  ğŸ”§ Memory tool: create /memories/review_progress.md\n",
      "  âœ“ Result: File created successfully at /memories/review_progress.md\n",
      "\n",
      "ğŸ”„ Turn 3:\n",
      "ğŸ’¬ Claude: \n",
      "\n",
      "## Code Review: Multi-threaded Web Scraper - Race Condition Issues\n",
      "\n",
      "Great catch on identifying this as a concurrency issue! I've found **multiple critical race conditions** that explain the inconsistent results.\n",
      "\n",
      "### ğŸ”´ **Critical Issues**\n",
      "\n",
      "#### **1. Race Condition in `self.results` (Primary Bug)**\n",
      "**Location:** Line in `scrape_urls()` method\n",
      "```python\n",
      "self.results.append(result)  # RACE CONDITION\n",
      "```\n",
      "\n",
      "**Problem:** \n",
      "- Python's `list.append()` is **NOT thread-safe** for concurrent modifications\n",
      "- Multiple threads simultaneously appending can cause:\n",
      "  - **Lost updates:** One thread's append overwrites another's\n",
      "  - **Inconsistent state:** Internal list structure can be corrupted\n",
      "  - **Unpredictable counts:** Results randomly dropped\n",
      "\n",
      "**Why it happens:**\n",
      "`list.append()` involves multiple operations:\n",
      "1. Read current list size\n",
      "2. Allocate space\n",
      "3. Insert item\n",
      "4. Update size\n",
      "\n",
      "Between these steps, another thread can interfere, causing data loss.\n",
      "\n",
      "#### **2. Race Condition in `self.failed_urls`**\n",
      "**Location:** Same method\n",
      "```python\n",
      "self.failed_urls.append(result[\"url\"])  # RACE CONDITION\n",
      "```\n",
      "\n",
      "**Problem:** Identical issue - concurrent appends without synchronization.\n",
      "\n",
      "#### **3. Shared State Without Protection**\n",
      "**Location:** Class initialization\n",
      "```python\n",
      "self.results = []  # BUG: Shared mutable state accessed by multiple threads!\n",
      "self.failed_urls = []  # BUG: Another race condition!\n",
      "```\n",
      "\n",
      "**Problem:** Instance variables shared across threads without any locking mechanism.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… **Solutions**\n",
      "\n",
      "Here are three approaches to fix this, from simplest to most robust:\n",
      "\n",
      "#### **Solution 1: Use Thread-Safe Queue (Recommended)**\n",
      "\n",
      "```python\n",
      "import time\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import List, Dict, Any\n",
      "from queue import Queue\n",
      "import requests\n",
      "\n",
      "\n",
      "class WebScraper:\n",
      "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
      "\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        # Use thread-safe Queue instead of lists\n",
      "        self.results_queue = Queue()\n",
      "        self.failed_queue = Queue()\n",
      "\n",
      "    def fetch_url(self, url: str) -> Dict[str, Any]:\n",
      "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
      "        try:\n",
      "            response = requests.get(url, timeout=5)\n",
      "            response.raise_for_status()\n",
      "            return {\n",
      "                \"url\": url,\n",
      "                \"status\": response.status_code,\n",
      "                \"content_length\": len(response.content),\n",
      "            }\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            return {\"url\": url, \"error\": str(e)}\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"Scrape multiple URLs concurrently - FIXED with Queue.\"\"\"\n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "                \n",
      "                # Thread-safe operations\n",
      "                if \"error\" in result:\n",
      "                    self.failed_queue.put(result[\"url\"])\n",
      "                else:\n",
      "                    self.results_queue.put(result)\n",
      "\n",
      "        # Convert queues to lists after all threads complete\n",
      "        results = []\n",
      "        while not self.results_queue.empty():\n",
      "            results.append(self.results_queue.get())\n",
      "        \n",
      "        return results\n",
      "\n",
      "    def get_stats(self) -> Dict[str, int]:\n",
      "        \"\"\"Get scraping statistics.\"\"\"\n",
      "        results_count = self.results_queue.qsize()\n",
      "        failed_count = self.failed_queue.qsize()\n",
      "        \n",
      "        return {\n",
      "            \"total_results\": results_count,\n",
      "            \"failed_urls\": failed_count,\n",
      "            \"success_rate\": (\n",
      "                results_count / (results_count + failed_count)\n",
      "                if (results_count + failed_count) > 0\n",
      "                else 0\n",
      "            ),\n",
      "        }\n",
      "```\n",
      "\n",
      "#### **Solution 2: Use Threading Lock**\n",
      "\n",
      "```python\n",
      "import threading\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import List, Dict, Any\n",
      "import requests\n",
      "\n",
      "\n",
      "class WebScraper:\n",
      "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
      "\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "        self.results = []\n",
      "        self.failed_urls = []\n",
      "        self.lock = threading.Lock()  # Add lock for synchronization\n",
      "\n",
      "    def fetch_url(self, url: str) -> Dict[str, Any]:\n",
      "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
      "        try:\n",
      "            response = requests.get(url, timeout=5)\n",
      "            response.raise_for_status()\n",
      "            return {\n",
      "                \"url\": url,\n",
      "                \"status\": response.status_code,\n",
      "                \"content_length\": len(response.content),\n",
      "            }\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            return {\"url\": url, \"error\": str(e)}\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"Scrape multiple URLs concurrently - FIXED with lock.\"\"\"\n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "                \n",
      "                # Protected by lock - only one thread at a time\n",
      "                with self.lock:\n",
      "                    if \"error\" in result:\n",
      "                        self.failed_urls.append(result[\"url\"])\n",
      "                    else:\n",
      "                        self.results.append(result)\n",
      "\n",
      "        return self.results\n",
      "\n",
      "    def get_stats(self) -> Dict[str, int]:\n",
      "        \"\"\"Get scraping statistics.\"\"\"\n",
      "        with self.lock:  # Protect reads too\n",
      "            return {\n",
      "                \"total_results\": len(self.results),\n",
      "                \"failed_urls\": len(self.failed_urls),\n",
      "                \"success_rate\": (\n",
      "                    len(self.results) / (len(self.results) + len(self.failed_urls))\n",
      "                    if (len(self.results) + len(self.failed_urls)) > 0\n",
      "                    else 0\n",
      "                ),\n",
      "            }\n",
      "```\n",
      "\n",
      "#### **Solution 3: Let ThreadPoolExecutor Handle It (Best)**\n",
      "\n",
      "```python\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from typing import List, Dict, Any\n",
      "import requests\n",
      "\n",
      "\n",
      "class WebScraper:\n",
      "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
      "\n",
      "    def __init__(self, max_workers: int = 10):\n",
      "        self.max_workers = max_workers\n",
      "\n",
      "    def fetch_url(self, url: str) -> Dict[str, Any]:\n",
      "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
      "        try:\n",
      "            response = requests.get(url, timeout=5)\n",
      "            response.raise_for_status()\n",
      "            return {\n",
      "                \"url\": url,\n",
      "                \"status\": response.status_code,\n",
      "                \"content_length\": len(response.content),\n",
      "            }\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            return {\"url\": url, \"error\": str(e)}\n",
      "\n",
      "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, Any]]:\n",
      "        \"\"\"\n",
      "        Scrape multiple URLs concurrently - FIXED by avoiding shared state.\n",
      "        Collect results after futures complete (no concurrent modifications).\n",
      "        \"\"\"\n",
      "        results = []\n",
      "        failed_urls = []\n",
      "        \n",
      "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
      "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
      "\n",
      "            # Single-threaded collection after async work completes\n",
      "            for future in as_completed(futures):\n",
      "                result = future.result()\n",
      "                \n",
      "                if \"error\" in result:\n",
      "                    failed_urls.append(result[\"url\"])\n",
      "                else:\n",
      "                    results.append(result)\n",
      "\n",
      "        self.results = results  # Store after all work is done\n",
      "        self.failed_urls = failed_urls\n",
      "        return results\n",
      "\n",
      "    def get_stats(self) -> Dict[str, int]:\n",
      "        \"\"\"Get scraping statistics.\"\"\"\n",
      "        return {\n",
      "            \"total_\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… Session 1 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import helper functions\n",
    "from memory_demo.demo_helpers import (\n",
    "    run_conversation_loop,\n",
    "    run_conversation_turn,\n",
    "    print_context_management_info,\n",
    ")\n",
    "from memory_tool import MemoryToolHandler\n",
    "\n",
    "# Initialize\n",
    "client = Anthropic(api_key=API_KEY)\n",
    "memory = MemoryToolHandler(base_path=\"./demo_memory\")\n",
    "\n",
    "# Clear any existing memories to start fresh\n",
    "print(\"ğŸ§¹ Clearing previous memories...\")\n",
    "memory.clear_all_memory()\n",
    "print(\"âœ“ Memory cleared\\n\")\n",
    "\n",
    "# Load example code with a race condition bug\n",
    "with open(\"memory_demo/sample_code/web_scraper_v1.py\", \"r\") as f:\n",
    "    code_to_review = f.read()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"I'm reviewing a multi-threaded web scraper that sometimes returns fewer results than expected. The count is inconsistent across runs. Can you find the issue?\\n\\n```python\\n{code_to_review}\\n```\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ SESSION 1: Learning from a bug\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run conversation loop\n",
    "response = run_conversation_loop(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    max_tokens=2048,\n",
    "    max_turns=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Session 1 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**\n\n1. Claudeæ£€æŸ¥äº†å®ƒçš„è®°å¿†ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶ä¸ºç©ºï¼‰\n2. è¯†åˆ«äº†é”™è¯¯ï¼š**ç«æ€æ¡ä»¶** - å¤šä¸ªçº¿ç¨‹åœ¨æ²¡æœ‰åŒæ­¥çš„æƒ…å†µä¸‹ä¿®æ”¹å…±äº«çŠ¶æ€ï¼ˆ`self.results`å’Œ`self.failed_urls`ï¼‰\n3. å°†å¹¶å‘æ¨¡å¼å­˜å‚¨åœ¨è®°å¿†ä¸­ä»¥ä¾›å°†æ¥å‚è€ƒ\n\nç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ç¥å¥‡ä¹‹å¤„ - Claudeåœ¨**æ–°å¯¹è¯**ä¸­åº”ç”¨è¿™ä¸ªå­¦åˆ°çš„æ¨¡å¼ï¼š"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ç¤ºä¾‹2ï¼šè·¨å¯¹è¯å­¦ä¹ \n\nå¼€å§‹ä¸€ä¸ªå…¨æ–°çš„å¯¹è¯ - è®°å¿†æŒç»­å­˜åœ¨ï¼"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ SESSION 2: Applying learned pattern\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Turn 1:\n",
      "  ğŸ”§ Memory tool: view /memories\n",
      "  âœ“ Result: Directory: /memories\n",
      "- review_progress.md\n",
      "\n",
      "ğŸ”„ Turn 2:\n",
      "  ğŸ”§ Memory tool: view /memories/review_progress.md\n",
      "  âœ“ Result:    1: # Code Review: Multi-threaded Web Scraper\n",
      "   2: \n",
      "   3: ## Task\n",
      "   4: Revie...\n",
      "\n",
      "ğŸ”„ Turn 3:\n",
      "  ğŸ”§ Memory tool: str_replace /memories/review_progress.md\n",
      "  âœ“ Result: File /memories/review_progress.md has been edited successfully\n",
      "\n",
      "ğŸ”„ Turn 4:\n",
      "ğŸ’¬ Claude: \n",
      "\n",
      "# Code Review: Async API Client\n",
      "\n",
      "## ğŸ”´ Critical Issues\n",
      "\n",
      "### 1. **Race Condition on Shared State (High Severity)**\n",
      "**Location:** `fetch_all()` method, lines with `self.responses.append()` and `self.error_count += 1`\n",
      "\n",
      "**Problem:** \n",
      "Multiple coroutines concurrently modify shared instance variables without synchronization. While the comments correctly identify this as an issue, there's an important nuance: Python's asyncio is **single-threaded**, so the GIL isn't the protective factor hereâ€”the real issue is that coroutines can be suspended mid-operation during `await` points.\n",
      "\n",
      "However, the actual race condition risk here is **somewhat overstated** because:\n",
      "- List `.append()` is atomic in CPython\n",
      "- The `+=` operation on integers is also atomic\n",
      "- Coroutines only switch at `await` points, and there are none between the operations\n",
      "\n",
      "**BUT** this is still problematic because:\n",
      "- It relies on CPython implementation details\n",
      "- It's not guaranteed by the language specification\n",
      "- The code is not portable to other Python implementations\n",
      "- Future refactoring could introduce `await` points that cause real races\n",
      "\n",
      "**Fix:** Use proper async coordination or refactor to avoid shared state:\n",
      "\n",
      "```python\n",
      "async def fetch_all(self, endpoints: List[str]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "    async with aiohttp.ClientSession() as session:\n",
      "        tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "        \n",
      "        # Collect all results at once - no shared state modification\n",
      "        results = await asyncio.gather(*tasks, return_exceptions=False)\n",
      "        \n",
      "        # Process results in single-threaded manner after all are collected\n",
      "        responses = []\n",
      "        error_count = 0\n",
      "        \n",
      "        for result in results:\n",
      "            if \"error\" in result:\n",
      "                error_count += 1\n",
      "            else:\n",
      "                responses.append(result)\n",
      "        \n",
      "        # Update instance state only once\n",
      "        self.responses = responses\n",
      "        self.error_count = error_count\n",
      "        \n",
      "        return responses\n",
      "```\n",
      "\n",
      "### 2. **Stateful Design Anti-Pattern (Medium-High Severity)**\n",
      "**Location:** Instance variables `self.responses` and `self.error_count`\n",
      "\n",
      "**Problem:**\n",
      "The client stores results as instance variables, which means:\n",
      "- **Not reusable:** Calling `fetch_all()` multiple times accumulates results\n",
      "- **Not thread-safe:** If someone wraps this in a thread pool, real race conditions occur\n",
      "- **Confusing API:** Results are both returned AND stored in instance\n",
      "- **Memory leak potential:** Old responses never cleared\n",
      "\n",
      "**Fix:** Remove stateful design entirely:\n",
      "\n",
      "```python\n",
      "async def fetch_all(self, endpoints: List[str]) -> Dict[str, Any]:\n",
      "    \"\"\"Fetch multiple endpoints concurrently.\"\"\"\n",
      "    async with aiohttp.ClientSession() as session:\n",
      "        tasks = [self.fetch_endpoint(session, endpoint) for endpoint in endpoints]\n",
      "        results = await asyncio.gather(*tasks, return_exceptions=False)\n",
      "        \n",
      "        responses = []\n",
      "        errors = []\n",
      "        \n",
      "        for result in results:\n",
      "            if \"error\" in result:\n",
      "                errors.append(result)\n",
      "            else:\n",
      "                responses.append(result)\n",
      "        \n",
      "        # Return everything as a structured result\n",
      "        return {\n",
      "            \"responses\": responses,\n",
      "            \"errors\": errors,\n",
      "            \"summary\": {\n",
      "                \"total_responses\": len(responses),\n",
      "                \"error_count\": len(errors),\n",
      "                \"success_rate\": (\n",
      "                    len(responses) / len(results) if results else 0\n",
      "                )\n",
      "            }\n",
      "        }\n",
      "```\n",
      "\n",
      "## âš ï¸ Medium Issues\n",
      "\n",
      "### 3. **Error Handling Loses Information**\n",
      "**Location:** `fetch_endpoint()` exception handler\n",
      "\n",
      "**Problem:**\n",
      "All errors are caught and returned as dictionaries with just the error string. This loses:\n",
      "- Exception type information\n",
      "- Stack traces (useful for debugging)\n",
      "- HTTP status codes for failed requests\n",
      "\n",
      "**Fix:**\n",
      "```python\n",
      "async def fetch_endpoint(\n",
      "    self, session: aiohttp.ClientSession, endpoint: str\n",
      ") -> Dict[str, Any]:\n",
      "    \"\"\"Fetch a single endpoint.\"\"\"\n",
      "    url = f\"{self.base_url}/{endpoint}\"\n",
      "    try:\n",
      "        async with session.get(\n",
      "            url, timeout=aiohttp.ClientTimeout(total=5)\n",
      "        ) as response:\n",
      "            data = await response.json()\n",
      "            return {\n",
      "                \"endpoint\": endpoint,\n",
      "                \"status\": response.status,\n",
      "                \"data\": data,\n",
      "                \"success\": True,\n",
      "            }\n",
      "    except aiohttp.ClientError as e:\n",
      "        return {\n",
      "            \"endpoint\": endpoint,\n",
      "            \"error\": str(e),\n",
      "            \"error_type\": type(e).__name__,\n",
      "            \"success\": False,\n",
      "        }\n",
      "    except Exception as e:\n",
      "        # Log unexpected errors\n",
      "        return {\n",
      "            \"endpoint\": endpoint,\n",
      "            \"error\": str(e),\n",
      "            \"error_type\": type(e).__name__,\n",
      "            \"success\": False,\n",
      "        }\n",
      "```\n",
      "\n",
      "### 4. **Using `asyncio.as_completed()` Unnecessarily**\n",
      "**Location:** `fetch_all()` method\n",
      "\n",
      "**Problem:**\n",
      "The code uses `asyncio.as_completed()` but doesn't benefit from processing results as they arrive. It still waits for all results and returns them all at once. This adds complexity without benefit.\n",
      "\n",
      "**Fix:** Use `asyncio.gather()` which is simpler and more idiomatic for this use case.\n",
      "\n",
      "### 5. **Missing HTTP Status Code Validation**\n",
      "**Location:** `fetch_endpoint()` method\n",
      "\n",
      "**Problem:**\n",
      "A 404 or 500 status code will be treated as success if it returns valid JSON. The code only checks for exceptions, not HTTP error status codes.\n",
      "\n",
      "**Fix:**\n",
      "```python\n",
      "async with session.get(\n",
      "    url, timeout=aiohttp.ClientTimeout(total=5)\n",
      ") as response:\n",
      "    if response.status >= 400:\n",
      "        return {\n",
      "            \"endpoint\": endpoint,\n",
      "            \"status\": response.status,\n",
      "            \"error\": f\"HTTP {response.status}\",\n",
      "            \"success\": False,\n",
      "        }\n",
      "    data = await response.json()\n",
      "    return {\n",
      "        \"endpoint\": endpoint,\n",
      "        \"status\": response.status,\n",
      "        \"data\": data,\n",
      "        \"success\": True,\n",
      "    }\n",
      "```\n",
      "\n",
      "## ğŸ’¡ Minor Issues & Suggestions\n",
      "\n",
      "### 6. **Session Should Be Reusable**\n",
      "**Location:** `fetch_all()` creates new session each time\n",
      "\n",
      "**Suggestion:**\n",
      "For better performance, consider managing the session at the class level or allowing session reuse:\n",
      "\n",
      "```python\n",
      "class AsyncAPIClient:\n",
      "    def __init__(self, base_url: str):\n",
      "        self.base_url = base_url\n",
      "        self._session: Optional[aiohttp.ClientSession] = None\n",
      "    \n",
      "    async def __aenter__(self):\n",
      "        self._session = aiohttp.ClientSession()\n",
      "        return self\n",
      "    \n",
      "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
      "        if self._session:\n",
      "            await self._session.close()\n",
      "    \n",
      "    async def fetch_endpoint(self, endpoint: str) -> Dict[str, Any]:\n",
      "        if not self._session:\n",
      "            raise RuntimeError(\"Use client as async context manager\")\n",
      "        # ... use self._session\n",
      "```\n",
      "\n",
      "### 7. **Type Hints Could Be More Specific**\n",
      "The return type `Dict[str, Any]` is too generic. Consider using TypedDict:\n",
      "\n",
      "```python\n",
      "from typing import TypedDict\n",
      "\n",
      "class SuccessResponse(TypedDict):\n",
      "    endpoint: str\n",
      "    status: int\n",
      "    data: Any\n",
      "    success: bool\n",
      "\n",
      "class ErrorResponse(TypedDict):\n",
      "    endpoint: str\n",
      "    error: str\n",
      "    error_type: str\n",
      "    success: bool\n",
      "```\n",
      "\n",
      "### 8. **Missing Docstring Details**\n",
      "Add information about return value structure, exceptions raised, and parameters.\n",
      "\n",
      "## ğŸ“‹ Summary\n",
      "\n",
      "| Severity | Count | Issues |\n",
      "|----------|-------|---------|\n",
      "| ğŸ”´ Critical | 2 | Shared state race conditions, stateful design anti-pattern |\n",
      "| âš ï¸ Medium | 3 | Error handling, unnecessary complexity, missing status validation |\n",
      "| ğŸ’¡ Minor | 3 | Session management, type hints, documentation |\n",
      "\n",
      "## Recommended Refactored Version\n",
      "\n",
      "```python\n",
      "\"\"\"\n",
      "Async API client with proper async patterns.\n",
      "\"\"\"\n",
      "\n",
      "import asyncio\n",
      "from typing import List, Dict, Any, TypedDict\n",
      "\n",
      "import aiohttp\n",
      "\n",
      "\n",
      "class ResponseResult(TypedDict):\n",
      "    endpoint: str\n",
      "    status: int\n",
      "    data:\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… Session 2 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# NEW conversation (empty messages)\n",
    "# Load API client code with similar concurrency issue\n",
    "with open(\"memory_demo/sample_code/api_client_v1.py\", \"r\") as f:\n",
    "    code_to_review = f.read()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"Review this API client code:\\n\\n```python\\n{code_to_review}\\n```\"}\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ SESSION 2: Applying learned pattern\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run conversation loop\n",
    "response = run_conversation_loop(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    max_tokens=2048,\n",
    "    max_turns=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Session 2 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**æ³¨æ„å·®å¼‚ï¼š**\n\n- Claude**ç«‹å³æ£€æŸ¥äº†è®°å¿†**å¹¶æ‰¾åˆ°äº†çº¿ç¨‹å®‰å…¨/å¹¶å‘æ¨¡å¼\n- **ç«‹å³è¯†åˆ«**äº†å¼‚æ­¥ä»£ç ä¸­çš„ç±»ä¼¼é—®é¢˜ï¼Œæ— éœ€é‡æ–°å­¦ä¹ \n- å“åº”**æ›´å¿«**ï¼Œå› ä¸ºå®ƒåº”ç”¨äº†å…³äºå…±äº«å¯å˜çŠ¶æ€çš„å­˜å‚¨çŸ¥è¯†\n\nè¿™æ˜¯**è·¨å¯¹è¯å­¦ä¹ **çš„å®é™…åº”ç”¨ï¼"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ç¤ºä¾‹3ï¼šåœ¨ä¿ç•™è®°å¿†çš„åŒæ—¶æ¸…é™¤ä¸Šä¸‹æ–‡\n\nåœ¨åŒ…å«è®¸å¤šä»£ç æ–‡ä»¶çš„**é•¿å®¡æŸ¥ä¼šè¯**æœŸé—´ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ\n\n- ä¸Šä¸‹æ–‡å……æ»¡äº†æ¥è‡ªå…ˆå‰å®¡æŸ¥çš„å·¥å…·ç»“æœ\n- ä½†è®°å¿†ï¼ˆå­¦åˆ°çš„æ¨¡å¼ï¼‰å¿…é¡»æŒç»­å­˜åœ¨ï¼\n\nè®©æˆ‘ä»¬è§¦å‘**ä¸Šä¸‹æ–‡ç¼–è¾‘**ä»¥æŸ¥çœ‹Claudeå¦‚ä½•è‡ªåŠ¨ç®¡ç†è¿™ä¸€ç‚¹ã€‚\n\n**å…³äºé…ç½®çš„è¯´æ˜ï¼š**æˆ‘ä»¬ä½¿ç”¨`clear_at_least: 50`ä¸ªæ ‡è®°ï¼Œå› ä¸ºè®°å¿†å·¥å…·æ“ä½œçš„ç»“æœå¾ˆå°ï¼ˆçº¦50-150ä¸ªæ ‡è®°ï¼‰ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨æ›´å¤§çš„å·¥å…·ç»“æœï¼ˆå¦‚ç½‘ç»œæœç´¢æˆ–ä»£ç æ‰§è¡Œï¼‰ï¼Œæ‚¨å°†ä½¿ç”¨3000-5000ä¸ªæ ‡è®°ç­‰æ›´é«˜å€¼ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“š SESSION 3: Long review session with context clearing\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Review 1: Data processor\n",
      "  ğŸ”§ Memory tool: str_replace /memories/review_progress.md\n",
      "  âœ“ Result: File /memories/review_progress.md has been edited successfully\n",
      "  ğŸ“Š Input tokens: 6,243\n",
      "  âœ‚ï¸  Context editing triggered!\n",
      "      â€¢ Cleared 1 tool uses\n",
      "      â€¢ Saved 66 tokens\n",
      "      â€¢ After clearing: 6,243 tokens\n",
      "\n",
      "ğŸ“ Review 2: SQL query builder\n",
      "  ğŸ”§ Memory tool: str_replace /memories/review_progress.md\n",
      "  âœ“ Result: File /memories/review_progress.md has been edited successfully\n",
      "  ğŸ“Š Input tokens: 7,471\n",
      "  âœ‚ï¸  Context editing triggered!\n",
      "      â€¢ Cleared 1 tool uses\n",
      "      â€¢ Saved 66 tokens\n",
      "      â€¢ After clearing: 7,471 tokens\n",
      "\n",
      "============================================================\n",
      "âœ… Session 3 complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure context management to clear aggressively for demo\n",
    "CONTEXT_MANAGEMENT = {\n",
    "    \"edits\": [\n",
    "        {\n",
    "            \"type\": \"clear_tool_uses_20250919\",\n",
    "            \"trigger\": {\n",
    "                \"type\": \"input_tokens\",\n",
    "                \"value\": 5000,\n",
    "            },  # Lower threshold to trigger clearing sooner\n",
    "            \"keep\": {\"type\": \"tool_uses\", \"value\": 1},  # Keep only the last tool use\n",
    "            \"clear_at_least\": {\"type\": \"input_tokens\", \"value\": 50},\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Continue from previous session - memory persists!\n",
    "# Add multiple code reviews to build up context\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“š SESSION 3: Long review session with context clearing\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Review 1: Data processor (larger file)\n",
    "with open(\"memory_demo/sample_code/data_processor_v1.py\", \"r\") as f:\n",
    "    data_processor_code = f.read()\n",
    "\n",
    "messages.extend(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Review this data processor:\\n\\n```python\\n{data_processor_code}\\n```\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Review 1: Data processor\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    max_tokens=2048,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Add response to messages\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  ğŸ“Š Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "print()\n",
    "\n",
    "# Review 2: Add SQL code\n",
    "with open(\"memory_demo/sample_code/sql_query_builder.py\", \"r\") as f:\n",
    "    sql_code = f.read()\n",
    "\n",
    "messages.extend(\n",
    "    [{\"role\": \"user\", \"content\": f\"Review this SQL query builder:\\n\\n```python\\n{sql_code}\\n```\"}]\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Review 2: SQL query builder\")\n",
    "response = run_conversation_turn(\n",
    "    client=client,\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    memory_handler=memory,\n",
    "    system=\"You are a code reviewer.\",\n",
    "    context_management=CONTEXT_MANAGEMENT,\n",
    "    max_tokens=2048,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[1]})\n",
    "if response[2]:\n",
    "    messages.append({\"role\": \"user\", \"content\": response[2]})\n",
    "\n",
    "print(f\"  ğŸ“Š Input tokens: {response[0].usage.input_tokens:,}\")\n",
    "context_cleared, saved = print_context_management_info(response[0])\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Session 3 complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**åˆšæ‰å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**\n\nåœ¨å¤šæ¬¡å®¡æŸ¥è¿‡ç¨‹ä¸­éšç€ä¸Šä¸‹æ–‡å¢é•¿ï¼š\n1. å½“è¾“å…¥æ ‡è®°è¶…è¿‡5,000æ—¶**è‡ªåŠ¨è§¦å‘ä¸Šä¸‹æ–‡æ¸…é™¤**\n2. **åˆ é™¤äº†æ—§å·¥å…·ç»“æœ** - æ¸…é™¤2ä¸ªå·¥å…·ä½¿ç”¨ï¼Œæ¯æ¬¡èŠ‚çœçº¦66ä¸ªæ ‡è®°\n3. **è®°å¿†æ–‡ä»¶ä¿æŒå®Œæ•´** - Claudeä»ç„¶å¯ä»¥æŸ¥è¯¢å­¦åˆ°çš„æ¨¡å¼\n4. **æ ‡è®°ä½¿ç”¨ç»§ç»­å¢é•¿**ä½†ç”±äºæ¸…é™¤è€Œå¢é•¿è¾ƒæ…¢\n\nè¿™å±•ç¤ºäº†å…³é”®å¥½å¤„ï¼š\n- **çŸ­æœŸè®°å¿†**ï¼ˆå¸¦æœ‰å·¥å…·ç»“æœçš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰â†’ æ¸…é™¤ä»¥èŠ‚çœç©ºé—´\n- **é•¿æœŸè®°å¿†**ï¼ˆå­˜å‚¨åœ¨`/memories`ä¸­çš„æ¨¡å¼ï¼‰â†’ è·¨ä¼šè¯æŒç»­\n\n**ä¸ºä»€ä¹ˆèŠ‚çœçš„æ ‡è®°è¿™ä¹ˆå°‘ï¼Ÿ**è®°å¿†å·¥å…·æ“ä½œè¿”å›ç´§å‡‘ç»“æœï¼ˆæ–‡ä»¶è·¯å¾„ã€æˆåŠŸæ¶ˆæ¯ï¼‰ã€‚`str_replace`æ“ä½œåªè¿”å›\"æ–‡ä»¶ç¼–è¾‘æˆåŠŸ\"åŠ ä¸Šå…ƒæ•°æ®ã€‚åœ¨ç”Ÿäº§ç”¨ä¾‹ä¸­ï¼Œå·¥å…·ç»“æœæ›´å¤§ï¼ˆç½‘ç»œæœç´¢è¿”å›å®Œæ•´æ–‡ç« ã€ä»£ç æ‰§è¡Œè¾“å‡ºè¾ƒé•¿ï¼‰ï¼Œä¸Šä¸‹æ–‡æ¸…é™¤å°†èŠ‚çœæ•°åƒä¸ªæ ‡è®°ã€‚\n\nè®©æˆ‘ä»¬éªŒè¯è®°å¿†åœ¨æ¸…é™¤ä¸­å¹¸å­˜ä¸‹æ¥ï¼š"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Memory files in demo_memory/:\n",
      "\n",
      "demo_memory/\n",
      "  memories/\n",
      "    â”œâ”€â”€ review_progress.md (257 bytes)\n",
      "\n",
      "âœ… All learned patterns preserved despite context clearing!\n"
     ]
    }
   ],
   "source": [
    "# Verify memory persists after context clearing\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“‚ Memory files in demo_memory/:\")\n",
    "print()\n",
    "\n",
    "for root, dirs, files in os.walk(\"./demo_memory\"):\n",
    "    # Calculate relative path for display\n",
    "    level = root.replace(\"./demo_memory\", \"\").count(os.sep)\n",
    "    indent = \"  \" * level\n",
    "    folder_name = os.path.basename(root) or \"demo_memory\"\n",
    "    print(f\"{indent}{folder_name}/\")\n",
    "\n",
    "    sub_indent = \"  \" * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"{sub_indent}â”œâ”€â”€ {file} ({size} bytes)\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… All learned patterns preserved despite context clearing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. å·¥ä½œåŸç† {#how-it-works}\n\n### è®°å¿†å·¥å…·æ¶æ„\n\nè®°å¿†å·¥å…·æ˜¯**å®¢æˆ·ç«¯çš„** - æ‚¨æ§åˆ¶å­˜å‚¨ã€‚Claudeè¿›è¡Œå·¥å…·è°ƒç”¨ï¼Œæ‚¨çš„åº”ç”¨ç¨‹åºæ‰§è¡Œå®ƒä»¬ã€‚\n\n#### è®°å¿†å·¥å…·å‘½ä»¤\n\n| å‘½ä»¤ | æè¿° | ç¤ºä¾‹ |\n|---------|-------------|---------|\n| `view` | æ˜¾ç¤ºç›®å½•æˆ–æ–‡ä»¶å†…å®¹ | `{\"command\": \"view\", \"path\": \"/memories\"}` |\n| `create` | åˆ›å»ºæˆ–è¦†ç›–æ–‡ä»¶ | `{\"command\": \"create\", \"path\": \"/memories/notes.md\", \"file_text\": \"...\"}` |\n| `str_replace` | æ›¿æ¢æ–‡ä»¶ä¸­çš„æ–‡æœ¬ | `{\"command\": \"str_replace\", \"path\": \"...\", \"old_str\": \"...\", \"new_str\": \"...\"}` |\n| `insert` | åœ¨è¡Œå·å¤„æ’å…¥æ–‡æœ¬ | `{\"command\": \"insert\", \"path\": \"...\", \"insert_line\": 2, \"insert_text\": \"...\"}` |\n| `delete` | åˆ é™¤æ–‡ä»¶æˆ–ç›®å½• | `{\"command\": \"delete\", \"path\": \"/memories/old.txt\"}` |\n| `rename` | é‡å‘½åæˆ–ç§»åŠ¨æ–‡ä»¶ | `{\"command\": \"rename\", \"old_path\": \"...\", \"new_path\": \"...\"}` |\n\næœ‰å…³å¸¦æœ‰è·¯å¾„éªŒè¯å’Œå®‰å…¨æªæ–½çš„å®Œæ•´å®ç°ï¼Œè¯·å‚è§`memory_tool.py`ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ç†è§£æ¼”ç¤ºä»£ç \n\n`code_review_demo.py`ä¸­çš„å…³é”®å®ç°ç»†èŠ‚ï¼š\n\n```python\nclass CodeReviewAssistant:\n    def __init__(self, memory_storage_path=\"./memory_storage\"):\n        self.client = Anthropic(api_key=API_KEY)\n        self.memory_handler = MemoryToolHandler(base_path=memory_storage_path)\n        self.messages = []\n    \n    def review_code(self, code, filename, description=\"\"):\n        # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯\n        self.messages.append({...})\n        \n        # 2. å¸¦æœ‰å·¥å…·æ‰§è¡Œçš„å¯¹è¯å¾ªç¯\n        while True:\n            response = self.client.beta.messages.create(\n                model=MODEL,\n                system=self._create_system_prompt(),\n                messages=self.messages,\n                tools=[{\"type\": \"memory_20250818\", \"name\": \"memory\"}],\n                betas=[\"context-management-2025-06-27\"],\n                context_management=CONTEXT_MANAGEMENT\n            )\n            \n            # 3. æ‰§è¡Œå·¥å…·ä½¿ç”¨\n            tool_results = []\n            for content in response.content:\n                if content.type == \"tool_use\":\n                    result = self._execute_tool_use(content)\n                    tool_results.append({...})\n            \n            # 4. å¦‚æœæœ‰å·¥å…·ä½¿ç”¨åˆ™ç»§ç»­ï¼Œå¦åˆ™å®Œæˆ\n            if tool_results:\n                self.messages.append({\"role\": \"user\", \"content\": tool_results})\n            else:\n                break\n```\n\n**å…³é”®æ¨¡å¼**ï¼šåªè¦æœ‰å·¥å…·ä½¿ç”¨å°±ç»§ç»­è°ƒç”¨APIï¼Œæ‰§è¡Œå®ƒä»¬å¹¶åé¦ˆç»“æœã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Claudeå®é™…å­¦åˆ°äº†ä»€ä¹ˆ\n\nè¿™ä½¿å¾—è®°å¿†å˜å¾—å¼ºå¤§ - **è¯­ä¹‰æ¨¡å¼è¯†åˆ«**ï¼Œè€Œä¸ä»…ä»…æ˜¯è¯­æ³•ï¼š\n\n**ä¼šè¯1ï¼šåŸºäºçº¿ç¨‹çš„Webçˆ¬è™«**\n\n```python\n# é”™è¯¯ï¼šç«æ€æ¡ä»¶\nclass WebScraper:\n    def __init__(self):\n        self.results = []  # å…±äº«çŠ¶æ€ï¼\n    \n    def scrape_urls(self, urls):\n        with ThreadPoolExecutor() as executor:\n            for future in as_completed(futures):\n                self.results.append(future.result())  # ç«æ€æ¡ä»¶ï¼\n```\n\n**Claudeåœ¨è®°å¿†ä¸­å­˜å‚¨çš„å†…å®¹**ï¼ˆç¤ºä¾‹æ–‡ä»¶ï¼š`/memories/concurrency_patterns/thread_safety.md`ï¼‰ï¼š\n\nå½“Claudeé‡åˆ°æ­¤æ¨¡å¼æ—¶ï¼Œå®ƒå°†å…¶ä»¥ä¸‹è§è§£å­˜å‚¨åˆ°å…¶è®°å¿†æ–‡ä»¶ä¸­ï¼š\n- **ç—‡çŠ¶**ï¼šå¹¶å‘æ“ä½œä¸­çš„ä¸ä¸€è‡´ç»“æœ\n- **åŸå› **ï¼šæ¥è‡ªå¤šä¸ªçº¿ç¨‹çš„å…±äº«å¯å˜çŠ¶æ€ï¼ˆåˆ—è¡¨/å­—å…¸ï¼‰è¢«ä¿®æ”¹\n- **è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨é”ã€çº¿ç¨‹å®‰å…¨æ•°æ®ç»“æ„ï¼Œæˆ–æ”¹ä¸ºè¿”å›ç»“æœ\n- **å±é™©ä¿¡å·**ï¼šçº¿ç¨‹å›è°ƒä¸­çš„å®ä¾‹å˜é‡ã€æœªä½¿ç”¨çš„é”ã€è®¡æ•°å™¨å¢é‡\n\n---\n\n**ä¼šè¯2ï¼šå¼‚æ­¥APIå®¢æˆ·ç«¯**ï¼ˆæ–°å¯¹è¯ï¼ï¼‰\n\nClaudeé¦–å…ˆæ£€æŸ¥è®°å¿†ï¼Œæ‰¾åˆ°çº¿ç¨‹å®‰å…¨æ¨¡å¼ï¼Œç„¶åï¼š\n1. **è¯†åˆ«**å¼‚æ­¥ä»£ç ä¸­çš„ç±»ä¼¼æ¨¡å¼ï¼ˆåç¨‹ä¹Ÿå¯ä»¥äº¤é”™ï¼‰\n2. **ç«‹å³åº”ç”¨**è§£å†³æ–¹æ¡ˆï¼ˆæ— éœ€é‡æ–°å­¦ä¹ ï¼‰\n3. **æ ¹æ®å­˜å‚¨çŸ¥è¯†è§£é‡Š**\n\n```python\n# Claudeç«‹å³å‘ç°è¿™ä¸€ç‚¹ï¼š\nasync def fetch_all(self, endpoints):\n    for coro in asyncio.as_completed(tasks):\n        self.responses.append(await coro)  # ç›¸åŒæ¨¡å¼ï¼\n```\n\n---\n\n**ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼š**\n\n- âŒ **è¯­æ³•æ£€æŸ¥å™¨**å®Œå…¨é”™è¿‡ç«æ€æ¡ä»¶\n- âœ… **Claudeå­¦ä¹ **æ¶æ„æ¨¡å¼å¹¶è·¨ä¸Šä¸‹æ–‡åº”ç”¨å®ƒä»¬\n- âœ… **è·¨è¯­è¨€**ï¼šæ¨¡å¼ä¹Ÿé€‚ç”¨äºGoã€Javaã€Rustå¹¶å‘\n- âœ… **å˜å¾—æ›´å¥½**ï¼šæ¯æ¬¡å®¡æŸ¥éƒ½ä¼šæ·»åŠ åˆ°çŸ¥è¯†åº“"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ç¤ºä¾‹ä»£ç æ–‡ä»¶\n\næ¼”ç¤ºä½¿ç”¨è¿™äº›ç¤ºä¾‹æ–‡ä»¶ï¼ˆéƒ½æœ‰å¹¶å‘/çº¿ç¨‹å®‰å…¨é”™è¯¯ï¼‰ï¼š\n\n- `memory_demo/sample_code/web_scraper_v1.py` - ç«æ€æ¡ä»¶ï¼šçº¿ç¨‹ä¿®æ”¹å…±äº«çŠ¶æ€\n- `memory_demo/sample_code/api_client_v1.py` - å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­çš„ç±»ä¼¼å¹¶å‘é”™è¯¯\n- `memory_demo/sample_code/data_processor_v1.py` - é•¿ä¼šè¯æ¼”ç¤ºçš„å¤šä¸ªå¹¶å‘é—®é¢˜\n\nè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸€ä¸ªï¼š"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`memory_demo/sample_code/web_scraper_v1.py`**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Concurrent web scraper with a race condition bug.\n",
    "Multiple threads modify shared state without synchronization.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"Web scraper that fetches multiple URLs concurrently.\"\"\"\n",
    "\n",
    "    def __init__(self, max_workers: int = 10):\n",
    "        self.max_workers = max_workers\n",
    "        self.results = []  # BUG: Shared mutable state accessed by multiple threads!\n",
    "        self.failed_urls = []  # BUG: Another race condition!\n",
    "\n",
    "    def fetch_url(self, url: str) -> Dict[str, any]:\n",
    "        \"\"\"Fetch a single URL and return the result.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            return {\n",
    "                \"url\": url,\n",
    "                \"status\": response.status_code,\n",
    "                \"content_length\": len(response.content),\n",
    "            }\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "    def scrape_urls(self, urls: List[str]) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Scrape multiple URLs concurrently.\n",
    "\n",
    "        BUG: self.results is accessed from multiple threads without locking!\n",
    "        This causes race conditions where results can be lost or corrupted.\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(self.fetch_url, url) for url in urls]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "\n",
    "                # RACE CONDITION: Multiple threads append to self.results simultaneously\n",
    "                if \"error\" in result:\n",
    "                    self.failed_urls.append(result[\"url\"])  # RACE CONDITION\n",
    "                else:\n",
    "                    self.results.append(result)  # RACE CONDITION\n",
    "\n",
    "        return self.results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**é”™è¯¯**ï¼šå¤šä¸ªçº¿ç¨‹åœ¨æ²¡æœ‰é”å®šçš„æƒ…å†µä¸‹ä¿®æ”¹`self.results`å’Œ`self.failed_urls`ï¼\n\nClaudeå°†ï¼š\n1. è¯†åˆ«ç«æ€æ¡ä»¶\n2. å°†æ¨¡å¼å­˜å‚¨åœ¨`/memories/concurrency_patterns/thread_safety.md`\n3. åœ¨ä¼šè¯2ä¸­å°†æ­¤å¹¶å‘æ¨¡å¼åº”ç”¨äºå¼‚æ­¥ä»£ç "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### æ¼”ç¤ºæ¦‚è¿°\n\næˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ä»£ç å®¡æŸ¥åŠ©æ‰‹ã€‚å®ç°ä½äº`memory_demo/code_review_demo.py`ã€‚\n\n**è¿è¡Œäº¤äº’å¼æ¼”ç¤ºï¼š**\n```bash\npython memory_demo/code_review_demo.py\n```\n\næ¼”ç¤ºå±•ç¤ºï¼š\n1. **ä¼šè¯1**ï¼šå®¡æŸ¥å¸¦æœ‰é”™è¯¯çš„Pythonä»£ç  â†’ Claudeå­¦ä¹ æ¨¡å¼\n2. **ä¼šè¯2**ï¼šå®¡æŸ¥ç±»ä¼¼ä»£ç ï¼ˆæ–°å¯¹è¯ï¼‰â†’ Claudeåº”ç”¨æ¨¡å¼\n3. **ä¼šè¯3**ï¼šé•¿å®¡æŸ¥ä¼šè¯ â†’ ä¸Šä¸‹æ–‡ç¼–è¾‘ä¿æŒå¯ç®¡ç†"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. æœ€ä½³å®è·µä¸å®‰å…¨ {#best-practices}\n\n### è®°å¿†ç®¡ç†\n\n**åº”è¯¥åšï¼š**\n- âœ… å­˜å‚¨ä»»åŠ¡ç›¸å…³çš„æ¨¡å¼ï¼Œè€Œä¸æ˜¯å¯¹è¯å†å²\n- âœ… ä½¿ç”¨æ¸…æ™°çš„ç›®å½•ç»“æ„ç»„ç»‡\n- âœ… ä½¿ç”¨æè¿°æ€§æ–‡ä»¶å\n- âœ… å®šæœŸå®¡æŸ¥å’Œæ¸…ç†è®°å¿†\n\n**ä¸åº”è¯¥åšï¼š**\n- âŒ å­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼ˆå¯†ç ã€APIå¯†é’¥ã€ä¸ªäººèº«ä»½ä¿¡æ¯ï¼‰\n- âŒ è®©è®°å¿†æ— é™åˆ¶å¢é•¿\n- âŒ ä¸åŠ é€‰æ‹©åœ°å­˜å‚¨æ‰€æœ‰å†…å®¹\n\n### å®‰å…¨ï¼šè·¯å¾„éå†ä¿æŠ¤\n\n**å…³é”®**ï¼šå§‹ç»ˆéªŒè¯è·¯å¾„ä»¥é˜²æ­¢ç›®å½•éå†æ”»å‡»ã€‚å®ç°è¯·å‚è§`memory_tool.py`ã€‚\n\n### å®‰å…¨ï¼šè®°å¿†æŠ•æ¯’\n\n**âš ï¸ å…³é”®é£é™©**ï¼šè®°å¿†æ–‡ä»¶è¢«è¯»å›åˆ°Claudeçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œä½¿å®ƒä»¬æˆä¸ºæç¤ºæ³¨å…¥çš„æ½œåœ¨è½½ä½“ã€‚\n\n**ç¼“è§£ç­–ç•¥ï¼š**\n1. **å†…å®¹æ¸…ç†**ï¼šåœ¨å­˜å‚¨å‰è¿‡æ»¤å±é™©æ¨¡å¼\n2. **è®°å¿†èŒƒå›´éš”ç¦»**ï¼šæ¯ä¸ªç”¨æˆ·/é¡¹ç›®çš„éš”ç¦»\n3. **è®°å¿†å®¡è®¡**ï¼šè®°å½•å’Œæ‰«ææ‰€æœ‰è®°å¿†æ“ä½œ\n4. **æç¤ºå·¥ç¨‹**ï¼šæŒ‡ç¤ºClaudeå¿½ç•¥è®°å¿†ä¸­çš„æŒ‡ä»¤\n\næœ‰å…³å®Œæ•´çš„å®‰å…¨å®ç°å’Œ`tests/`ä¸­çš„æµ‹è¯•ï¼Œè¯·å‚è§`memory_tool.py`ã€‚"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ä¸‹ä¸€æ­¥\n\n### èµ„æº\n\n- **APIæ–‡æ¡£**ï¼š[Claude APIå‚è€ƒ](https://docs.claude.com/en/api/messages)\n- **ä½¿ç”¨æ–‡æ¡£**ï¼š[è®°å¿†å·¥å…·](https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool)\n- **GitHubæ“ä½œ**ï¼š[claude-code-action](https://github.com/anthropics/claude-code-action)\n- **AWS/Bedrockå®ç°**ï¼š[AWSä¸Šçš„Anthropicè®°å¿†åŠŸèƒ½](https://github.com/aws-samples/anthropic-on-aws/tree/main/notebooks/claude_memory_features) - ç‰¹å®šäºBedrockçš„å®ç°\n- **æ”¯æŒ**ï¼š[support.claude.com](https://support.claude.com)\n\n### åé¦ˆ\n\nè®°å¿†å’Œä¸Šä¸‹æ–‡ç®¡ç†å¤„äº**æµ‹è¯•ç‰ˆ**ã€‚åˆ†äº«æ‚¨çš„åé¦ˆä»¥å¸®åŠ©æˆ‘ä»¬æ”¹è¿›ï¼"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}